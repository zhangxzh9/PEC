{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim=None, hid_dim, n_layers, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        #self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.rnn = nn.LSTM(input_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        embedded = self.dropout(src)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R267_U1_V10/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>video_type</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "      <th>level4</th>\n",
       "      <th>time</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>343</td>\n",
       "      <td>5753</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22</td>\n",
       "      <td>5806</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>294</td>\n",
       "      <td>5463</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>551</td>\n",
       "      <td>2902</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>278</td>\n",
       "      <td>4277</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267680</th>\n",
       "      <td>913</td>\n",
       "      <td>2875</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43198</td>\n",
       "      <td>2875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267681</th>\n",
       "      <td>594</td>\n",
       "      <td>7449</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43199</td>\n",
       "      <td>7449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267682</th>\n",
       "      <td>271</td>\n",
       "      <td>8016</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43199</td>\n",
       "      <td>8016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267683</th>\n",
       "      <td>482</td>\n",
       "      <td>1772</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>43199</td>\n",
       "      <td>1772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267684</th>\n",
       "      <td>653</td>\n",
       "      <td>4918</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43199</td>\n",
       "      <td>4918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>267685 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day  video_type  level1  level2  level3  level4   time  \\\n",
       "0       343  5753    0          11       0       0       0       0      0   \n",
       "1        22  5806    0          11       0       0       0       0      0   \n",
       "2       294  5463    0          11       0       0       0       2      0   \n",
       "3       551  2902    0          11       0       0       0       0      0   \n",
       "4       278  4277    0          11       0       0       0       0      0   \n",
       "...     ...   ...  ...         ...     ...     ...     ...     ...    ...   \n",
       "267680  913  2875   29          11       0       0       0       0  43198   \n",
       "267681  594  7449   29          11       0       0       0       0  43199   \n",
       "267682  271  8016   29          13       0       0       0       0  43199   \n",
       "267683  482  1772   29          13       0       0       0       2  43199   \n",
       "267684  653  4918   29          13       0       0       0       0  43199   \n",
       "\n",
       "           v  \n",
       "0       5753  \n",
       "1       5806  \n",
       "2       5463  \n",
       "3       2902  \n",
       "4       4277  \n",
       "...      ...  \n",
       "267680  2875  \n",
       "267681  7449  \n",
       "267682  8016  \n",
       "267683  1772  \n",
       "267684  4918  \n",
       "\n",
       "[267685 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=10,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "\n",
    "        self.p = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p) , \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                self.l_edge,\n",
    "                self.l_cp)\n",
    "\n",
    "    def reset(self):\n",
    "        self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "        self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "        self.e = np.zeros(shape=self.contentNum)\n",
    "        self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "        self.B = np.full(shape=self.userNum,fill_value=10,dtype=int)\n",
    "        self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "#每个神经网络单独作为一个reward进行训练\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']\n",
    "        self.EPS_START = 0.9\n",
    "        self.EPS_END = 0.1\n",
    "        self.EPS_DECAY = 10\n",
    "        \n",
    "        self.t = 0\n",
    "        \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e):\n",
    "\n",
    "        Rh = - self.ALPHAh * (torch.log(lastru * lastp + (1-lastru) * (1-lastp)) - torch.log(ru * p + (1-ru) * (1-p)))\n",
    "\n",
    "        Ro =   self.ALPHAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        Rl =   self.ALPHAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        Rh[i] = Rh[i] + Ro + Rl\n",
    "\n",
    "        return  Rh\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "\n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.action,self.S,self.Bu,self.l_edge,self.l_cp,self.e)\n",
    "        \n",
    "        if train:\n",
    "            \n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                    lastAction.to(device), \n",
    "                    self.statusFeature,\n",
    "                    self.reward.float().to(device),\n",
    "                    torch.tensor([self.W[-1]]).to(device))\n",
    "        \n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) *  np.exp(-1. * self.t / self.EPS_DECAY)\n",
    "        self.t = self.t + 1\n",
    "        \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "        if  not train or (train and sample > eps_threshold):\n",
    "            QNetwork.eval()\n",
    "            with torch.no_grad():\n",
    "                Q_value = QNetwork(self.statusFeature)\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "            QNetwork.train()\n",
    "        else:\n",
    "            actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.masked_select(policy_net(state_batch),state_action_mask_bacth)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        with torch.no_grad():\n",
    "            Q_value= torch.stack(target_net(s_batch).chunk(BATCH_SIZE,dim=0))\n",
    "            c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "            action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "            for b in range(BATCH_SIZE):\n",
    "                Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort()[0:10])\n",
    "                i = c[b].squeeze()\n",
    "                if i not in Q_value_sortindex:\n",
    "                    Q_value_sortindex.pop()\n",
    "                action[b,i,1] = 1\n",
    "                for index in Q_value_sortindex:\n",
    "                    action[b,index,1] = 1\n",
    "            action[:,:,0]=1-action[:,:,1]\n",
    "            action_mask = action.ge(0.5).to(device)\n",
    "            next_state_values = torch.masked_select(Q_value,action_mask).float()\n",
    "\n",
    "            return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "MODELPATH =  './model_dict/lstm1_ep'\n",
    "\n",
    "num_episodes = 20\n",
    "TARGET_UPDATE = 2\n",
    "bestReward =  0\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.5,\"betal\":0.5}\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "\n",
    "\n",
    "UEs = {}\n",
    "sumReward = 0\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward += ue.reward.sum()\n",
    "        \n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        if index % 1000 == 0:\n",
    "            loss += optimize_model()\n",
    "        \n",
    "        if index % 10000 == 0:\n",
    "            print(\"Time:\",time.asctime( time.localtime(time.time())),\"--Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",float(loss/(index+1)),\"  Reward:\",sumReward/(index+1),)\n",
    "            print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1))\n",
    "            print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Time:\",time.asctime( time.localtime(time.time())),\"--End episode:\",i_episode,\"  Loss:\",loss/(index+1),\"  Reward:\",sumReward/(index+1))\n",
    "    print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "    \n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0: \n",
    "        if sumReward > bestReward:\n",
    "            bestLoss = loss\n",
    "            bestReward = sumReward\n",
    "            bestUEHit = UEHit\n",
    "            bestEdgeHit = edgeHit\n",
    "            torch.save(policy_net.state_dict(),MODELPATH+\"{}_\".format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time())))\n",
    "\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    env.reset()\n",
    "    UEs = {}\n",
    "    sumReward = 0\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Time: Sat Sep 11 14:49:58 2021 --Episode: 0   Index: 0   Loss: 0.0   Reward: tensor(0., dtype=torch.float64)\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0\n",
      "\n",
      "Time: Sat Sep 11 14:50:46 2021 --Episode: 0   Index: 10000   Loss: 1.1372811059118249e-05   Reward: tensor(0.0654, dtype=torch.float64)\n",
      "UEHitrate: 0.0098990100989901  edgeHitrate 0.1421857814218578 sumHitrate 0.15208479152084792\n",
      "\n",
      "Time: Sat Sep 11 14:51:35 2021 --Episode: 0   Index: 20000   Loss: 8.50613560032798e-06   Reward: tensor(0.0982, dtype=torch.float64)\n",
      "UEHitrate: 0.01239938003099845  edgeHitrate 0.2136393180340983 sumHitrate 0.22603869806509674\n",
      "\n",
      "Time: Sat Sep 11 14:52:25 2021 --Episode: 0   Index: 30000   Loss: 6.514484539366094e-06   Reward: tensor(0.1200, dtype=torch.float64)\n",
      "UEHitrate: 0.014599513349555015  edgeHitrate 0.2619579347355088 sumHitrate 0.27655744808506383\n",
      "\n",
      "Time: Sat Sep 11 14:53:17 2021 --Episode: 0   Index: 40000   Loss: 5.192690423427848e-06   Reward: tensor(0.1354, dtype=torch.float64)\n",
      "UEHitrate: 0.015924601884952875  edgeHitrate 0.2959926001849954 sumHitrate 0.3119172020699483\n",
      "\n",
      "Time: Sat Sep 11 14:54:11 2021 --Episode: 0   Index: 50000   Loss: 4.291410732548684e-06   Reward: tensor(0.1426, dtype=torch.float64)\n",
      "UEHitrate: 0.016599668006639867  edgeHitrate 0.3120537589248215 sumHitrate 0.3286534269314614\n",
      "\n",
      "Time: Sat Sep 11 14:55:12 2021 --Episode: 0   Index: 60000   Loss: 3.656761919046403e-06   Reward: tensor(0.1480, dtype=torch.float64)\n",
      "UEHitrate: 0.016549724171263814  edgeHitrate 0.3245112581456976 sumHitrate 0.34106098231696136\n",
      "\n",
      "Time: Sat Sep 11 14:56:23 2021 --Episode: 0   Index: 70000   Loss: 3.192302301613381e-06   Reward: tensor(0.1577, dtype=torch.float64)\n",
      "UEHitrate: 0.01719975428922444  edgeHitrate 0.3459236296624334 sumHitrate 0.36312338395165783\n",
      "\n",
      "Time: Sat Sep 11 14:57:28 2021 --Episode: 0   Index: 80000   Loss: 2.8430602014850592e-06   Reward: tensor(0.1665, dtype=torch.float64)\n",
      "UEHitrate: 0.018162272971587855  edgeHitrate 0.3651079361507981 sumHitrate 0.38327020912238596\n",
      "\n",
      "Time: Sat Sep 11 14:58:33 2021 --Episode: 0   Index: 90000   Loss: 2.5717258722579572e-06   Reward: tensor(0.1723, dtype=torch.float64)\n",
      "UEHitrate: 0.01895534494061177  edgeHitrate 0.378073576960256 sumHitrate 0.39702892190086775\n",
      "\n",
      "Time: Sat Sep 11 14:59:27 2021 --Episode: 0   Index: 100000   Loss: 2.352019691898022e-06   Reward: tensor(0.1768, dtype=torch.float64)\n",
      "UEHitrate: 0.01978980210197898  edgeHitrate 0.3879461205387946 sumHitrate 0.4077359226407736\n",
      "\n",
      "Time: Sat Sep 11 15:00:20 2021 --Episode: 0   Index: 110000   Loss: 2.169501385651529e-06   Reward: tensor(0.1797, dtype=torch.float64)\n",
      "UEHitrate: 0.02044526867937564  edgeHitrate 0.3942600521813438 sumHitrate 0.41470532086071943\n",
      "\n",
      "Time: Sat Sep 11 15:01:13 2021 --Episode: 0   Index: 120000   Loss: 2.0155134734523017e-06   Reward: tensor(0.1820, dtype=torch.float64)\n",
      "UEHitrate: 0.02064982791810068  edgeHitrate 0.399405004958292 sumHitrate 0.42005483287639267\n",
      "\n",
      "Time: Sat Sep 11 15:02:09 2021 --Episode: 0   Index: 130000   Loss: 1.883698018900759e-06   Reward: tensor(0.1852, dtype=torch.float64)\n",
      "UEHitrate: 0.020938300474611735  edgeHitrate 0.4065737955861878 sumHitrate 0.42751209606079954\n",
      "\n",
      "Time: Sat Sep 11 15:03:08 2021 --Episode: 0   Index: 140000   Loss: 1.7703953290038044e-06   Reward: tensor(0.1865, dtype=torch.float64)\n",
      "UEHitrate: 0.02107842086842237  edgeHitrate 0.4093185048678224 sumHitrate 0.4303969257362447\n",
      "\n",
      "Time: Sat Sep 11 15:04:14 2021 --Episode: 0   Index: 150000   Loss: 1.6720590565455495e-06   Reward: tensor(0.1878, dtype=torch.float64)\n",
      "UEHitrate: 0.021493190045399697  edgeHitrate 0.41233725108499275 sumHitrate 0.4338304411303925\n",
      "\n",
      "Time: Sat Sep 11 15:05:17 2021 --Episode: 0   Index: 160000   Loss: 1.5861229485381045e-06   Reward: tensor(0.1899, dtype=torch.float64)\n",
      "UEHitrate: 0.022081111993050042  edgeHitrate 0.4170286435709777 sumHitrate 0.4391097555640277\n",
      "\n",
      "Time: Sat Sep 11 15:06:19 2021 --Episode: 0   Index: 170000   Loss: 1.5097854202394956e-06   Reward: tensor(0.1909, dtype=torch.float64)\n",
      "UEHitrate: 0.02245869141946224  edgeHitrate 0.419097534720384 sumHitrate 0.44155622613984624\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Sat Sep 11 15:06:33 2021 --End episode: 0   Loss: tensor(1.4950e-06, device='cuda:0', grad_fn=<DivBackward0>)   Reward: tensor(0.1912, dtype=torch.float64)\n",
      "UEHitrate: 0.022581940266374962  edgeHitrate 0.41988677027632476 sumHitrate 0.4424687105426997\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Sat Sep 11 15:06:34 2021 --Episode: 1   Index: 0   Loss: 0.00010659318650141358   Reward: tensor(0., dtype=torch.float64)\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0\n",
      "\n",
      "Time: Sat Sep 11 15:07:45 2021 --Episode: 1   Index: 10000   Loss: 9.33835565319896e-07   Reward: tensor(0.0796, dtype=torch.float64)\n",
      "UEHitrate: 0.010998900109989001  edgeHitrate 0.17228277172282772 sumHitrate 0.1832816718328167\n",
      "\n",
      "Time: Sat Sep 11 15:08:53 2021 --Episode: 1   Index: 20000   Loss: 1.0922631190624088e-06   Reward: tensor(0.0806, dtype=torch.float64)\n",
      "UEHitrate: 0.013249337533123344  edgeHitrate 0.17374131293435327 sumHitrate 0.18699065046747662\n",
      "\n",
      "Time: Sat Sep 11 15:10:12 2021 --Episode: 1   Index: 30000   Loss: 9.215577279064746e-07   Reward: tensor(0.0968, dtype=torch.float64)\n",
      "UEHitrate: 0.018099396686777108  edgeHitrate 0.20745975134162195 sumHitrate 0.22555914802839905\n",
      "\n",
      "Time: Sat Sep 11 15:11:45 2021 --Episode: 1   Index: 40000   Loss: 7.990383892320096e-07   Reward: tensor(0.1089, dtype=torch.float64)\n",
      "UEHitrate: 0.02007449813754656  edgeHitrate 0.23311917202069948 sumHitrate 0.25319367015824606\n",
      "\n",
      "Time: Sat Sep 11 15:13:17 2021 --Episode: 1   Index: 50000   Loss: 7.047498797874141e-07   Reward: tensor(0.1083, dtype=torch.float64)\n",
      "UEHitrate: 0.01867962640747185  edgeHitrate 0.23251534969300613 sumHitrate 0.251194976100478\n",
      "\n",
      "Time: Sat Sep 11 15:14:39 2021 --Episode: 1   Index: 60000   Loss: 6.364943487824348e-07   Reward: tensor(0.1082, dtype=torch.float64)\n",
      "UEHitrate: 0.017399710004833254  edgeHitrate 0.23304611589806837 sumHitrate 0.2504458259029016\n",
      "\n",
      "Time: Sat Sep 11 15:16:06 2021 --Episode: 1   Index: 70000   Loss: 5.869816277481732e-07   Reward: tensor(0.1088, dtype=torch.float64)\n",
      "UEHitrate: 0.01657119184011657  edgeHitrate 0.2347966457622034 sumHitrate 0.25136783760231995\n",
      "\n",
      "Time: Sat Sep 11 15:17:29 2021 --Episode: 1   Index: 80000   Loss: 5.51909522528149e-07   Reward: tensor(0.1095, dtype=torch.float64)\n",
      "UEHitrate: 0.016099798752515593  edgeHitrate 0.23659704253696828 sumHitrate 0.2526968412894839\n",
      "\n",
      "Time: Sat Sep 11 15:19:02 2021 --Episode: 1   Index: 90000   Loss: 5.2471006029009e-07   Reward: tensor(0.1087, dtype=torch.float64)\n",
      "UEHitrate: 0.01581093543405073  edgeHitrate 0.23540849546116155 sumHitrate 0.25121943089521226\n",
      "\n",
      "Time: Sat Sep 11 15:20:39 2021 --Episode: 1   Index: 100000   Loss: 5.012880706090073e-07   Reward: tensor(0.1083, dtype=torch.float64)\n",
      "UEHitrate: 0.015679843201567983  edgeHitrate 0.23491765082349175 sumHitrate 0.25059749402505976\n",
      "\n",
      "Time: Sat Sep 11 15:22:22 2021 --Episode: 1   Index: 110000   Loss: 4.841149916501308e-07   Reward: tensor(0.1082, dtype=torch.float64)\n",
      "UEHitrate: 0.015708948100471815  edgeHitrate 0.23464332142435068 sumHitrate 0.2503522695248225\n",
      "\n",
      "Time: Sat Sep 11 15:24:15 2021 --Episode: 1   Index: 120000   Loss: 4.704134823896311e-07   Reward: tensor(0.1084, dtype=torch.float64)\n",
      "UEHitrate: 0.015541537153857051  edgeHitrate 0.2352730393913384 sumHitrate 0.25081457654519546\n",
      "\n",
      "Time: Sat Sep 11 15:25:43 2021 --Episode: 1   Index: 130000   Loss: 4.594343181452132e-07   Reward: tensor(0.1095, dtype=torch.float64)\n",
      "UEHitrate: 0.015307574557118791  edgeHitrate 0.2381212452211906 sumHitrate 0.2534288197783094\n",
      "\n",
      "Time: Sat Sep 11 15:27:24 2021 --Episode: 1   Index: 140000   Loss: 4.512857856298069e-07   Reward: tensor(0.1099, dtype=torch.float64)\n",
      "UEHitrate: 0.015071320919136292  edgeHitrate 0.2392411482775123 sumHitrate 0.25431246919664857\n",
      "\n",
      "Time: Sat Sep 11 15:28:56 2021 --Episode: 1   Index: 150000   Loss: 4.454209374671336e-07   Reward: tensor(0.1108, dtype=torch.float64)\n",
      "UEHitrate: 0.014846567689548736  edgeHitrate 0.2412583916107226 sumHitrate 0.25610495930027133\n",
      "\n",
      "Time: Sat Sep 11 15:30:24 2021 --Episode: 1   Index: 160000   Loss: 4.405688400765939e-07   Reward: tensor(0.1121, dtype=torch.float64)\n",
      "UEHitrate: 0.015062405859963376  edgeHitrate 0.24416722395485027 sumHitrate 0.2592296298148137\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}