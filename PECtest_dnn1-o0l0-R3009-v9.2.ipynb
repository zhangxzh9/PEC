{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2 增加神经网络输出\n",
    "#v3 修改reward为直接变化量\n",
    "#v4 删除e,S特征 \n",
    "#v7 增加邻居节点特征\n",
    "#V8 修改时序上bug\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_o0l0_v9.2_'\n",
    "\n",
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=np.int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0,\"betal\":0}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    #expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        #self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        statusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        if train and len(self.W)>1: #d\n",
    "            lastAction = np.row_stack((1-self.lastAction,self.lastAction)).T\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature.to(device), \n",
    "                        torch.from_numpy(lastAction).to(device), \n",
    "                        statusFeature.to(device),\n",
    "                        torch.tensor([self.reward]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "        self.lastStatusFeature = statusFeature\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            #eps_threshold = 0\n",
    "            if  not train or (train and sample >= eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                #with torch.no_grad():\n",
    "                Q_value = QNetwork(statusFeature).detach()\n",
    "                QNetwork.train()\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                \n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "            \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 03:20:28 2021 Episode: 0   Index: 198174   Loss: 0.00145 --\n",
      "Reward: [0.55028 0.      0.     ] total reward: 0.55028\n",
      "UEHitrate: 0.00816  edgeHitrate 0.07271 sumHitrate 0.08087  privacy: 1.06567\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 03:40:25 2021 Episode: 0   Index: 253629   Loss: 0.0 --\n",
      "Reward: [1.26365 0.      0.     ] total reward: 1.26365\n",
      "UEHitrate: 0.0064  edgeHitrate 0.06489 sumHitrate 0.07128  privacy: 0.30916\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v9.2_ep0_1016-03-40-25\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 05:36:25 2021 Episode: 1   Index: 198174   Loss: 0.00111 --\n",
      "Reward: [0.478 0.    0.   ] total reward: 0.478\n",
      "UEHitrate: 0.02309  edgeHitrate 0.12825 sumHitrate 0.15134  privacy: 1.39746\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 05:49:34 2021 Episode: 1   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.60871 0.      0.     ] total reward: 0.60871\n",
      "UEHitrate: 0.00822  edgeHitrate 0.21452 sumHitrate 0.22274  privacy: 1.56582\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 07:11:29 2021 Episode: 2   Index: 198174   Loss: 0.00152 --\n",
      "Reward: [0.43606 0.      0.     ] total reward: 0.43606\n",
      "UEHitrate: 0.01636  edgeHitrate 0.1341 sumHitrate 0.15046  privacy: 1.67119\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 07:24:52 2021 Episode: 2   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.70206 0.      0.     ] total reward: 0.70206\n",
      "UEHitrate: 0.008  edgeHitrate 0.11874 sumHitrate 0.12674  privacy: 1.30241\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 08:49:06 2021 Episode: 3   Index: 198174   Loss: 0.00179 --\n",
      "Reward: [0.41219 0.      0.     ] total reward: 0.41219\n",
      "UEHitrate: 0.02748  edgeHitrate 0.19269 sumHitrate 0.22017  privacy: 1.88851\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 09:03:01 2021 Episode: 3   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.8295 0.     0.    ] total reward: 0.8295\n",
      "UEHitrate: 0.03835  edgeHitrate 0.39812 sumHitrate 0.43647  privacy: 1.08352\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 10:30:26 2021 Episode: 4   Index: 198174   Loss: 0.00194 --\n",
      "Reward: [0.40167 0.      0.     ] total reward: 0.40167\n",
      "UEHitrate: 0.02482  edgeHitrate 0.17744 sumHitrate 0.20226  privacy: 2.03267\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 10:44:52 2021 Episode: 4   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.73924 0.      0.     ] total reward: 0.73924\n",
      "UEHitrate: 0.02358  edgeHitrate 0.19165 sumHitrate 0.21523  privacy: 1.23749\n",
      "------------------------------validation---------------------------#-------\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-50b93593d513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0medgeHit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0msumReward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-88420545dea5>\u001b[0m in \u001b[0;36mselectAction\u001b[0;34m(self, env, uit, QNetwork, train, memory)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontentNum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactionIndex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0mactionIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactionIndex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(4000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if (index+1) % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sat Oct 16 12:05:07 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [0.32716 0.      0.     ] total reward: 0.32716\n",
      "UEHitrate: 0.0071  edgeHitrate 0.1598 sumHitrate 0.1669  privacy: 2.79406\n",
      "\n",
      "--Time: Sat Oct 16 12:05:38 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [0.34111 0.      0.     ] total reward: 0.34111\n",
      "UEHitrate: 0.00655  edgeHitrate 0.12255 sumHitrate 0.1291  privacy: 2.55752\n",
      "\n",
      "--Time: Sat Oct 16 12:06:09 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [0.35702 0.      0.     ] total reward: 0.35702\n",
      "UEHitrate: 0.00653  edgeHitrate 0.10217 sumHitrate 0.1087  privacy: 2.40975\n",
      "\n",
      "--Time: Sat Oct 16 12:06:39 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [0.36977 0.      0.     ] total reward: 0.36977\n",
      "UEHitrate: 0.00625  edgeHitrate 0.09292 sumHitrate 0.09918  privacy: 2.29733\n",
      "\n",
      "--Time: Sat Oct 16 12:07:10 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [0.37586 0.      0.     ] total reward: 0.37586\n",
      "UEHitrate: 0.00622  edgeHitrate 0.0853 sumHitrate 0.09152  privacy: 2.18294\n",
      "\n",
      "--Time: Sat Oct 16 12:07:41 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [0.38635 0.      0.     ] total reward: 0.38635\n",
      "UEHitrate: 0.00603  edgeHitrate 0.07968 sumHitrate 0.08572  privacy: 2.03409\n",
      "\n",
      "--Time: Sat Oct 16 12:08:12 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [0.39833 0.      0.     ] total reward: 0.39833\n",
      "UEHitrate: 0.00573  edgeHitrate 0.07821 sumHitrate 0.08394  privacy: 1.85779\n",
      "\n",
      "--Time: Sat Oct 16 12:08:42 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [0.41401 0.      0.     ] total reward: 0.41401\n",
      "UEHitrate: 0.00569  edgeHitrate 0.08081 sumHitrate 0.0865  privacy: 1.69597\n",
      "\n",
      "--Time: Sat Oct 16 12:09:13 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [0.43311 0.      0.     ] total reward: 0.43311\n",
      "UEHitrate: 0.00573  edgeHitrate 0.08177 sumHitrate 0.0875  privacy: 1.5334\n",
      "\n",
      "--Time: Sat Oct 16 12:09:45 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [0.45795 0.      0.     ] total reward: 0.45795\n",
      "UEHitrate: 0.00564  edgeHitrate 0.08034 sumHitrate 0.08598  privacy: 1.37975\n",
      "\n",
      "--Time: Sat Oct 16 12:10:18 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [0.48512 0.      0.     ] total reward: 0.48512\n",
      "UEHitrate: 0.00576  edgeHitrate 0.07774 sumHitrate 0.0835  privacy: 1.25223\n",
      "\n",
      "--Time: Sat Oct 16 12:10:50 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [0.51342 0.      0.     ] total reward: 0.51342\n",
      "UEHitrate: 0.00578  edgeHitrate 0.07515 sumHitrate 0.08093  privacy: 1.15246\n",
      "\n",
      "--Time: Sat Oct 16 12:11:23 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [0.54208 0.      0.     ] total reward: 0.54208\n",
      "UEHitrate: 0.00593  edgeHitrate 0.07258 sumHitrate 0.07851  privacy: 1.0682\n",
      "\n",
      "--Time: Sat Oct 16 12:11:55 2021   Index: 139999   Loss: 0.0 --\n",
      "Reward: [0.57252 0.      0.     ] total reward: 0.57252\n",
      "UEHitrate: 0.00601  edgeHitrate 0.07092 sumHitrate 0.07694  privacy: 0.98457\n",
      "\n",
      "--Time: Sat Oct 16 12:12:27 2021   Index: 149999   Loss: 0.0 --\n",
      "Reward: [0.60499 0.      0.     ] total reward: 0.60499\n",
      "UEHitrate: 0.00603  edgeHitrate 0.06918 sumHitrate 0.07521  privacy: 0.90721\n",
      "\n",
      "--Time: Sat Oct 16 12:12:59 2021   Index: 159999   Loss: 0.0 --\n",
      "Reward: [0.64024 0.      0.     ] total reward: 0.64024\n",
      "UEHitrate: 0.00615  edgeHitrate 0.06796 sumHitrate 0.07411  privacy: 0.83101\n",
      "\n",
      "--Time: Sat Oct 16 12:13:30 2021   Index: 169999   Loss: 0.0 --\n",
      "Reward: [0.67931 0.      0.     ] total reward: 0.67931\n",
      "UEHitrate: 0.00618  edgeHitrate 0.06709 sumHitrate 0.07327  privacy: 0.76304\n",
      "\n",
      "--Time: Sat Oct 16 12:14:01 2021   Index: 179999   Loss: 0.0 --\n",
      "Reward: [0.72074 0.      0.     ] total reward: 0.72074\n",
      "UEHitrate: 0.00624  edgeHitrate 0.06698 sumHitrate 0.07322  privacy: 0.69506\n",
      "\n",
      "--Time: Sat Oct 16 12:14:32 2021   Index: 189999   Loss: 0.0 --\n",
      "Reward: [0.76675 0.      0.     ] total reward: 0.76675\n",
      "UEHitrate: 0.00625  edgeHitrate 0.06616 sumHitrate 0.07241  privacy: 0.62555\n",
      "\n",
      "--Time: Sat Oct 16 12:15:03 2021   Index: 199999   Loss: 0.0 --\n",
      "Reward: [0.82244 0.      0.     ] total reward: 0.82244\n",
      "UEHitrate: 0.00627  edgeHitrate 0.06589 sumHitrate 0.07217  privacy: 0.56705\n",
      "\n",
      "--Time: Sat Oct 16 12:15:36 2021   Index: 209999   Loss: 0.0 --\n",
      "Reward: [0.88232 0.      0.     ] total reward: 0.88232\n",
      "UEHitrate: 0.00634  edgeHitrate 0.06621 sumHitrate 0.07256  privacy: 0.50963\n",
      "\n",
      "--Time: Sat Oct 16 12:16:10 2021   Index: 219999   Loss: 0.0 --\n",
      "Reward: [0.9559 0.     0.    ] total reward: 0.9559\n",
      "UEHitrate: 0.0064  edgeHitrate 0.06615 sumHitrate 0.07255  privacy: 0.45996\n",
      "\n",
      "--Time: Sat Oct 16 12:16:45 2021   Index: 229999   Loss: 0.0 --\n",
      "Reward: [1.03177 0.      0.     ] total reward: 1.03177\n",
      "UEHitrate: 0.00641  edgeHitrate 0.06574 sumHitrate 0.07216  privacy: 0.40611\n",
      "\n",
      "--Time: Sat Oct 16 12:17:20 2021   Index: 239999   Loss: 0.0 --\n",
      "Reward: [1.12126 0.      0.     ] total reward: 1.12126\n",
      "UEHitrate: 0.00642  edgeHitrate 0.06522 sumHitrate 0.07163  privacy: 0.35996\n",
      "\n",
      "--Time: Sat Oct 16 12:17:54 2021   Index: 249999   Loss: 0.0 --\n",
      "Reward: [1.22243 0.      0.     ] total reward: 1.22243\n",
      "UEHitrate: 0.00642  edgeHitrate 0.0651 sumHitrate 0.07152  privacy: 0.32064\n",
      "\n",
      "--Time: Sat Oct 16 12:18:29 2021   Index: 259999   Loss: 0.0 --\n",
      "Reward: [1.33998 0.      0.     ] total reward: 1.33998\n",
      "UEHitrate: 0.00641  edgeHitrate 0.06431 sumHitrate 0.07072  privacy: 0.29168\n",
      "\n",
      "--Time: Sat Oct 16 12:19:03 2021   Index: 269999   Loss: 0.0 --\n",
      "Reward: [1.46407 0.      0.     ] total reward: 1.46407\n",
      "UEHitrate: 0.00646  edgeHitrate 0.06346 sumHitrate 0.06992  privacy: 0.2698\n",
      "\n",
      "--Time: Sat Oct 16 12:19:38 2021   Index: 279999   Loss: 0.0 --\n",
      "Reward: [1.58874 0.      0.     ] total reward: 1.58874\n",
      "UEHitrate: 0.00649  edgeHitrate 0.06306 sumHitrate 0.06955  privacy: 0.2486\n",
      "\n",
      "--Time: Sat Oct 16 12:20:13 2021   Index: 289999   Loss: 0.0 --\n",
      "Reward: [1.7128 0.     0.    ] total reward: 1.7128\n",
      "UEHitrate: 0.00648  edgeHitrate 0.06263 sumHitrate 0.06911  privacy: 0.23059\n",
      "\n",
      "--Time: Sat Oct 16 12:20:49 2021   Index: 299999   Loss: 0.0 --\n",
      "Reward: [1.84455 0.      0.     ] total reward: 1.84455\n",
      "UEHitrate: 0.00645  edgeHitrate 0.06203 sumHitrate 0.06847  privacy: 0.21846\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sat Oct 16 12:20:53 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [1.859 0.    0.   ] total reward: 1.859\n",
      "UEHitrate: 0.00644  edgeHitrate 0.062 sumHitrate 0.06844  privacy: 0.2174\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.1669 , 0.1291 , 0.1087 , 0.09918, 0.09152, 0.08572, 0.08394,\n",
       "        0.0865 , 0.0875 , 0.08598, 0.0835 , 0.08093, 0.07851, 0.07694,\n",
       "        0.07521, 0.07411, 0.07327, 0.07322, 0.07241, 0.07217, 0.07256,\n",
       "        0.07255, 0.07216, 0.07163, 0.07152, 0.07072, 0.06992, 0.06955,\n",
       "        0.06911, 0.06847, 0.06844]),\n",
       " array([0.0071 , 0.00655, 0.00653, 0.00625, 0.00622, 0.00603, 0.00573,\n",
       "        0.00569, 0.00573, 0.00564, 0.00576, 0.00578, 0.00593, 0.00601,\n",
       "        0.00603, 0.00615, 0.00618, 0.00624, 0.00625, 0.00627, 0.00634,\n",
       "        0.0064 , 0.00641, 0.00642, 0.00642, 0.00641, 0.00646, 0.00649,\n",
       "        0.00648, 0.00645, 0.00644]),\n",
       " array([0.1598 , 0.12255, 0.10217, 0.09292, 0.0853 , 0.07968, 0.07821,\n",
       "        0.08081, 0.08177, 0.08034, 0.07774, 0.07515, 0.07258, 0.07092,\n",
       "        0.06918, 0.06796, 0.06709, 0.06698, 0.06616, 0.06589, 0.06621,\n",
       "        0.06615, 0.06574, 0.06522, 0.0651 , 0.06431, 0.06346, 0.06306,\n",
       "        0.06263, 0.06203, 0.062  ]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.79406, 2.55752, 2.40975, 2.29733, 2.18294, 2.03409, 1.85779,\n",
       "       1.69597, 1.5334 , 1.37975, 1.25223, 1.15246, 1.0682 , 0.98457,\n",
       "       0.90721, 0.83101, 0.76304, 0.69506, 0.62555, 0.56705, 0.50963,\n",
       "       0.45996, 0.40611, 0.35996, 0.32064, 0.29168, 0.2698 , 0.2486 ,\n",
       "       0.23059, 0.21846, 0.2174 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v9.2_ep0_1016-03-40-25'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a71a010a29be3df77e49d8f42909021441fd02d28ccbb313c9d0d3f90d3253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
