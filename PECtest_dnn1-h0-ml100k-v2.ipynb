{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1 修改时序上bug\n",
    "#v2 增加边缘特征\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_h0_v2.0_'\n",
    "\n",
    "data_path = '/home/ubuntu/data/dataset/ml-100k/'\n",
    "UIT = pd.read_csv(data_path + 'ml_header.trace')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=np.int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":0,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    '''\n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    '''\n",
    "    #next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    #expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = self.statusEmbedding(r,p,e)\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        statusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        if train and len(self.W)>1: #d\n",
    "            lastAction = np.row_stack((1-self.lastAction,self.lastAction)).T\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature.to(device), \n",
    "                        torch.from_numpy(lastAction).to(device), \n",
    "                        statusFeature.to(device),\n",
    "                        torch.tensor([self.reward]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "        self.lastStatusFeature = statusFeature\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            #eps_threshold = 0\n",
    "            if  not train or (train and sample >= eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                #with torch.no_grad():\n",
    "                Q_value = QNetwork(statusFeature).detach()\n",
    "                QNetwork.train()\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                \n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "            \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 15:24:17 2021 Episode: 0   Index: 76251   Loss: 0.61888 --\n",
      "Reward: [0.      0.00062 0.05366] total reward: 0.05428\n",
      "UEHitrate: 0.00273  edgeHitrate 0.06708 sumHitrate 0.06981  privacy: 3.49424\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 15:31:04 2021 Episode: 0   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00095 0.0854 ] total reward: 0.08634\n",
      "UEHitrate: 0.00428  edgeHitrate 0.10674 sumHitrate 0.11102  privacy: 1.0904\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_h0_v2.0_ep0_1015-15-31-04\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 15:43:02 2021 Episode: 1   Index: 76251   Loss: 0.38959 --\n",
      "Reward: [0.      0.00051 0.06716] total reward: 0.06767\n",
      "UEHitrate: 0.00241  edgeHitrate 0.08395 sumHitrate 0.08636  privacy: 3.03863\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 15:50:57 2021 Episode: 1   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00088 0.09222] total reward: 0.0931\n",
      "UEHitrate: 0.00392  edgeHitrate 0.11528 sumHitrate 0.11919  privacy: 1.03573\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_h0_v2.0_ep1_1015-15-50-57\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 16:02:46 2021 Episode: 2   Index: 76251   Loss: 0.37803 --\n",
      "Reward: [0.      0.00062 0.0781 ] total reward: 0.07872\n",
      "UEHitrate: 0.00294  edgeHitrate 0.09762 sumHitrate 0.10056  privacy: 2.80336\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 16:09:04 2021 Episode: 2   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00102 0.108  ] total reward: 0.10902\n",
      "UEHitrate: 0.0047  edgeHitrate 0.135 sumHitrate 0.1397  privacy: 1.2098\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_h0_v2.0_ep2_1015-16-09-04\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 16:18:26 2021 Episode: 3   Index: 76251   Loss: 0.3641 --\n",
      "Reward: [0.      0.00071 0.08253] total reward: 0.08323\n",
      "UEHitrate: 0.00328  edgeHitrate 0.10316 sumHitrate 0.10644  privacy: 2.66153\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 16:25:20 2021 Episode: 3   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00181 0.10288] total reward: 0.10469\n",
      "UEHitrate: 0.00874  edgeHitrate 0.12861 sumHitrate 0.13734  privacy: 1.24703\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 16:34:35 2021 Episode: 4   Index: 76251   Loss: 0.35726 --\n",
      "Reward: [0.      0.00174 0.07895] total reward: 0.08069\n",
      "UEHitrate: 0.00837  edgeHitrate 0.09869 sumHitrate 0.10705  privacy: 2.43467\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 16:41:21 2021 Episode: 4   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00402 0.11715] total reward: 0.12117\n",
      "UEHitrate: 0.01972  edgeHitrate 0.14644 sumHitrate 0.16616  privacy: 1.24467\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_h0_v2.0_ep4_1015-16-41-21\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 16:50:38 2021 Episode: 5   Index: 76251   Loss: 0.35577 --\n",
      "Reward: [0.      0.00194 0.08227] total reward: 0.08422\n",
      "UEHitrate: 0.00961  edgeHitrate 0.10284 sumHitrate 0.11246  privacy: 2.27435\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 16:56:59 2021 Episode: 5   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00405 0.1275 ] total reward: 0.13155\n",
      "UEHitrate: 0.01978  edgeHitrate 0.15937 sumHitrate 0.17915  privacy: 1.29612\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_h0_v2.0_ep5_1015-16-56-59\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 17:06:25 2021 Episode: 6   Index: 76251   Loss: 0.35438 --\n",
      "Reward: [0.      0.00265 0.09038] total reward: 0.09304\n",
      "UEHitrate: 0.01279  edgeHitrate 0.11298 sumHitrate 0.12577  privacy: 2.10508\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 17:12:54 2021 Episode: 6   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.0055  0.14136] total reward: 0.14686\n",
      "UEHitrate: 0.02703  edgeHitrate 0.1767 sumHitrate 0.20373  privacy: 1.1697\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_h0_v2.0_ep6_1015-17-12-54\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 17:22:10 2021 Episode: 7   Index: 76251   Loss: 0.35311 --\n",
      "Reward: [0.      0.00352 0.08955] total reward: 0.09306\n",
      "UEHitrate: 0.01717  edgeHitrate 0.11193 sumHitrate 0.1291  privacy: 1.90233\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 17:28:44 2021 Episode: 7   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00609 0.13753] total reward: 0.14363\n",
      "UEHitrate: 0.02931  edgeHitrate 0.17192 sumHitrate 0.20123  privacy: 1.13201\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 17:37:51 2021 Episode: 8   Index: 76251   Loss: 0.35158 --\n",
      "Reward: [0.      0.00393 0.08952] total reward: 0.09346\n",
      "UEHitrate: 0.01903  edgeHitrate 0.11191 sumHitrate 0.13093  privacy: 1.84583\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 17:44:30 2021 Episode: 8   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00623 0.1311 ] total reward: 0.13733\n",
      "UEHitrate: 0.03028  edgeHitrate 0.16388 sumHitrate 0.19416  privacy: 1.10114\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 17:54:01 2021 Episode: 9   Index: 76251   Loss: 0.35003 --\n",
      "Reward: [0.      0.00441 0.10577] total reward: 0.11017\n",
      "UEHitrate: 0.02135  edgeHitrate 0.13221 sumHitrate 0.15356  privacy: 1.67906\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 18:00:50 2021 Episode: 9   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00609 0.13605] total reward: 0.14215\n",
      "UEHitrate: 0.02976  edgeHitrate 0.17007 sumHitrate 0.19982  privacy: 1.19286\n",
      "------------------------------validation---------------------------#-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(4000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if (index+1) % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        '''\n",
    "        if (index+1) % 1000 == 0 :\n",
    "            psi = 0\n",
    "            for u in UEs:\n",
    "                psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "        '''\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Fri Oct 15 18:01:29 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [0.      0.00056 0.07112] total reward: 0.07168\n",
      "UEHitrate: 0.0028  edgeHitrate 0.0889 sumHitrate 0.0917  privacy: 3.22091\n",
      "\n",
      "--Time: Fri Oct 15 18:02:04 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [0.      0.00064 0.06448] total reward: 0.06512\n",
      "UEHitrate: 0.0032  edgeHitrate 0.0806 sumHitrate 0.0838  privacy: 2.41957\n",
      "\n",
      "--Time: Fri Oct 15 18:02:39 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [0.      0.00073 0.07155] total reward: 0.07227\n",
      "UEHitrate: 0.0035  edgeHitrate 0.08943 sumHitrate 0.09293  privacy: 1.9142\n",
      "\n",
      "--Time: Fri Oct 15 18:03:15 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [0.      0.001   0.07196] total reward: 0.07296\n",
      "UEHitrate: 0.00478  edgeHitrate 0.08995 sumHitrate 0.09472  privacy: 1.60372\n",
      "\n",
      "--Time: Fri Oct 15 18:03:52 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [0.      0.00194 0.07787] total reward: 0.07981\n",
      "UEHitrate: 0.00938  edgeHitrate 0.09734 sumHitrate 0.10672  privacy: 1.35976\n",
      "\n",
      "--Time: Fri Oct 15 18:04:28 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [0.      0.00286 0.08673] total reward: 0.08959\n",
      "UEHitrate: 0.01383  edgeHitrate 0.10842 sumHitrate 0.12225  privacy: 1.25884\n",
      "\n",
      "--Time: Fri Oct 15 18:05:04 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [0.      0.00373 0.09621] total reward: 0.09994\n",
      "UEHitrate: 0.01827  edgeHitrate 0.12026 sumHitrate 0.13853  privacy: 1.22401\n",
      "\n",
      "--Time: Fri Oct 15 18:05:41 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [0.      0.00442 0.10598] total reward: 0.1104\n",
      "UEHitrate: 0.02161  edgeHitrate 0.13248 sumHitrate 0.15409  privacy: 1.20925\n",
      "\n",
      "--Time: Fri Oct 15 18:06:19 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [0.      0.00493 0.12404] total reward: 0.12898\n",
      "UEHitrate: 0.02418  edgeHitrate 0.15506 sumHitrate 0.17923  privacy: 1.19206\n",
      "\n",
      "--Time: Fri Oct 15 18:06:56 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [0.      0.00524 0.13238] total reward: 0.13761\n",
      "UEHitrate: 0.0257  edgeHitrate 0.16547 sumHitrate 0.19117  privacy: 1.18018\n",
      "\n",
      "--Time: Fri Oct 15 18:07:34 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [0.      0.00552 0.14588] total reward: 0.15139\n",
      "UEHitrate: 0.02715  edgeHitrate 0.18235 sumHitrate 0.20949  privacy: 1.16331\n",
      "\n",
      "--Time: Fri Oct 15 18:08:11 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [0.      0.00562 0.15294] total reward: 0.15856\n",
      "UEHitrate: 0.02756  edgeHitrate 0.19118 sumHitrate 0.21873  privacy: 1.14609\n",
      "\n",
      "--Time: Fri Oct 15 18:08:46 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [0.      0.00564 0.1549 ] total reward: 0.16054\n",
      "UEHitrate: 0.02765  edgeHitrate 0.19363 sumHitrate 0.22128  privacy: 1.1307\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Fri Oct 15 18:08:56 2021   Index: 132689   Loss: 0.0 --\n",
      "Reward: [0.      0.00564 0.15658] total reward: 0.16222\n",
      "UEHitrate: 0.02764  edgeHitrate 0.19573 sumHitrate 0.22337  privacy: 1.12596\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5 , 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0917 , 0.0838 , 0.09293, 0.09472, 0.10672, 0.12225, 0.13853,\n",
       "        0.15409, 0.17923, 0.19117, 0.20949, 0.21873, 0.22128, 0.22337]),\n",
       " array([0.0028 , 0.0032 , 0.0035 , 0.00478, 0.00938, 0.01383, 0.01827,\n",
       "        0.02161, 0.02418, 0.0257 , 0.02715, 0.02756, 0.02765, 0.02764]),\n",
       " array([0.0889 , 0.0806 , 0.08943, 0.08995, 0.09734, 0.10842, 0.12026,\n",
       "        0.13248, 0.15506, 0.16547, 0.18235, 0.19118, 0.19363, 0.19573]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.22091, 2.41957, 1.9142 , 1.60372, 1.35976, 1.25884, 1.22401,\n",
       "       1.20925, 1.19206, 1.18018, 1.16331, 1.14609, 1.1307 , 1.12596])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    " \n",
    "def sigmoid(x):\n",
    "    # 直接返回sigmoid函数\n",
    "    return 1. / (1. + np.exp(10*x))\n",
    " \n",
    " \n",
    "def plot_sigmoid():\n",
    "    # param:起点，终点，间距\n",
    "    x = np.arange(-8, 8, 0.2)\n",
    "    y = sigmoid(x)\n",
    "    plt.plot(x, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a71a010a29be3df77e49d8f42909021441fd02d28ccbb313c9d0d3f90d3253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
