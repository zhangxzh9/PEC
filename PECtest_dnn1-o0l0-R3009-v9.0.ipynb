{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2 增加神经网络输出\n",
    "#v3 修改reward为直接变化量\n",
    "#v4 删除e,S特征 \n",
    "#v7 增加邻居节点特征\n",
    "#v8 修改时序上bug\n",
    "#v9 长期reward\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_o0l0_v9.0_'\n",
    "\n",
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=np.int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0,\"betal\":0}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    #expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        statusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        if train and len(self.W)>1: #d\n",
    "            lastAction = np.row_stack((1-self.lastAction,self.lastAction)).T\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature.to(device), \n",
    "                        torch.from_numpy(lastAction).to(device), \n",
    "                        statusFeature.to(device),\n",
    "                        torch.tensor([self.reward]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "        self.lastStatusFeature = statusFeature\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            #eps_threshold = 0\n",
    "            if  not train or (train and sample >= eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                #with torch.no_grad():\n",
    "                Q_value = QNetwork(statusFeature).detach()\n",
    "                QNetwork.train()\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                \n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "            \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 03:12:02 2021 Episode: 0   Index: 198174   Loss: 0.77557 --\n",
      "Reward: [-15.74533   0.        0.     ] total reward: -15.74533\n",
      "UEHitrate: 0.0164  edgeHitrate 0.13025 sumHitrate 0.14665  privacy: 1.13637\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 03:32:27 2021 Episode: 0   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.73373  0.       0.     ] total reward: -1.73373\n",
      "UEHitrate: 0.01384  edgeHitrate 0.39771 sumHitrate 0.41154  privacy: 0.87755\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v9.0_ep0_1016-03-32-27\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 05:39:06 2021 Episode: 1   Index: 198174   Loss: 0.51851 --\n",
      "Reward: [-15.20577   0.        0.     ] total reward: -15.20577\n",
      "UEHitrate: 0.02429  edgeHitrate 0.14588 sumHitrate 0.17016  privacy: 1.43113\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 05:51:34 2021 Episode: 1   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.85605  0.       0.     ] total reward: -1.85605\n",
      "UEHitrate: 0.04318  edgeHitrate 0.17412 sumHitrate 0.2173  privacy: 1.04577\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 07:18:33 2021 Episode: 2   Index: 198174   Loss: 0.48448 --\n",
      "Reward: [-14.05766   0.        0.     ] total reward: -14.05766\n",
      "UEHitrate: 0.01334  edgeHitrate 0.10865 sumHitrate 0.12199  privacy: 1.68732\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 07:31:22 2021 Episode: 2   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.74211  0.       0.     ] total reward: -1.74211\n",
      "UEHitrate: 0.07875  edgeHitrate 0.33764 sumHitrate 0.41639  privacy: 0.93743\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 08:58:19 2021 Episode: 3   Index: 198174   Loss: 0.47126 --\n",
      "Reward: [-12.24129   0.        0.     ] total reward: -12.24129\n",
      "UEHitrate: 0.03776  edgeHitrate 0.21295 sumHitrate 0.25071  privacy: 1.82035\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 09:11:31 2021 Episode: 3   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.78877  0.       0.     ] total reward: -1.78877\n",
      "UEHitrate: 0.05849  edgeHitrate 0.38596 sumHitrate 0.44445  privacy: 0.97812\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 10:40:50 2021 Episode: 4   Index: 198174   Loss: 0.63923 --\n",
      "Reward: [-10.82927   0.        0.     ] total reward: -10.82927\n",
      "UEHitrate: 0.02657  edgeHitrate 0.16572 sumHitrate 0.19228  privacy: 1.9972\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 10:54:20 2021 Episode: 4   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.73413  0.       0.     ] total reward: -1.73413\n",
      "UEHitrate: 0.05768  edgeHitrate 0.36756 sumHitrate 0.42524  privacy: 0.8777\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 12:30:55 2021 Episode: 5   Index: 198174   Loss: 1.15695 --\n",
      "Reward: [-9.1969  0.      0.    ] total reward: -9.1969\n",
      "UEHitrate: 0.03725  edgeHitrate 0.19087 sumHitrate 0.22813  privacy: 2.07806\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 12:44:02 2021 Episode: 5   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.73361  0.       0.     ] total reward: -1.73361\n",
      "UEHitrate: 0.00584  edgeHitrate 0.37807 sumHitrate 0.38391  privacy: 0.87752\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v9.0_ep5_1016-12-44-02\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 14:14:21 2021 Episode: 6   Index: 198174   Loss: 1.44551 --\n",
      "Reward: [-7.87567  0.       0.     ] total reward: -7.87567\n",
      "UEHitrate: 0.02061  edgeHitrate 0.15886 sumHitrate 0.17947  privacy: 2.05704\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 14:27:36 2021 Episode: 6   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.73333  0.       0.     ] total reward: -1.73333\n",
      "UEHitrate: 0.05524  edgeHitrate 0.20925 sumHitrate 0.26449  privacy: 0.87678\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v9.0_ep6_1016-14-27-36\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 15:58:12 2021 Episode: 7   Index: 198174   Loss: 1.75456 --\n",
      "Reward: [-6.78263  0.       0.     ] total reward: -6.78263\n",
      "UEHitrate: 0.00673  edgeHitrate 0.18602 sumHitrate 0.19275  privacy: 2.00646\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 16:11:56 2021 Episode: 7   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.84573  0.       0.     ] total reward: -1.84573\n",
      "UEHitrate: 0.04767  edgeHitrate 0.1916 sumHitrate 0.23927  privacy: 1.05106\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 17:42:07 2021 Episode: 8   Index: 198174   Loss: 1.95637 --\n",
      "Reward: [-5.91611  0.       0.     ] total reward: -5.91611\n",
      "UEHitrate: 0.00765  edgeHitrate 0.19426 sumHitrate 0.20191  privacy: 1.96171\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 17:56:12 2021 Episode: 8   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.84784  0.       0.     ] total reward: -1.84784\n",
      "UEHitrate: 0.03886  edgeHitrate 0.28293 sumHitrate 0.32179  privacy: 1.047\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 19:24:12 2021 Episode: 9   Index: 198174   Loss: 1.85079 --\n",
      "Reward: [-5.07913  0.       0.     ] total reward: -5.07913\n",
      "UEHitrate: 0.07026  edgeHitrate 0.24946 sumHitrate 0.31972  privacy: 1.96234\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 19:37:25 2021 Episode: 9   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.73389  0.       0.     ] total reward: -1.73389\n",
      "UEHitrate: 0.07224  edgeHitrate 0.34935 sumHitrate 0.42159  privacy: 0.87765\n",
      "------------------------------validation---------------------------#-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(4000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if (index+1) % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        '''\n",
    "        if (index+1) % 1000 == 0 :\n",
    "            psi = 0\n",
    "            for u in UEs:\n",
    "                psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "        '''\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sat Oct 16 19:38:01 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-4.31931  0.       0.     ] total reward: -4.31931\n",
      "UEHitrate: 0.0192  edgeHitrate 0.3227 sumHitrate 0.3419  privacy: 0.88683\n",
      "\n",
      "--Time: Sat Oct 16 19:38:34 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-3.62384  0.       0.     ] total reward: -3.62384\n",
      "UEHitrate: 0.0397  edgeHitrate 0.31235 sumHitrate 0.35205  privacy: 0.87111\n",
      "\n",
      "--Time: Sat Oct 16 19:39:06 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-3.37746  0.       0.     ] total reward: -3.37746\n",
      "UEHitrate: 0.05123  edgeHitrate 0.28063 sumHitrate 0.33187  privacy: 0.87353\n",
      "\n",
      "--Time: Sat Oct 16 19:39:37 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-3.19898  0.       0.     ] total reward: -3.19898\n",
      "UEHitrate: 0.05665  edgeHitrate 0.26605 sumHitrate 0.3227  privacy: 0.87906\n",
      "\n",
      "--Time: Sat Oct 16 19:40:09 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-3.01161  0.       0.     ] total reward: -3.01161\n",
      "UEHitrate: 0.06052  edgeHitrate 0.25288 sumHitrate 0.3134  privacy: 0.87733\n",
      "\n",
      "--Time: Sat Oct 16 19:40:40 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-2.89171  0.       0.     ] total reward: -2.89171\n",
      "UEHitrate: 0.06145  edgeHitrate 0.24432 sumHitrate 0.30577  privacy: 0.88447\n",
      "\n",
      "--Time: Sat Oct 16 19:41:10 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-2.78545  0.       0.     ] total reward: -2.78545\n",
      "UEHitrate: 0.06176  edgeHitrate 0.23517 sumHitrate 0.29693  privacy: 0.8922\n",
      "\n",
      "--Time: Sat Oct 16 19:41:41 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-2.65304  0.       0.     ] total reward: -2.65304\n",
      "UEHitrate: 0.06224  edgeHitrate 0.23025 sumHitrate 0.29249  privacy: 0.89272\n",
      "\n",
      "--Time: Sat Oct 16 19:42:15 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-2.55035  0.       0.     ] total reward: -2.55035\n",
      "UEHitrate: 0.06262  edgeHitrate 0.22352 sumHitrate 0.28614  privacy: 0.89315\n",
      "\n",
      "--Time: Sat Oct 16 19:42:47 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-2.46929  0.       0.     ] total reward: -2.46929\n",
      "UEHitrate: 0.06161  edgeHitrate 0.22002 sumHitrate 0.28163  privacy: 0.88667\n",
      "\n",
      "--Time: Sat Oct 16 19:43:19 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-2.38772  0.       0.     ] total reward: -2.38772\n",
      "UEHitrate: 0.06221  edgeHitrate 0.21835 sumHitrate 0.28056  privacy: 0.88222\n",
      "\n",
      "--Time: Sat Oct 16 19:43:51 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-2.32562  0.       0.     ] total reward: -2.32562\n",
      "UEHitrate: 0.06223  edgeHitrate 0.21472 sumHitrate 0.27695  privacy: 0.88747\n",
      "\n",
      "--Time: Sat Oct 16 19:44:25 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-2.26025  0.       0.     ] total reward: -2.26025\n",
      "UEHitrate: 0.06222  edgeHitrate 0.21042 sumHitrate 0.27263  privacy: 0.88586\n",
      "\n",
      "--Time: Sat Oct 16 19:44:55 2021   Index: 139999   Loss: 0.0 --\n",
      "Reward: [-2.20391  0.       0.     ] total reward: -2.20391\n",
      "UEHitrate: 0.06166  edgeHitrate 0.20947 sumHitrate 0.27114  privacy: 0.88879\n",
      "\n",
      "--Time: Sat Oct 16 19:45:26 2021   Index: 149999   Loss: 0.0 --\n",
      "Reward: [-2.14784  0.       0.     ] total reward: -2.14784\n",
      "UEHitrate: 0.06073  edgeHitrate 0.20861 sumHitrate 0.26934  privacy: 0.89329\n",
      "\n",
      "--Time: Sat Oct 16 19:45:58 2021   Index: 159999   Loss: 0.0 --\n",
      "Reward: [-2.10111  0.       0.     ] total reward: -2.10111\n",
      "UEHitrate: 0.05996  edgeHitrate 0.20628 sumHitrate 0.26624  privacy: 0.89496\n",
      "\n",
      "--Time: Sat Oct 16 19:46:29 2021   Index: 169999   Loss: 0.0 --\n",
      "Reward: [-2.0516  0.      0.    ] total reward: -2.0516\n",
      "UEHitrate: 0.05889  edgeHitrate 0.20686 sumHitrate 0.26575  privacy: 0.89571\n",
      "\n",
      "--Time: Sat Oct 16 19:47:00 2021   Index: 179999   Loss: 0.0 --\n",
      "Reward: [-2.00144  0.       0.     ] total reward: -2.00144\n",
      "UEHitrate: 0.05848  edgeHitrate 0.20771 sumHitrate 0.26619  privacy: 0.88806\n",
      "\n",
      "--Time: Sat Oct 16 19:47:30 2021   Index: 189999   Loss: 0.0 --\n",
      "Reward: [-1.95285  0.       0.     ] total reward: -1.95285\n",
      "UEHitrate: 0.05821  edgeHitrate 0.20804 sumHitrate 0.26625  privacy: 0.88629\n",
      "\n",
      "--Time: Sat Oct 16 19:48:03 2021   Index: 199999   Loss: 0.0 --\n",
      "Reward: [-1.90877  0.       0.     ] total reward: -1.90877\n",
      "UEHitrate: 0.05754  edgeHitrate 0.20871 sumHitrate 0.26626  privacy: 0.88298\n",
      "\n",
      "--Time: Sat Oct 16 19:48:36 2021   Index: 209999   Loss: 0.0 --\n",
      "Reward: [-1.87186  0.       0.     ] total reward: -1.87186\n",
      "UEHitrate: 0.05722  edgeHitrate 0.21113 sumHitrate 0.26835  privacy: 0.86463\n",
      "\n",
      "--Time: Sat Oct 16 19:49:08 2021   Index: 219999   Loss: 0.0 --\n",
      "Reward: [-1.83493  0.       0.     ] total reward: -1.83493\n",
      "UEHitrate: 0.05671  edgeHitrate 0.2104 sumHitrate 0.26712  privacy: 0.86749\n",
      "\n",
      "--Time: Sat Oct 16 19:49:42 2021   Index: 229999   Loss: 0.0 --\n",
      "Reward: [-1.80577  0.       0.     ] total reward: -1.80577\n",
      "UEHitrate: 0.05627  edgeHitrate 0.21 sumHitrate 0.26627  privacy: 0.87067\n",
      "\n",
      "--Time: Sat Oct 16 19:50:14 2021   Index: 239999   Loss: 0.0 --\n",
      "Reward: [-1.77457  0.       0.     ] total reward: -1.77457\n",
      "UEHitrate: 0.05598  edgeHitrate 0.20933 sumHitrate 0.26532  privacy: 0.87345\n",
      "\n",
      "--Time: Sat Oct 16 19:50:46 2021   Index: 249999   Loss: 0.0 --\n",
      "Reward: [-1.74377  0.       0.     ] total reward: -1.74377\n",
      "UEHitrate: 0.05531  edgeHitrate 0.2096 sumHitrate 0.26491  privacy: 0.87591\n",
      "\n",
      "--Time: Sat Oct 16 19:51:18 2021   Index: 259999   Loss: 0.0 --\n",
      "Reward: [-1.71361  0.       0.     ] total reward: -1.71361\n",
      "UEHitrate: 0.05506  edgeHitrate 0.2087 sumHitrate 0.26375  privacy: 0.87812\n",
      "\n",
      "--Time: Sat Oct 16 19:51:49 2021   Index: 269999   Loss: 0.0 --\n",
      "Reward: [-1.68853  0.       0.     ] total reward: -1.68853\n",
      "UEHitrate: 0.05474  edgeHitrate 0.20781 sumHitrate 0.26256  privacy: 0.8805\n",
      "\n",
      "--Time: Sat Oct 16 19:52:21 2021   Index: 279999   Loss: 0.0 --\n",
      "Reward: [-1.6627  0.      0.    ] total reward: -1.6627\n",
      "UEHitrate: 0.05438  edgeHitrate 0.20812 sumHitrate 0.2625  privacy: 0.8826\n",
      "\n",
      "--Time: Sat Oct 16 19:52:55 2021   Index: 289999   Loss: 0.0 --\n",
      "Reward: [-1.63412  0.       0.     ] total reward: -1.63412\n",
      "UEHitrate: 0.05384  edgeHitrate 0.20789 sumHitrate 0.26173  privacy: 0.88436\n",
      "\n",
      "--Time: Sat Oct 16 19:53:28 2021   Index: 299999   Loss: 0.0 --\n",
      "Reward: [-1.60269  0.       0.     ] total reward: -1.60269\n",
      "UEHitrate: 0.05303  edgeHitrate 0.20875 sumHitrate 0.26178  privacy: 0.88559\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sat Oct 16 19:53:32 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-1.5997  0.      0.    ] total reward: -1.5997\n",
      "UEHitrate: 0.05292  edgeHitrate 0.20865 sumHitrate 0.26158  privacy: 0.88569\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.3419 , 0.35205, 0.33187, 0.3227 , 0.3134 , 0.30577, 0.29693,\n",
       "        0.29249, 0.28614, 0.28163, 0.28056, 0.27695, 0.27263, 0.27114,\n",
       "        0.26934, 0.26624, 0.26575, 0.26619, 0.26625, 0.26626, 0.26835,\n",
       "        0.26712, 0.26627, 0.26532, 0.26491, 0.26375, 0.26256, 0.2625 ,\n",
       "        0.26173, 0.26178, 0.26158]),\n",
       " array([0.0192 , 0.0397 , 0.05123, 0.05665, 0.06052, 0.06145, 0.06176,\n",
       "        0.06224, 0.06262, 0.06161, 0.06221, 0.06223, 0.06222, 0.06166,\n",
       "        0.06073, 0.05996, 0.05889, 0.05848, 0.05821, 0.05754, 0.05722,\n",
       "        0.05671, 0.05627, 0.05598, 0.05531, 0.05506, 0.05474, 0.05438,\n",
       "        0.05384, 0.05303, 0.05292]),\n",
       " array([0.3227 , 0.31235, 0.28063, 0.26605, 0.25288, 0.24432, 0.23517,\n",
       "        0.23025, 0.22352, 0.22002, 0.21835, 0.21472, 0.21042, 0.20947,\n",
       "        0.20861, 0.20628, 0.20686, 0.20771, 0.20804, 0.20871, 0.21113,\n",
       "        0.2104 , 0.21   , 0.20933, 0.2096 , 0.2087 , 0.20781, 0.20812,\n",
       "        0.20789, 0.20875, 0.20865]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88683, 0.87111, 0.87353, 0.87906, 0.87733, 0.88447, 0.8922 ,\n",
       "       0.89272, 0.89315, 0.88667, 0.88222, 0.88747, 0.88586, 0.88879,\n",
       "       0.89329, 0.89496, 0.89571, 0.88806, 0.88629, 0.88298, 0.86463,\n",
       "       0.86749, 0.87067, 0.87345, 0.87591, 0.87812, 0.8805 , 0.8826 ,\n",
       "       0.88436, 0.88559, 0.88569])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v9.0_ep6_1016-14-27-36'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a71a010a29be3df77e49d8f42909021441fd02d28ccbb313c9d0d3f90d3253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
