{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>video_type</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "      <th>level4</th>\n",
       "      <th>time</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79</td>\n",
       "      <td>3536</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>3568</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>2649</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>4636</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>2570</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89288</th>\n",
       "      <td>101</td>\n",
       "      <td>5652</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>43198</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89289</th>\n",
       "      <td>131</td>\n",
       "      <td>1989</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43198</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89290</th>\n",
       "      <td>3</td>\n",
       "      <td>2206</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>43198</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89291</th>\n",
       "      <td>101</td>\n",
       "      <td>1159</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>43199</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89292</th>\n",
       "      <td>131</td>\n",
       "      <td>3038</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43199</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89293 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         u     i  day  video_type  level1  level2  level3  level4   time  hour\n",
       "0       79  3536    0          11       0       0       0       0      0     0\n",
       "1        6  3568    0          11       0       0       0       0      0     0\n",
       "2       60  2649    0          11       0       0       0       0      0     0\n",
       "3       63  4636    0          11       0       0       0       2      0     0\n",
       "4       66  2570    0          11       0       0       0       0      0     0\n",
       "...    ...   ...  ...         ...     ...     ...     ...     ...    ...   ...\n",
       "89288  101  5652   29          13       0       0       0       2  43198   239\n",
       "89289  131  1989   29          13       0       0       0       0  43198   239\n",
       "89290    3  2206   29          11       0       0       0       2  43198   239\n",
       "89291  101  1159   29          13       0       0       0       2  43199   239\n",
       "89292  131  3038   29          13       0       0       0       0  43199   239\n",
       "\n",
       "[89293 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R89293_U200_V6000/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000,\n",
       " 200,\n",
       "          u     i  day  video_type  level1  level2  level3  level4   time  hour\n",
       " 0       79  3536    0          11       0       0       0       0      0     0\n",
       " 1        6  3568    0          11       0       0       0       0      0     0\n",
       " 2       60  2649    0          11       0       0       0       0      0     0\n",
       " 3       63  4636    0          11       0       0       0       2      0     0\n",
       " 4       66  2570    0          11       0       0       0       0      0     0\n",
       " ...    ...   ...  ...         ...     ...     ...     ...     ...    ...   ...\n",
       " 62466  128  2398   19          13       0       0       0       1  28795   159\n",
       " 62467  174  1510   19          47       0       0       0       2  28796   159\n",
       " 62468  174  1512   19          47       0       0       0       2  28797   159\n",
       " 62469  114  2860   19          11       0       0       0       0  28797   159\n",
       " 62470   95  4056   19          13       0       0       0       2  28799   159\n",
       " \n",
       " [62471 rows x 10 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainUIT = UIT[UIT['day']<20]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=15,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p) , \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                self.l_edge,\n",
    "                self.l_cp)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e,v):\n",
    "        \n",
    "        self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum()/torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)) - torch.log(lastru * lastp + (1-lastru) * (1-lastp))).sum()\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        self.Rl =   self.BETAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "\n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],\\\n",
    "                                     self.lastAction,self.S,self.Bu,self.l_edge,self.l_cp,self.e,self.v)\n",
    "        \n",
    "        if train:\n",
    "            \n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                    lastAction.to(device), \n",
    "                    self.statusFeature,\n",
    "                    torch.tensor([self.reward.float()]).to(device),\n",
    "                    torch.tensor([self.W[-1]]).to(device))\n",
    "        \n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "        \n",
    "\n",
    "        if  not train or (train and sample > eps_threshold):\n",
    "            QNetwork.eval()\n",
    "            with torch.no_grad():\n",
    "                Q_value = QNetwork(self.statusFeature)\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "            QNetwork.train()\n",
    "        else:\n",
    "            actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort()[0:10])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELPATH =  './model_dict/dnn_reward_o0l0_'\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.0,\"betal\":0}\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Wed Sep 15 15:47:45 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.176 0.    0.   ] total reward: 1.176\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 2.85149\n",
      "\n",
      "--Time: Wed Sep 15 15:48:59 2021 Episode: 0   Index: 10000   Loss: 0.57354 --\n",
      "Reward: [0.33526 0.      0.     ] total reward: 0.33526\n",
      "UEHitrate: 0.0062  edgeHitrate 0.12589 sumHitrate 0.13209  privacy: 2.52985\n",
      "\n",
      "--Time: Wed Sep 15 15:50:11 2021 Episode: 0   Index: 20000   Loss: 0.55013 --\n",
      "Reward: [0.38085 0.      0.     ] total reward: 0.38085\n",
      "UEHitrate: 0.0109  edgeHitrate 0.16674 sumHitrate 0.17764  privacy: 2.11142\n",
      "\n",
      "--Time: Wed Sep 15 15:51:23 2021 Episode: 0   Index: 30000   Loss: 0.53028 --\n",
      "Reward: [0.42282 0.      0.     ] total reward: 0.42282\n",
      "UEHitrate: 0.01263  edgeHitrate 0.17119 sumHitrate 0.18383  privacy: 1.83527\n",
      "\n",
      "--Time: Wed Sep 15 15:52:37 2021 Episode: 0   Index: 40000   Loss: 0.51606 --\n",
      "Reward: [0.4636 0.     0.    ] total reward: 0.4636\n",
      "UEHitrate: 0.01385  edgeHitrate 0.17342 sumHitrate 0.18727  privacy: 1.61292\n",
      "\n",
      "--Time: Wed Sep 15 15:53:52 2021 Episode: 0   Index: 50000   Loss: 0.50404 --\n",
      "Reward: [0.50588 0.      0.     ] total reward: 0.50588\n",
      "UEHitrate: 0.0127  edgeHitrate 0.19012 sumHitrate 0.20282  privacy: 1.43042\n",
      "\n",
      "--Time: Wed Sep 15 15:55:07 2021 Episode: 0   Index: 60000   Loss: 0.49267 --\n",
      "Reward: [0.54467 0.      0.     ] total reward: 0.54467\n",
      "UEHitrate: 0.0116  edgeHitrate 0.20568 sumHitrate 0.21728  privacy: 1.31648\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 15 15:55:26 2021 Episode: 0   Index: 62470   Loss: 0.48902 --\n",
      "Reward: [0.55409 0.      0.     ] total reward: 0.55409\n",
      "UEHitrate: 0.01146  edgeHitrate 0.21213 sumHitrate 0.22359  privacy: 1.29109\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_reward_o0l0_ep0_0915-15-55-26\n",
      "\n",
      "--Time: Wed Sep 15 15:55:26 2021 Episode: 1   Index: 0   Loss: 2.76927 --\n",
      "Reward: [1.176 0.    0.   ] total reward: 1.176\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 2.85149\n",
      "\n",
      "--Time: Wed Sep 15 15:56:42 2021 Episode: 1   Index: 10000   Loss: 0.07453 --\n",
      "Reward: [0.36536 0.      0.     ] total reward: 0.36536\n",
      "UEHitrate: 0.0092  edgeHitrate 0.23288 sumHitrate 0.24208  privacy: 2.4441\n",
      "\n",
      "--Time: Wed Sep 15 15:57:58 2021 Episode: 1   Index: 20000   Loss: 0.06887 --\n",
      "Reward: [0.38361 0.      0.     ] total reward: 0.38361\n",
      "UEHitrate: 0.00645  edgeHitrate 0.16909 sumHitrate 0.17554  privacy: 2.15173\n",
      "\n",
      "--Time: Wed Sep 15 15:59:15 2021 Episode: 1   Index: 30000   Loss: 0.06589 --\n",
      "Reward: [0.42506 0.      0.     ] total reward: 0.42506\n",
      "UEHitrate: 0.00597  edgeHitrate 0.14266 sumHitrate 0.14863  privacy: 1.69398\n",
      "\n",
      "--Time: Wed Sep 15 16:00:32 2021 Episode: 1   Index: 40000   Loss: 0.06414 --\n",
      "Reward: [0.48395 0.      0.     ] total reward: 0.48395\n",
      "UEHitrate: 0.00535  edgeHitrate 0.1294 sumHitrate 0.13475  privacy: 1.31\n",
      "\n",
      "--Time: Wed Sep 15 16:01:49 2021 Episode: 1   Index: 50000   Loss: 0.06223 --\n",
      "Reward: [0.56155 0.      0.     ] total reward: 0.56155\n",
      "UEHitrate: 0.00506  edgeHitrate 0.1215 sumHitrate 0.12656  privacy: 0.99275\n",
      "\n",
      "--Time: Wed Sep 15 16:03:09 2021 Episode: 1   Index: 60000   Loss: 0.06069 --\n",
      "Reward: [0.66664 0.      0.     ] total reward: 0.66664\n",
      "UEHitrate: 0.00493  edgeHitrate 0.1161 sumHitrate 0.12103  privacy: 0.73733\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 15 16:03:29 2021 Episode: 1   Index: 62470   Loss: 0.05991 --\n",
      "Reward: [0.69755 0.      0.     ] total reward: 0.69755\n",
      "UEHitrate: 0.00498  edgeHitrate 0.11521 sumHitrate 0.12018  privacy: 0.68464\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_reward_o0l0_ep1_0915-16-03-29\n",
      "\n",
      "--Time: Wed Sep 15 16:03:29 2021 Episode: 2   Index: 0   Loss: 0.8712 --\n",
      "Reward: [1.176 0.    0.   ] total reward: 1.176\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 2.85149\n",
      "\n",
      "--Time: Wed Sep 15 16:04:49 2021 Episode: 2   Index: 10000   Loss: 0.24492 --\n",
      "Reward: [0.55056 0.      0.     ] total reward: 0.55056\n",
      "UEHitrate: 0.0046  edgeHitrate 0.14509 sumHitrate 0.14969  privacy: 1.4673\n",
      "\n",
      "--Time: Wed Sep 15 16:06:08 2021 Episode: 2   Index: 20000   Loss: 0.23072 --\n",
      "Reward: [0.59747 0.      0.     ] total reward: 0.59747\n",
      "UEHitrate: 0.00535  edgeHitrate 0.18664 sumHitrate 0.19199  privacy: 1.54633\n",
      "\n",
      "--Time: Wed Sep 15 16:07:29 2021 Episode: 2   Index: 30000   Loss: 0.2239 --\n",
      "Reward: [0.60723 0.      0.     ] total reward: 0.60723\n",
      "UEHitrate: 0.00623  edgeHitrate 0.21953 sumHitrate 0.22576  privacy: 1.59983\n",
      "\n",
      "--Time: Wed Sep 15 16:08:49 2021 Episode: 2   Index: 40000   Loss: 0.21644 --\n",
      "Reward: [0.61162 0.      0.     ] total reward: 0.61162\n",
      "UEHitrate: 0.00655  edgeHitrate 0.24132 sumHitrate 0.24787  privacy: 1.60905\n",
      "\n",
      "--Time: Wed Sep 15 16:10:05 2021 Episode: 2   Index: 50000   Loss: 0.212 --\n",
      "Reward: [0.61607 0.      0.     ] total reward: 0.61607\n",
      "UEHitrate: 0.0067  edgeHitrate 0.25589 sumHitrate 0.26259  privacy: 1.59773\n",
      "\n",
      "--Time: Wed Sep 15 16:11:20 2021 Episode: 2   Index: 60000   Loss: 0.20737 --\n",
      "Reward: [0.62057 0.      0.     ] total reward: 0.62057\n",
      "UEHitrate: 0.00677  edgeHitrate 0.26005 sumHitrate 0.26681  privacy: 1.57633\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 15 16:11:40 2021 Episode: 2   Index: 62470   Loss: 0.20643 --\n",
      "Reward: [0.62188 0.      0.     ] total reward: 0.62188\n",
      "UEHitrate: 0.00677  edgeHitrate 0.25881 sumHitrate 0.26558  privacy: 1.56928\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Wed Sep 15 16:11:40 2021 Episode: 3   Index: 0   Loss: 1.10926 --\n",
      "Reward: [1.176 0.    0.   ] total reward: 1.176\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 2.85149\n",
      "\n",
      "--Time: Wed Sep 15 16:12:57 2021 Episode: 3   Index: 10000   Loss: 0.19719 --\n",
      "Reward: [0.4556 0.     0.    ] total reward: 0.4556\n",
      "UEHitrate: 0.0048  edgeHitrate 0.11679 sumHitrate 0.12159  privacy: 1.97871\n",
      "\n",
      "--Time: Wed Sep 15 16:14:14 2021 Episode: 3   Index: 20000   Loss: 0.19384 --\n",
      "Reward: [0.47536 0.      0.     ] total reward: 0.47536\n",
      "UEHitrate: 0.00445  edgeHitrate 0.13354 sumHitrate 0.13799  privacy: 1.96611\n",
      "\n",
      "--Time: Wed Sep 15 16:15:29 2021 Episode: 3   Index: 30000   Loss: 0.19321 --\n",
      "Reward: [0.48522 0.      0.     ] total reward: 0.48522\n",
      "UEHitrate: 0.00507  edgeHitrate 0.15396 sumHitrate 0.15903  privacy: 1.96196\n",
      "\n",
      "--Time: Wed Sep 15 16:16:45 2021 Episode: 3   Index: 40000   Loss: 0.19046 --\n",
      "Reward: [0.49337 0.      0.     ] total reward: 0.49337\n",
      "UEHitrate: 0.0053  edgeHitrate 0.16737 sumHitrate 0.17267  privacy: 1.94644\n",
      "\n",
      "--Time: Wed Sep 15 16:18:01 2021 Episode: 3   Index: 50000   Loss: 0.18905 --\n",
      "Reward: [0.50162 0.      0.     ] total reward: 0.50162\n",
      "UEHitrate: 0.00552  edgeHitrate 0.17642 sumHitrate 0.18194  privacy: 1.91487\n",
      "\n",
      "--Time: Wed Sep 15 16:19:18 2021 Episode: 3   Index: 60000   Loss: 0.18673 --\n",
      "Reward: [0.50853 0.      0.     ] total reward: 0.50853\n",
      "UEHitrate: 0.00577  edgeHitrate 0.17963 sumHitrate 0.1854  privacy: 1.87624\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 15 16:19:36 2021 Episode: 3   Index: 62470   Loss: 0.18617 --\n",
      "Reward: [0.51032 0.      0.     ] total reward: 0.51032\n",
      "UEHitrate: 0.00583  edgeHitrate 0.18063 sumHitrate 0.18645  privacy: 1.86639\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Wed Sep 15 16:19:36 2021 Episode: 4   Index: 0   Loss: 1.24076 --\n",
      "Reward: [1.176 0.    0.   ] total reward: 1.176\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 2.85149\n",
      "\n",
      "--Time: Wed Sep 15 16:20:49 2021 Episode: 4   Index: 10000   Loss: 0.19784 --\n",
      "Reward: [0.47115 0.      0.     ] total reward: 0.47115\n",
      "UEHitrate: 0.0033  edgeHitrate 0.10839 sumHitrate 0.11169  privacy: 1.89618\n",
      "\n",
      "--Time: Wed Sep 15 16:22:10 2021 Episode: 4   Index: 20000   Loss: 0.19856 --\n",
      "Reward: [0.4969 0.     0.    ] total reward: 0.4969\n",
      "UEHitrate: 0.00415  edgeHitrate 0.12329 sumHitrate 0.12744  privacy: 1.88\n",
      "\n",
      "--Time: Wed Sep 15 16:23:25 2021 Episode: 4   Index: 30000   Loss: 0.19756 --\n",
      "Reward: [0.50792 0.      0.     ] total reward: 0.50792\n",
      "UEHitrate: 0.0048  edgeHitrate 0.14296 sumHitrate 0.14776  privacy: 1.88887\n",
      "\n",
      "--Time: Wed Sep 15 16:24:43 2021 Episode: 4   Index: 40000   Loss: 0.19639 --\n",
      "Reward: [0.5146 0.     0.    ] total reward: 0.5146\n",
      "UEHitrate: 0.00492  edgeHitrate 0.15797 sumHitrate 0.1629  privacy: 1.89008\n",
      "\n",
      "--Time: Wed Sep 15 16:26:00 2021 Episode: 4   Index: 50000   Loss: 0.19524 --\n",
      "Reward: [0.52124 0.      0.     ] total reward: 0.52124\n",
      "UEHitrate: 0.005  edgeHitrate 0.17142 sumHitrate 0.17642  privacy: 1.86983\n",
      "\n",
      "--Time: Wed Sep 15 16:27:12 2021 Episode: 4   Index: 60000   Loss: 0.19409 --\n",
      "Reward: [0.52653 0.      0.     ] total reward: 0.52653\n",
      "UEHitrate: 0.00495  edgeHitrate 0.18206 sumHitrate 0.18701  privacy: 1.8421\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 15 16:27:29 2021 Episode: 4   Index: 62470   Loss: 0.19369 --\n",
      "Reward: [0.52795 0.      0.     ] total reward: 0.52795\n",
      "UEHitrate: 0.00498  edgeHitrate 0.18506 sumHitrate 0.19004  privacy: 1.83494\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 5\n",
    "TARGET_UPDATE = 1\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = trainUIT.shape[0]\n",
    "agentStep = 0\n",
    "\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "        agentStep += 1\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 200 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 10000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+'ep{}_'.format(bestEpisode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "        print(bestPath)\n",
    "    print()\n",
    "    env = ENV(userNum,contentNum)\n",
    "    UEs = {}\n",
    "    sumReward = sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Wed Sep 15 22:58:00 2021   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.176 0.    0.   ] total reward: 1.176\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 2.85149\n",
      "\n",
      "--Time: Wed Sep 15 22:58:28 2021   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.77192 0.      0.     ] total reward: 0.77192\n",
      "UEHitrate: 0.0084  edgeHitrate 0.28697 sumHitrate 0.29537  privacy: 0.91622\n",
      "\n",
      "--Time: Wed Sep 15 22:58:57 2021   Index: 20000   Loss: 0.0 --\n",
      "Reward: [1.003 0.    0.   ] total reward: 1.003\n",
      "UEHitrate: 0.00695  edgeHitrate 0.23959 sumHitrate 0.24654  privacy: 0.71096\n",
      "\n",
      "--Time: Wed Sep 15 22:59:27 2021   Index: 30000   Loss: 0.0 --\n",
      "Reward: [1.18291 0.      0.     ] total reward: 1.18291\n",
      "UEHitrate: 0.00623  edgeHitrate 0.19153 sumHitrate 0.19776  privacy: 0.6303\n",
      "\n",
      "--Time: Wed Sep 15 22:59:56 2021   Index: 40000   Loss: 0.0 --\n",
      "Reward: [1.32353 0.      0.     ] total reward: 1.32353\n",
      "UEHitrate: 0.00547  edgeHitrate 0.1599 sumHitrate 0.16537  privacy: 0.56008\n",
      "\n",
      "--Time: Wed Sep 15 23:00:25 2021   Index: 50000   Loss: 0.0 --\n",
      "Reward: [1.43428 0.      0.     ] total reward: 1.43428\n",
      "UEHitrate: 0.00494  edgeHitrate 0.13832 sumHitrate 0.14326  privacy: 0.52897\n",
      "\n",
      "--Time: Wed Sep 15 23:00:54 2021   Index: 60000   Loss: 0.0 --\n",
      "Reward: [1.53728 0.      0.     ] total reward: 1.53728\n",
      "UEHitrate: 0.00475  edgeHitrate 0.12328 sumHitrate 0.12803  privacy: 0.50901\n",
      "\n",
      "--Time: Wed Sep 15 23:01:22 2021   Index: 70000   Loss: 0.0 --\n",
      "Reward: [1.60562 0.      0.     ] total reward: 1.60562\n",
      "UEHitrate: 0.00469  edgeHitrate 0.11246 sumHitrate 0.11714  privacy: 0.50338\n",
      "\n",
      "--Time: Wed Sep 15 23:01:50 2021   Index: 80000   Loss: 0.0 --\n",
      "Reward: [1.67656 0.      0.     ] total reward: 1.67656\n",
      "UEHitrate: 0.00466  edgeHitrate 0.1057 sumHitrate 0.11036  privacy: 0.44956\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 15 23:02:17 2021   Index: 89292   Loss: 0.0 --\n",
      "Reward: [1.83267 0.      0.     ] total reward: 1.83267\n",
      "UEHitrate: 0.00458  edgeHitrate 0.10259 sumHitrate 0.10718  privacy: 0.3072\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "sumHitrate = np.zeros(10)\n",
    "UEHitrate = np.zeros(10)\n",
    "edgeHitrate = np.zeros(10)\n",
    "privacyReduction = np.zeros(10)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.     , 0.29537, 0.24654, 0.19776, 0.16537, 0.14326, 0.12803,\n",
       "        0.11714, 0.11036, 0.10718]),\n",
       " array([0.     , 0.0084 , 0.00695, 0.00623, 0.00547, 0.00494, 0.00475,\n",
       "        0.00469, 0.00466, 0.00458]),\n",
       " array([0.     , 0.28697, 0.23959, 0.19153, 0.1599 , 0.13832, 0.12328,\n",
       "        0.11246, 0.1057 , 0.10259]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.85149, 0.91622, 0.71096, 0.6303 , 0.56008, 0.52897, 0.50901,\n",
       "       0.50338, 0.44956, 0.3072 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "py38_ML",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
