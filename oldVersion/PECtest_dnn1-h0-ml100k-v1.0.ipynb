{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1 修改时序上bug\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_h0_v1.0_'\n",
    "\n",
    "data_path = '/home/ubuntu/data/dataset/ml-100k/'\n",
    "UIT = pd.read_csv(data_path + 'ml_header.trace')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=np.int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":0,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    '''\n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    '''\n",
    "    #next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    #expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = self.statusEmbedding(r,p,e)\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        statusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        if train and len(self.W)>1: #d\n",
    "            lastAction = np.row_stack((1-self.lastAction,self.lastAction)).T\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature.to(device), \n",
    "                        torch.from_numpy(lastAction).to(device), \n",
    "                        statusFeature.to(device),\n",
    "                        torch.tensor([self.reward]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "        self.lastStatusFeature = statusFeature\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            #eps_threshold = 0\n",
    "            if  not train or (train and sample >= eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                #with torch.no_grad():\n",
    "                Q_value = QNetwork(statusFeature).detach()\n",
    "                QNetwork.train()\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                \n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "            \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 14:09:01 2021 Episode: 0   Index: 76251   Loss: 2.6139 --\n",
      "Reward: [0.      0.00079 0.03778] total reward: 0.03857\n",
      "UEHitrate: 0.0031  edgeHitrate 0.04722 sumHitrate 0.05032  privacy: 3.45626\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 14:15:50 2021 Episode: 0   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00113 0.03859] total reward: 0.03971\n",
      "UEHitrate: 0.00383  edgeHitrate 0.04824 sumHitrate 0.05207  privacy: 2.75843\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_h0_v1.0_ep0_1015-14-15-50\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 14:24:09 2021 Episode: 1   Index: 76251   Loss: 2.36473 --\n",
      "Reward: [0.      0.00053 0.04069] total reward: 0.04122\n",
      "UEHitrate: 0.00177  edgeHitrate 0.05086 sumHitrate 0.05263  privacy: 3.42357\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 14:30:43 2021 Episode: 1   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00048 0.0223 ] total reward: 0.02277\n",
      "UEHitrate: 0.0016  edgeHitrate 0.02787 sumHitrate 0.02947  privacy: 2.73575\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 14:39:21 2021 Episode: 2   Index: 76251   Loss: 2.33162 --\n",
      "Reward: [0.      0.00042 0.0233 ] total reward: 0.02373\n",
      "UEHitrate: 0.0015  edgeHitrate 0.02913 sumHitrate 0.03062  privacy: 3.39502\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 14:46:10 2021 Episode: 2   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00029 0.01672] total reward: 0.01701\n",
      "UEHitrate: 0.00085  edgeHitrate 0.0209 sumHitrate 0.02175  privacy: 2.7203\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 14:54:49 2021 Episode: 3   Index: 76251   Loss: 2.29268 --\n",
      "Reward: [0.      0.00034 0.02094] total reward: 0.02128\n",
      "UEHitrate: 0.00108  edgeHitrate 0.02618 sumHitrate 0.02725  privacy: 3.3645\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 15:01:55 2021 Episode: 3   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00026 0.0183 ] total reward: 0.01855\n",
      "UEHitrate: 0.00103  edgeHitrate 0.02287 sumHitrate 0.0239  privacy: 2.72197\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 15:11:30 2021 Episode: 4   Index: 76251   Loss: 2.27613 --\n",
      "Reward: [0.      0.00034 0.02185] total reward: 0.0222\n",
      "UEHitrate: 0.00109  edgeHitrate 0.02732 sumHitrate 0.02841  privacy: 3.35571\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 15:19:07 2021 Episode: 4   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00031 0.01608] total reward: 0.01639\n",
      "UEHitrate: 0.00099  edgeHitrate 0.02009 sumHitrate 0.02109  privacy: 2.72957\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 15:28:53 2021 Episode: 5   Index: 76251   Loss: 2.25182 --\n",
      "Reward: [0.      0.00034 0.02163] total reward: 0.02198\n",
      "UEHitrate: 0.00114  edgeHitrate 0.02704 sumHitrate 0.02818  privacy: 3.3479\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 15:40:01 2021 Episode: 5   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00031 0.01871] total reward: 0.01902\n",
      "UEHitrate: 0.00098  edgeHitrate 0.02338 sumHitrate 0.02437  privacy: 2.715\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 15:56:38 2021 Episode: 6   Index: 76251   Loss: 2.23401 --\n",
      "Reward: [0.      0.00047 0.02151] total reward: 0.02198\n",
      "UEHitrate: 0.00135  edgeHitrate 0.02688 sumHitrate 0.02824  privacy: 3.34009\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 16:05:13 2021 Episode: 6   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00035 0.01854] total reward: 0.01889\n",
      "UEHitrate: 0.00113  edgeHitrate 0.02317 sumHitrate 0.0243  privacy: 2.7153\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 16:15:11 2021 Episode: 7   Index: 76251   Loss: 2.22877 --\n",
      "Reward: [0.      0.00035 0.02224] total reward: 0.02259\n",
      "UEHitrate: 0.00123  edgeHitrate 0.0278 sumHitrate 0.02904  privacy: 3.34079\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 16:22:55 2021 Episode: 7   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00032 0.0171 ] total reward: 0.01742\n",
      "UEHitrate: 0.00082  edgeHitrate 0.02138 sumHitrate 0.0222  privacy: 2.71369\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 16:32:59 2021 Episode: 8   Index: 76251   Loss: 2.21876 --\n",
      "Reward: [0.      0.0005  0.02491] total reward: 0.02541\n",
      "UEHitrate: 0.00151  edgeHitrate 0.03113 sumHitrate 0.03264  privacy: 3.34212\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 16:40:48 2021 Episode: 8   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.0003  0.01835] total reward: 0.01864\n",
      "UEHitrate: 0.00088  edgeHitrate 0.02294 sumHitrate 0.02381  privacy: 2.71514\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct 15 16:50:35 2021 Episode: 9   Index: 76251   Loss: 2.20121 --\n",
      "Reward: [0.      0.00055 0.02266] total reward: 0.02321\n",
      "UEHitrate: 0.0016  edgeHitrate 0.02833 sumHitrate 0.02993  privacy: 3.33622\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Fri Oct 15 16:58:03 2021 Episode: 9   Index: 106993   Loss: 0.0 --\n",
      "Reward: [0.      0.00034 0.01877] total reward: 0.01911\n",
      "UEHitrate: 0.00113  edgeHitrate 0.02346 sumHitrate 0.02459  privacy: 2.71659\n",
      "------------------------------validation---------------------------#-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(4, 2).to(device)\n",
    "target_net = DQN(4, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(4000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if (index+1) % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        '''\n",
    "        if (index+1) % 1000 == 0 :\n",
    "            psi = 0\n",
    "            for u in UEs:\n",
    "                psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "        '''\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Fri Oct 15 16:58:52 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [0.      0.0006  0.02648] total reward: 0.02708\n",
      "UEHitrate: 0.0026  edgeHitrate 0.0331 sumHitrate 0.0357  privacy: 6.09295\n",
      "\n",
      "--Time: Fri Oct 15 16:59:34 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [0.      0.00057 0.02716] total reward: 0.02773\n",
      "UEHitrate: 0.00205  edgeHitrate 0.03395 sumHitrate 0.036  privacy: 5.10033\n",
      "\n",
      "--Time: Fri Oct 15 17:00:16 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [0.      0.00083 0.03269] total reward: 0.03352\n",
      "UEHitrate: 0.00293  edgeHitrate 0.04087 sumHitrate 0.0438  privacy: 4.72013\n",
      "\n",
      "--Time: Fri Oct 15 17:00:59 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [0.      0.001   0.03784] total reward: 0.03884\n",
      "UEHitrate: 0.00358  edgeHitrate 0.0473 sumHitrate 0.05088  privacy: 4.34187\n",
      "\n",
      "--Time: Fri Oct 15 17:01:38 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [0.      0.00102 0.04187] total reward: 0.04289\n",
      "UEHitrate: 0.00364  edgeHitrate 0.05234 sumHitrate 0.05598  privacy: 4.00494\n",
      "\n",
      "--Time: Fri Oct 15 17:02:20 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [0.      0.00114 0.04261] total reward: 0.04375\n",
      "UEHitrate: 0.00395  edgeHitrate 0.05327 sumHitrate 0.05722  privacy: 3.68654\n",
      "\n",
      "--Time: Fri Oct 15 17:03:02 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [0.      0.00118 0.04198] total reward: 0.04315\n",
      "UEHitrate: 0.00411  edgeHitrate 0.05247 sumHitrate 0.05659  privacy: 3.48885\n",
      "\n",
      "--Time: Fri Oct 15 17:03:45 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [0.      0.00118 0.04068] total reward: 0.04186\n",
      "UEHitrate: 0.00406  edgeHitrate 0.05085 sumHitrate 0.05491  privacy: 3.27397\n",
      "\n",
      "--Time: Fri Oct 15 17:04:27 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [0.      0.00114 0.03973] total reward: 0.04088\n",
      "UEHitrate: 0.00389  edgeHitrate 0.04967 sumHitrate 0.05356  privacy: 3.054\n",
      "\n",
      "--Time: Fri Oct 15 17:05:11 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [0.      0.00111 0.03877] total reward: 0.03988\n",
      "UEHitrate: 0.0038  edgeHitrate 0.04846 sumHitrate 0.05226  privacy: 2.88272\n",
      "\n",
      "--Time: Fri Oct 15 17:05:54 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [0.      0.00113 0.03836] total reward: 0.03948\n",
      "UEHitrate: 0.00378  edgeHitrate 0.04795 sumHitrate 0.05173  privacy: 2.70783\n",
      "\n",
      "--Time: Fri Oct 15 17:06:37 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [0.      0.00117 0.03813] total reward: 0.0393\n",
      "UEHitrate: 0.00394  edgeHitrate 0.04766 sumHitrate 0.0516  privacy: 2.58556\n",
      "\n",
      "--Time: Fri Oct 15 17:07:20 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [0.      0.00121 0.03772] total reward: 0.03893\n",
      "UEHitrate: 0.00417  edgeHitrate 0.04715 sumHitrate 0.05132  privacy: 2.49957\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Fri Oct 15 17:07:32 2021   Index: 132689   Loss: 0.0 --\n",
      "Reward: [0.      0.00124 0.03764] total reward: 0.03888\n",
      "UEHitrate: 0.00432  edgeHitrate 0.04705 sumHitrate 0.05137  privacy: 2.47212\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(4, 2).to(device)\n",
    "target_net = DQN(4, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0357 , 0.036  , 0.0438 , 0.05088, 0.05598, 0.05722, 0.05659,\n",
       "        0.05491, 0.05356, 0.05226, 0.05173, 0.0516 , 0.05132, 0.05137]),\n",
       " array([0.0026 , 0.00205, 0.00293, 0.00358, 0.00364, 0.00395, 0.00411,\n",
       "        0.00406, 0.00389, 0.0038 , 0.00378, 0.00394, 0.00417, 0.00432]),\n",
       " array([0.0331 , 0.03395, 0.04087, 0.0473 , 0.05234, 0.05327, 0.05247,\n",
       "        0.05085, 0.04967, 0.04846, 0.04795, 0.04766, 0.04715, 0.04705]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.09295, 5.10033, 4.72013, 4.34187, 4.00494, 3.68654, 3.48885,\n",
       "       3.27397, 3.054  , 2.88272, 2.70783, 2.58556, 2.49957, 2.47212])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    " \n",
    "def sigmoid(x):\n",
    "    # 直接返回sigmoid函数\n",
    "    return 1. / (1. + np.exp(10*x))\n",
    " \n",
    " \n",
    "def plot_sigmoid():\n",
    "    # param:起点，终点，间距\n",
    "    x = np.arange(-8, 8, 0.2)\n",
    "    y = sigmoid(x)\n",
    "    plt.plot(x, y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a71a010a29be3df77e49d8f42909021441fd02d28ccbb313c9d0d3f90d3253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
