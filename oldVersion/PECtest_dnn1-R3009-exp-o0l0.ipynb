{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "validUIT = UIT[UIT['day']<24]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    " \n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)).sum() - torch.log(v * p + (1-v) * (1-p)).sum())\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + torch.exp( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "\n",
    "        if self.W[-1] not in np.argwhere(self.lastAction.numpy()): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            if  not train or (train and sample > eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                with torch.no_grad():\n",
    "                    Q_value = QNetwork(self.statusFeature)\n",
    "                    actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                QNetwork.train()\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "\n",
    "            self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "            self.action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                self.action[index] = 1\n",
    "            env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "        else:\n",
    "            self.action = self.lastAction # keep the cache and no request the new video\n",
    "\n",
    "        if train:\n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                        lastAction.to(device), \n",
    "                        self.statusFeature,\n",
    "                        torch.tensor([self.reward.float()]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "                        \n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName+'dnn_o0l0_'\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0,\"betal\":0}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 15\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 50000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "\n",
    "        if index % 50000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sat Oct  9 14:24:12 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 14:35:37 2021 Episode: 0   Index: 50000   Loss: 0.31134 --\n",
      "Reward: [0.29442 0.      0.     ] total reward: 0.29442\n",
      "UEHitrate: 0.00632  edgeHitrate 0.08658 sumHitrate 0.0929  privacy: 2.59569\n",
      "\n",
      "--Time: Sat Oct  9 14:47:21 2021 Episode: 0   Index: 100000   Loss: 0.29257 --\n",
      "Reward: [0.35978 0.      0.     ] total reward: 0.35978\n",
      "UEHitrate: 0.00573  edgeHitrate 0.08034 sumHitrate 0.08607  privacy: 1.92351\n",
      "\n",
      "--Time: Sat Oct  9 14:59:14 2021 Episode: 0   Index: 150000   Loss: 0.28015 --\n",
      "Reward: [0.43625 0.      0.     ] total reward: 0.43625\n",
      "UEHitrate: 0.00624  edgeHitrate 0.0787 sumHitrate 0.08494  privacy: 1.4915\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 15:11:54 2021 Episode: 0   Index: 198174   Loss: 0.27264 --\n",
      "Reward: [0.51314 0.      0.     ] total reward: 0.51314\n",
      "UEHitrate: 0.00649  edgeHitrate 0.07854 sumHitrate 0.08504  privacy: 1.17545\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 15:11:54 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 15:18:09 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.75375 0.      0.     ] total reward: 0.75375\n",
      "UEHitrate: 0.01334  edgeHitrate 0.33625 sumHitrate 0.34959  privacy: 0.93485\n",
      "\n",
      "--Time: Sat Oct  9 15:24:19 2021 Episode: 0   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.93094 0.      0.     ] total reward: 0.93094\n",
      "UEHitrate: 0.0101  edgeHitrate 0.27604 sumHitrate 0.28614  privacy: 0.77223\n",
      "\n",
      "--Time: Sat Oct  9 15:29:54 2021 Episode: 0   Index: 150000   Loss: 0.0 --\n",
      "Reward: [1.09292 0.      0.     ] total reward: 1.09292\n",
      "UEHitrate: 0.0092  edgeHitrate 0.25031 sumHitrate 0.25951  privacy: 0.62521\n",
      "\n",
      "--Time: Sat Oct  9 15:35:06 2021 Episode: 0   Index: 200000   Loss: 0.0 --\n",
      "Reward: [1.23425 0.      0.     ] total reward: 1.23425\n",
      "UEHitrate: 0.0086  edgeHitrate 0.23497 sumHitrate 0.24358  privacy: 0.56287\n",
      "\n",
      "--Time: Sat Oct  9 15:40:01 2021 Episode: 0   Index: 250000   Loss: 0.0 --\n",
      "Reward: [1.35867 0.      0.     ] total reward: 1.35867\n",
      "UEHitrate: 0.00836  edgeHitrate 0.22581 sumHitrate 0.23418  privacy: 0.49501\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 15:40:22 2021 Episode: 0   Index: 253629   Loss: 0.0 --\n",
      "Reward: [1.36837 0.      0.     ] total reward: 1.36837\n",
      "UEHitrate: 0.0083  edgeHitrate 0.2251 sumHitrate 0.23341  privacy: 0.48991\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_ep0_1009-15-40-22\n",
      "\n",
      "--Time: Sat Oct  9 15:40:22 2021 Episode: 1   Index: 0   Loss: 7.83595 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 15:52:38 2021 Episode: 1   Index: 50000   Loss: 0.2496 --\n",
      "Reward: [0.30239 0.      0.     ] total reward: 0.30239\n",
      "UEHitrate: 0.00716  edgeHitrate 0.1126 sumHitrate 0.11976  privacy: 2.5168\n",
      "\n",
      "--Time: Sat Oct  9 16:04:48 2021 Episode: 1   Index: 100000   Loss: 0.22171 --\n",
      "Reward: [0.38441 0.      0.     ] total reward: 0.38441\n",
      "UEHitrate: 0.00664  edgeHitrate 0.10115 sumHitrate 0.10779  privacy: 1.69962\n",
      "\n",
      "--Time: Sat Oct  9 16:17:14 2021 Episode: 1   Index: 150000   Loss: 0.2082 --\n",
      "Reward: [0.47873 0.      0.     ] total reward: 0.47873\n",
      "UEHitrate: 0.0069  edgeHitrate 0.09526 sumHitrate 0.10216  privacy: 1.32833\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 16:30:25 2021 Episode: 1   Index: 198174   Loss: 0.19964 --\n",
      "Reward: [0.55762 0.      0.     ] total reward: 0.55762\n",
      "UEHitrate: 0.00705  edgeHitrate 0.09189 sumHitrate 0.09894  privacy: 1.09044\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 16:30:26 2021 Episode: 1   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 16:36:23 2021 Episode: 1   Index: 50000   Loss: 0.0 --\n",
      "Reward: [1.04443 0.      0.     ] total reward: 1.04443\n",
      "UEHitrate: 0.04786  edgeHitrate 0.33163 sumHitrate 0.37949  privacy: 0.90884\n",
      "\n",
      "--Time: Sat Oct  9 16:42:30 2021 Episode: 1   Index: 100000   Loss: 0.0 --\n",
      "Reward: [1.06902 0.      0.     ] total reward: 1.06902\n",
      "UEHitrate: 0.04502  edgeHitrate 0.31654 sumHitrate 0.36156  privacy: 0.8675\n",
      "\n",
      "--Time: Sat Oct  9 16:47:58 2021 Episode: 1   Index: 150000   Loss: 0.0 --\n",
      "Reward: [1.12241 0.      0.     ] total reward: 1.12241\n",
      "UEHitrate: 0.04156  edgeHitrate 0.30609 sumHitrate 0.34765  privacy: 0.79347\n",
      "\n",
      "--Time: Sat Oct  9 16:53:15 2021 Episode: 1   Index: 200000   Loss: 0.0 --\n",
      "Reward: [1.16884 0.      0.     ] total reward: 1.16884\n",
      "UEHitrate: 0.03876  edgeHitrate 0.29922 sumHitrate 0.33798  privacy: 0.76322\n",
      "\n",
      "--Time: Sat Oct  9 16:58:08 2021 Episode: 1   Index: 250000   Loss: 0.0 --\n",
      "Reward: [1.20829 0.      0.     ] total reward: 1.20829\n",
      "UEHitrate: 0.03773  edgeHitrate 0.29231 sumHitrate 0.33004  privacy: 0.76366\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 16:58:33 2021 Episode: 1   Index: 253629   Loss: 0.0 --\n",
      "Reward: [1.2103 0.     0.    ] total reward: 1.2103\n",
      "UEHitrate: 0.03755  edgeHitrate 0.29159 sumHitrate 0.32914  privacy: 0.76488\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 16:58:33 2021 Episode: 2   Index: 0   Loss: 6.76772 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 17:11:27 2021 Episode: 2   Index: 50000   Loss: 0.18884 --\n",
      "Reward: [0.30675 0.      0.     ] total reward: 0.30675\n",
      "UEHitrate: 0.0054  edgeHitrate 0.09374 sumHitrate 0.09914  privacy: 2.77371\n",
      "\n",
      "--Time: Sat Oct  9 17:24:51 2021 Episode: 2   Index: 100000   Loss: 0.18154 --\n",
      "Reward: [0.34105 0.      0.     ] total reward: 0.34105\n",
      "UEHitrate: 0.0055  edgeHitrate 0.08663 sumHitrate 0.09213  privacy: 2.35885\n",
      "\n",
      "--Time: Sat Oct  9 17:37:52 2021 Episode: 2   Index: 150000   Loss: 0.17581 --\n",
      "Reward: [0.38084 0.      0.     ] total reward: 0.38084\n",
      "UEHitrate: 0.00667  edgeHitrate 0.08771 sumHitrate 0.09438  privacy: 2.09515\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 17:52:32 2021 Episode: 2   Index: 198174   Loss: 0.17176 --\n",
      "Reward: [0.41354 0.      0.     ] total reward: 0.41354\n",
      "UEHitrate: 0.00677  edgeHitrate 0.08852 sumHitrate 0.09529  privacy: 1.89697\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 17:52:32 2021 Episode: 2   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 17:58:16 2021 Episode: 2   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.92587 0.      0.     ] total reward: 0.92587\n",
      "UEHitrate: 0.044  edgeHitrate 0.39147 sumHitrate 0.43547  privacy: 1.07822\n",
      "\n",
      "--Time: Sat Oct  9 18:03:36 2021 Episode: 2   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.94373 0.      0.     ] total reward: 0.94373\n",
      "UEHitrate: 0.04097  edgeHitrate 0.37237 sumHitrate 0.41334  privacy: 1.04618\n",
      "\n",
      "--Time: Sat Oct  9 18:09:08 2021 Episode: 2   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.9515 0.     0.    ] total reward: 0.9515\n",
      "UEHitrate: 0.03939  edgeHitrate 0.35223 sumHitrate 0.39162  privacy: 1.03127\n",
      "\n",
      "--Time: Sat Oct  9 18:13:53 2021 Episode: 2   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.9572 0.     0.    ] total reward: 0.9572\n",
      "UEHitrate: 0.03766  edgeHitrate 0.33972 sumHitrate 0.37739  privacy: 1.02411\n",
      "\n",
      "--Time: Sat Oct  9 18:18:33 2021 Episode: 2   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.96145 0.      0.     ] total reward: 0.96145\n",
      "UEHitrate: 0.03666  edgeHitrate 0.32588 sumHitrate 0.36254  privacy: 1.01995\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 18:18:54 2021 Episode: 2   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.96173 0.      0.     ] total reward: 0.96173\n",
      "UEHitrate: 0.03645  edgeHitrate 0.32473 sumHitrate 0.36119  privacy: 1.0198\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 18:18:54 2021 Episode: 3   Index: 0   Loss: 6.24856 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 18:31:12 2021 Episode: 3   Index: 50000   Loss: 0.13297 --\n",
      "Reward: [0.30611 0.      0.     ] total reward: 0.30611\n",
      "UEHitrate: 0.00612  edgeHitrate 0.08158 sumHitrate 0.0877  privacy: 2.64748\n",
      "\n",
      "--Time: Sat Oct  9 18:43:24 2021 Episode: 3   Index: 100000   Loss: 0.12552 --\n",
      "Reward: [0.36336 0.      0.     ] total reward: 0.36336\n",
      "UEHitrate: 0.00555  edgeHitrate 0.08555 sumHitrate 0.0911  privacy: 2.26501\n",
      "\n",
      "--Time: Sat Oct  9 18:55:42 2021 Episode: 3   Index: 150000   Loss: 0.12157 --\n",
      "Reward: [0.40902 0.      0.     ] total reward: 0.40902\n",
      "UEHitrate: 0.01171  edgeHitrate 0.10049 sumHitrate 0.1122  privacy: 2.05317\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 19:10:01 2021 Episode: 3   Index: 198174   Loss: 0.11934 --\n",
      "Reward: [0.44048 0.      0.     ] total reward: 0.44048\n",
      "UEHitrate: 0.01184  edgeHitrate 0.10383 sumHitrate 0.11567  privacy: 1.89702\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 19:10:01 2021 Episode: 3   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 19:15:24 2021 Episode: 3   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.74916 0.      0.     ] total reward: 0.74916\n",
      "UEHitrate: 0.0393  edgeHitrate 0.26729 sumHitrate 0.30659  privacy: 1.35931\n",
      "\n",
      "--Time: Sat Oct  9 19:20:50 2021 Episode: 3   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.79362 0.      0.     ] total reward: 0.79362\n",
      "UEHitrate: 0.04104  edgeHitrate 0.25645 sumHitrate 0.29749  privacy: 1.22971\n",
      "\n",
      "--Time: Sat Oct  9 19:25:31 2021 Episode: 3   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.81221 0.      0.     ] total reward: 0.81221\n",
      "UEHitrate: 0.04222  edgeHitrate 0.25496 sumHitrate 0.29718  privacy: 1.17545\n",
      "\n",
      "--Time: Sat Oct  9 19:30:23 2021 Episode: 3   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.82711 0.      0.     ] total reward: 0.82711\n",
      "UEHitrate: 0.04118  edgeHitrate 0.2569 sumHitrate 0.29808  privacy: 1.14388\n",
      "\n",
      "--Time: Sat Oct  9 19:34:53 2021 Episode: 3   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.83827 0.      0.     ] total reward: 0.83827\n",
      "UEHitrate: 0.04099  edgeHitrate 0.25592 sumHitrate 0.29691  privacy: 1.12297\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 19:35:16 2021 Episode: 3   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.83899 0.      0.     ] total reward: 0.83899\n",
      "UEHitrate: 0.04082  edgeHitrate 0.25573 sumHitrate 0.29655  privacy: 1.12158\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 19:35:16 2021 Episode: 4   Index: 0   Loss: 5.05783 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 19:47:25 2021 Episode: 4   Index: 50000   Loss: 0.12321 --\n",
      "Reward: [0.34623 0.      0.     ] total reward: 0.34623\n",
      "UEHitrate: 0.02114  edgeHitrate 0.13682 sumHitrate 0.15796  privacy: 2.60189\n",
      "\n",
      "--Time: Sat Oct  9 19:59:12 2021 Episode: 4   Index: 100000   Loss: 0.12017 --\n",
      "Reward: [0.36486 0.      0.     ] total reward: 0.36486\n",
      "UEHitrate: 0.02147  edgeHitrate 0.1347 sumHitrate 0.15617  privacy: 2.41403\n",
      "\n",
      "--Time: Sat Oct  9 20:11:34 2021 Episode: 4   Index: 150000   Loss: 0.11762 --\n",
      "Reward: [0.38875 0.      0.     ] total reward: 0.38875\n",
      "UEHitrate: 0.02649  edgeHitrate 0.14548 sumHitrate 0.17197  privacy: 2.28804\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 20:24:59 2021 Episode: 4   Index: 198174   Loss: 0.11291 --\n",
      "Reward: [0.40744 0.      0.     ] total reward: 0.40744\n",
      "UEHitrate: 0.02937  edgeHitrate 0.14901 sumHitrate 0.17838  privacy: 2.17382\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 20:24:59 2021 Episode: 4   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 20:30:41 2021 Episode: 4   Index: 50000   Loss: 0.0 --\n",
      "Reward: [1.07892 0.      0.     ] total reward: 1.07892\n",
      "UEHitrate: 0.0431  edgeHitrate 0.33247 sumHitrate 0.37557  privacy: 0.90629\n",
      "\n",
      "--Time: Sat Oct  9 20:36:09 2021 Episode: 4   Index: 100000   Loss: 0.0 --\n",
      "Reward: [1.07884 0.      0.     ] total reward: 1.07884\n",
      "UEHitrate: 0.0448  edgeHitrate 0.30879 sumHitrate 0.35359  privacy: 0.91596\n",
      "\n",
      "--Time: Sat Oct  9 20:41:35 2021 Episode: 4   Index: 150000   Loss: 0.0 --\n",
      "Reward: [1.08268 0.      0.     ] total reward: 1.08268\n",
      "UEHitrate: 0.04592  edgeHitrate 0.29938 sumHitrate 0.3453  privacy: 0.92303\n",
      "\n",
      "--Time: Sat Oct  9 20:46:36 2021 Episode: 4   Index: 200000   Loss: 0.0 --\n",
      "Reward: [1.08352 0.      0.     ] total reward: 1.08352\n",
      "UEHitrate: 0.04498  edgeHitrate 0.29532 sumHitrate 0.3403  privacy: 0.92105\n",
      "\n",
      "--Time: Sat Oct  9 20:51:35 2021 Episode: 4   Index: 250000   Loss: 0.0 --\n",
      "Reward: [1.08666 0.      0.     ] total reward: 1.08666\n",
      "UEHitrate: 0.04482  edgeHitrate 0.28875 sumHitrate 0.33357  privacy: 0.91333\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 20:51:59 2021 Episode: 4   Index: 253629   Loss: 0.0 --\n",
      "Reward: [1.08676 0.      0.     ] total reward: 1.08676\n",
      "UEHitrate: 0.0446  edgeHitrate 0.28819 sumHitrate 0.33278  privacy: 0.91399\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 20:51:59 2021 Episode: 5   Index: 0   Loss: 4.43343 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 21:04:53 2021 Episode: 5   Index: 50000   Loss: 0.11098 --\n",
      "Reward: [0.37924 0.      0.     ] total reward: 0.37924\n",
      "UEHitrate: 0.0379  edgeHitrate 0.19252 sumHitrate 0.23042  privacy: 2.43903\n",
      "\n",
      "--Time: Sat Oct  9 21:16:59 2021 Episode: 5   Index: 100000   Loss: 0.10454 --\n",
      "Reward: [0.39152 0.      0.     ] total reward: 0.39152\n",
      "UEHitrate: 0.03749  edgeHitrate 0.17898 sumHitrate 0.21647  privacy: 2.33099\n",
      "\n",
      "--Time: Sat Oct  9 21:29:33 2021 Episode: 5   Index: 150000   Loss: 0.10196 --\n",
      "Reward: [0.40986 0.      0.     ] total reward: 0.40986\n",
      "UEHitrate: 0.04072  edgeHitrate 0.17715 sumHitrate 0.21787  privacy: 2.2645\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 21:43:07 2021 Episode: 5   Index: 198174   Loss: 0.1001 --\n",
      "Reward: [0.42357 0.      0.     ] total reward: 0.42357\n",
      "UEHitrate: 0.04568  edgeHitrate 0.18713 sumHitrate 0.23281  privacy: 2.19079\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 21:43:07 2021 Episode: 5   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 21:49:02 2021 Episode: 5   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.80719 0.      0.     ] total reward: 0.80719\n",
      "UEHitrate: 0.0483  edgeHitrate 0.26879 sumHitrate 0.31709  privacy: 1.20034\n",
      "\n",
      "--Time: Sat Oct  9 21:54:26 2021 Episode: 5   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.85607 0.      0.     ] total reward: 0.85607\n",
      "UEHitrate: 0.04954  edgeHitrate 0.25604 sumHitrate 0.30558  privacy: 1.10812\n",
      "\n",
      "--Time: Sat Oct  9 21:59:09 2021 Episode: 5   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.88077 0.      0.     ] total reward: 0.88077\n",
      "UEHitrate: 0.05148  edgeHitrate 0.25488 sumHitrate 0.30636  privacy: 1.07424\n",
      "\n",
      "--Time: Sat Oct  9 22:03:35 2021 Episode: 5   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.89667 0.      0.     ] total reward: 0.89667\n",
      "UEHitrate: 0.05241  edgeHitrate 0.25679 sumHitrate 0.3092  privacy: 1.05881\n",
      "\n",
      "--Time: Sat Oct  9 22:08:07 2021 Episode: 5   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.90831 0.      0.     ] total reward: 0.90831\n",
      "UEHitrate: 0.05389  edgeHitrate 0.25614 sumHitrate 0.31003  privacy: 1.04916\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 22:08:30 2021 Episode: 5   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.90905 0.      0.     ] total reward: 0.90905\n",
      "UEHitrate: 0.05378  edgeHitrate 0.25583 sumHitrate 0.3096  privacy: 1.04864\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 22:08:30 2021 Episode: 6   Index: 0   Loss: 3.93946 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 22:21:05 2021 Episode: 6   Index: 50000   Loss: 0.10199 --\n",
      "Reward: [0.40995 0.      0.     ] total reward: 0.40995\n",
      "UEHitrate: 0.03508  edgeHitrate 0.20084 sumHitrate 0.23592  privacy: 2.27281\n",
      "\n",
      "--Time: Sat Oct  9 22:33:17 2021 Episode: 6   Index: 100000   Loss: 0.10019 --\n",
      "Reward: [0.41934 0.      0.     ] total reward: 0.41934\n",
      "UEHitrate: 0.03687  edgeHitrate 0.19181 sumHitrate 0.22868  privacy: 2.21766\n",
      "\n",
      "--Time: Sat Oct  9 22:46:28 2021 Episode: 6   Index: 150000   Loss: 0.09891 --\n",
      "Reward: [0.43434 0.      0.     ] total reward: 0.43434\n",
      "UEHitrate: 0.03867  edgeHitrate 0.19195 sumHitrate 0.23061  privacy: 2.19293\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 23:00:08 2021 Episode: 6   Index: 198174   Loss: 0.09783 --\n",
      "Reward: [0.44524 0.      0.     ] total reward: 0.44524\n",
      "UEHitrate: 0.04016  edgeHitrate 0.19031 sumHitrate 0.23047  privacy: 2.15096\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 23:00:08 2021 Episode: 6   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 23:05:41 2021 Episode: 6   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.84078 0.      0.     ] total reward: 0.84078\n",
      "UEHitrate: 0.04416  edgeHitrate 0.29829 sumHitrate 0.34245  privacy: 1.12766\n",
      "\n",
      "--Time: Sat Oct  9 23:10:36 2021 Episode: 6   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.88936 0.      0.     ] total reward: 0.88936\n",
      "UEHitrate: 0.04478  edgeHitrate 0.28514 sumHitrate 0.32992  privacy: 1.06499\n",
      "\n",
      "--Time: Sat Oct  9 23:15:55 2021 Episode: 6   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.91308 0.      0.     ] total reward: 0.91308\n",
      "UEHitrate: 0.0466  edgeHitrate 0.27946 sumHitrate 0.32606  privacy: 1.04453\n",
      "\n",
      "--Time: Sat Oct  9 23:21:40 2021 Episode: 6   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.92672 0.      0.     ] total reward: 0.92672\n",
      "UEHitrate: 0.04716  edgeHitrate 0.27811 sumHitrate 0.32527  privacy: 1.03443\n",
      "\n",
      "--Time: Sat Oct  9 23:26:45 2021 Episode: 6   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.93634 0.      0.     ] total reward: 0.93634\n",
      "UEHitrate: 0.04768  edgeHitrate 0.27395 sumHitrate 0.32162  privacy: 1.02856\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 23:27:04 2021 Episode: 6   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.93695 0.      0.     ] total reward: 0.93695\n",
      "UEHitrate: 0.04753  edgeHitrate 0.27352 sumHitrate 0.32105  privacy: 1.02824\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 23:27:04 2021 Episode: 7   Index: 0   Loss: 3.41811 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 23:39:51 2021 Episode: 7   Index: 50000   Loss: 0.08582 --\n",
      "Reward: [0.44632 0.      0.     ] total reward: 0.44632\n",
      "UEHitrate: 0.03744  edgeHitrate 0.20112 sumHitrate 0.23856  privacy: 2.11473\n",
      "\n",
      "--Time: Sat Oct  9 23:52:46 2021 Episode: 7   Index: 100000   Loss: 0.08441 --\n",
      "Reward: [0.45253 0.      0.     ] total reward: 0.45253\n",
      "UEHitrate: 0.03621  edgeHitrate 0.18759 sumHitrate 0.2238  privacy: 2.0879\n",
      "\n",
      "--Time: Sun Oct 10 00:06:13 2021 Episode: 7   Index: 150000   Loss: 0.08338 --\n",
      "Reward: [0.46326 0.      0.     ] total reward: 0.46326\n",
      "UEHitrate: 0.0336  edgeHitrate 0.18233 sumHitrate 0.21593  privacy: 2.08855\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 00:19:31 2021 Episode: 7   Index: 198174   Loss: 0.0826 --\n",
      "Reward: [0.47194 0.      0.     ] total reward: 0.47194\n",
      "UEHitrate: 0.02772  edgeHitrate 0.17057 sumHitrate 0.19829  privacy: 2.07462\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 00:19:31 2021 Episode: 7   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 00:25:01 2021 Episode: 7   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.78075 0.      0.     ] total reward: 0.78075\n",
      "UEHitrate: 0.04606  edgeHitrate 0.23362 sumHitrate 0.27967  privacy: 1.24154\n",
      "\n",
      "--Time: Sun Oct 10 00:30:04 2021 Episode: 7   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.8301 0.     0.    ] total reward: 0.8301\n",
      "UEHitrate: 0.04497  edgeHitrate 0.23406 sumHitrate 0.27903  privacy: 1.14659\n",
      "\n",
      "--Time: Sun Oct 10 00:35:09 2021 Episode: 7   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.8537 0.     0.    ] total reward: 0.8537\n",
      "UEHitrate: 0.0452  edgeHitrate 0.2393 sumHitrate 0.2845  privacy: 1.1022\n",
      "\n",
      "--Time: Sun Oct 10 00:40:41 2021 Episode: 7   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.87055 0.      0.     ] total reward: 0.87055\n",
      "UEHitrate: 0.04492  edgeHitrate 0.24722 sumHitrate 0.29214  privacy: 1.08131\n",
      "\n",
      "--Time: Sun Oct 10 00:45:38 2021 Episode: 7   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.88369 0.      0.     ] total reward: 0.88369\n",
      "UEHitrate: 0.04524  edgeHitrate 0.25124 sumHitrate 0.29648  privacy: 1.06829\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 00:46:00 2021 Episode: 7   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.88457 0.      0.     ] total reward: 0.88457\n",
      "UEHitrate: 0.0451  edgeHitrate 0.25122 sumHitrate 0.29632  privacy: 1.06765\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 00:46:00 2021 Episode: 8   Index: 0   Loss: 3.02389 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 00:59:43 2021 Episode: 8   Index: 50000   Loss: 0.08495 --\n",
      "Reward: [0.43888 0.      0.     ] total reward: 0.43888\n",
      "UEHitrate: 0.01556  edgeHitrate 0.16476 sumHitrate 0.18032  privacy: 2.26615\n",
      "\n",
      "--Time: Sun Oct 10 01:12:57 2021 Episode: 8   Index: 100000   Loss: 0.08343 --\n",
      "Reward: [0.44407 0.      0.     ] total reward: 0.44407\n",
      "UEHitrate: 0.0156  edgeHitrate 0.18742 sumHitrate 0.20302  privacy: 2.08897\n",
      "\n",
      "--Time: Sun Oct 10 01:26:25 2021 Episode: 8   Index: 150000   Loss: 0.08259 --\n",
      "Reward: [0.4639 0.     0.    ] total reward: 0.4639\n",
      "UEHitrate: 0.02293  edgeHitrate 0.21441 sumHitrate 0.23735  privacy: 2.01059\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 01:40:19 2021 Episode: 8   Index: 198174   Loss: 0.08142 --\n",
      "Reward: [0.47815 0.      0.     ] total reward: 0.47815\n",
      "UEHitrate: 0.02873  edgeHitrate 0.21892 sumHitrate 0.24764  privacy: 1.97365\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 01:40:19 2021 Episode: 8   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 01:45:22 2021 Episode: 8   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.79796 0.      0.     ] total reward: 0.79796\n",
      "UEHitrate: 0.03346  edgeHitrate 0.43309 sumHitrate 0.46655  privacy: 1.19208\n",
      "\n",
      "--Time: Sun Oct 10 01:50:35 2021 Episode: 8   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.85492 0.      0.     ] total reward: 0.85492\n",
      "UEHitrate: 0.03708  edgeHitrate 0.42038 sumHitrate 0.45746  privacy: 1.07197\n",
      "\n",
      "--Time: Sun Oct 10 01:56:07 2021 Episode: 8   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.89376 0.      0.     ] total reward: 0.89376\n",
      "UEHitrate: 0.0383  edgeHitrate 0.41133 sumHitrate 0.44963  privacy: 1.02202\n",
      "\n",
      "--Time: Sun Oct 10 02:01:38 2021 Episode: 8   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.91765 0.      0.     ] total reward: 0.91765\n",
      "UEHitrate: 0.04087  edgeHitrate 0.40263 sumHitrate 0.44351  privacy: 1.0064\n",
      "\n",
      "--Time: Sun Oct 10 02:06:41 2021 Episode: 8   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.93708 0.      0.     ] total reward: 0.93708\n",
      "UEHitrate: 0.03973  edgeHitrate 0.39357 sumHitrate 0.43331  privacy: 0.99428\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 02:07:01 2021 Episode: 8   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.93824 0.      0.     ] total reward: 0.93824\n",
      "UEHitrate: 0.03952  edgeHitrate 0.39249 sumHitrate 0.43201  privacy: 0.99412\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 02:07:01 2021 Episode: 9   Index: 0   Loss: 2.49821 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 02:20:11 2021 Episode: 9   Index: 50000   Loss: 0.07981 --\n",
      "Reward: [0.46357 0.      0.     ] total reward: 0.46357\n",
      "UEHitrate: 0.01692  edgeHitrate 0.36881 sumHitrate 0.38573  privacy: 2.05717\n",
      "\n",
      "--Time: Sun Oct 10 02:33:11 2021 Episode: 9   Index: 100000   Loss: 0.07828 --\n",
      "Reward: [0.4684 0.     0.    ] total reward: 0.4684\n",
      "UEHitrate: 0.01819  edgeHitrate 0.35683 sumHitrate 0.37502  privacy: 2.05688\n",
      "\n",
      "--Time: Sun Oct 10 02:46:04 2021 Episode: 9   Index: 150000   Loss: 0.07739 --\n",
      "Reward: [0.48324 0.      0.     ] total reward: 0.48324\n",
      "UEHitrate: 0.02469  edgeHitrate 0.34494 sumHitrate 0.36962  privacy: 1.94425\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 02:58:59 2021 Episode: 9   Index: 198174   Loss: 0.07672 --\n",
      "Reward: [0.49857 0.      0.     ] total reward: 0.49857\n",
      "UEHitrate: 0.03035  edgeHitrate 0.32411 sumHitrate 0.35446  privacy: 1.89059\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 02:59:00 2021 Episode: 9   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 03:04:26 2021 Episode: 9   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.40361 0.      0.     ] total reward: 0.40361\n",
      "UEHitrate: 0.01152  edgeHitrate 0.13764 sumHitrate 0.14916  privacy: 2.09612\n",
      "\n",
      "--Time: Sun Oct 10 03:10:08 2021 Episode: 9   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.45467 0.      0.     ] total reward: 0.45467\n",
      "UEHitrate: 0.02356  edgeHitrate 0.20995 sumHitrate 0.23351  privacy: 1.92257\n",
      "\n",
      "--Time: Sun Oct 10 03:15:59 2021 Episode: 9   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.50853 0.      0.     ] total reward: 0.50853\n",
      "UEHitrate: 0.02878  edgeHitrate 0.26215 sumHitrate 0.29093  privacy: 1.82586\n",
      "\n",
      "--Time: Sun Oct 10 03:21:44 2021 Episode: 9   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.54705 0.      0.     ] total reward: 0.54705\n",
      "UEHitrate: 0.03328  edgeHitrate 0.28915 sumHitrate 0.32243  privacy: 1.76059\n",
      "\n",
      "--Time: Sun Oct 10 03:27:43 2021 Episode: 9   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.57629 0.      0.     ] total reward: 0.57629\n",
      "UEHitrate: 0.0337  edgeHitrate 0.30271 sumHitrate 0.33642  privacy: 1.70769\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 03:28:13 2021 Episode: 9   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.57795 0.      0.     ] total reward: 0.57795\n",
      "UEHitrate: 0.03363  edgeHitrate 0.30287 sumHitrate 0.33651  privacy: 1.70504\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_ep0_1009-15-40-22'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sun Oct 10 03:28:13 2021   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 03:29:27 2021   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.54667 0.      0.     ] total reward: 0.54667\n",
      "UEHitrate: 0.0173  edgeHitrate 0.42366 sumHitrate 0.44096  privacy: 1.38968\n",
      "\n",
      "--Time: Sun Oct 10 03:30:39 2021   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.60237 0.      0.     ] total reward: 0.60237\n",
      "UEHitrate: 0.01315  edgeHitrate 0.43708 sumHitrate 0.45023  privacy: 1.21158\n",
      "\n",
      "--Time: Sun Oct 10 03:31:51 2021   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.65853 0.      0.     ] total reward: 0.65853\n",
      "UEHitrate: 0.01117  edgeHitrate 0.38755 sumHitrate 0.39872  privacy: 1.06844\n",
      "\n",
      "--Time: Sun Oct 10 03:33:01 2021   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.71029 0.      0.     ] total reward: 0.71029\n",
      "UEHitrate: 0.01047  edgeHitrate 0.35824 sumHitrate 0.36872  privacy: 0.98793\n",
      "\n",
      "--Time: Sun Oct 10 03:34:13 2021   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.75375 0.      0.     ] total reward: 0.75375\n",
      "UEHitrate: 0.00968  edgeHitrate 0.33885 sumHitrate 0.34853  privacy: 0.93485\n",
      "\n",
      "--Time: Sun Oct 10 03:35:17 2021   Index: 60000   Loss: 0.0 --\n",
      "Reward: [0.79456 0.      0.     ] total reward: 0.79456\n",
      "UEHitrate: 0.0092  edgeHitrate 0.32328 sumHitrate 0.33248  privacy: 0.89527\n",
      "\n",
      "--Time: Sun Oct 10 03:36:31 2021   Index: 70000   Loss: 0.0 --\n",
      "Reward: [0.82972 0.      0.     ] total reward: 0.82972\n",
      "UEHitrate: 0.00853  edgeHitrate 0.30714 sumHitrate 0.31567  privacy: 0.86556\n",
      "\n",
      "--Time: Sun Oct 10 03:37:37 2021   Index: 80000   Loss: 0.0 --\n",
      "Reward: [0.86529 0.      0.     ] total reward: 0.86529\n",
      "UEHitrate: 0.00806  edgeHitrate 0.29782 sumHitrate 0.30588  privacy: 0.8385\n",
      "\n",
      "--Time: Sun Oct 10 03:38:48 2021   Index: 90000   Loss: 0.0 --\n",
      "Reward: [0.89769 0.      0.     ] total reward: 0.89769\n",
      "UEHitrate: 0.00771  edgeHitrate 0.28713 sumHitrate 0.29484  privacy: 0.80671\n",
      "\n",
      "--Time: Sun Oct 10 03:39:26 2021   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.93094 0.      0.     ] total reward: 0.93094\n",
      "UEHitrate: 0.00729  edgeHitrate 0.27768 sumHitrate 0.28497  privacy: 0.77223\n",
      "\n",
      "--Time: Sun Oct 10 03:40:07 2021   Index: 110000   Loss: 0.0 --\n",
      "Reward: [0.96464 0.      0.     ] total reward: 0.96464\n",
      "UEHitrate: 0.00697  edgeHitrate 0.27021 sumHitrate 0.27718  privacy: 0.73948\n",
      "\n",
      "--Time: Sun Oct 10 03:40:50 2021   Index: 120000   Loss: 0.0 --\n",
      "Reward: [0.99692 0.      0.     ] total reward: 0.99692\n",
      "UEHitrate: 0.00673  edgeHitrate 0.26298 sumHitrate 0.26971  privacy: 0.70591\n",
      "\n",
      "--Time: Sun Oct 10 03:41:32 2021   Index: 130000   Loss: 0.0 --\n",
      "Reward: [1.03003 0.      0.     ] total reward: 1.03003\n",
      "UEHitrate: 0.00668  edgeHitrate 0.25651 sumHitrate 0.26319  privacy: 0.678\n",
      "\n",
      "--Time: Sun Oct 10 03:42:15 2021   Index: 140000   Loss: 0.0 --\n",
      "Reward: [1.06108 0.      0.     ] total reward: 1.06108\n",
      "UEHitrate: 0.00665  edgeHitrate 0.25473 sumHitrate 0.26138  privacy: 0.64757\n",
      "\n",
      "--Time: Sun Oct 10 03:43:01 2021   Index: 150000   Loss: 0.0 --\n",
      "Reward: [1.09292 0.      0.     ] total reward: 1.09292\n",
      "UEHitrate: 0.00652  edgeHitrate 0.25168 sumHitrate 0.2582  privacy: 0.62521\n",
      "\n",
      "--Time: Sun Oct 10 03:43:43 2021   Index: 160000   Loss: 0.0 --\n",
      "Reward: [1.12455 0.      0.     ] total reward: 1.12455\n",
      "UEHitrate: 0.00632  edgeHitrate 0.2462 sumHitrate 0.25253  privacy: 0.60409\n",
      "\n",
      "--Time: Sun Oct 10 03:44:24 2021   Index: 170000   Loss: 0.0 --\n",
      "Reward: [1.15527 0.      0.     ] total reward: 1.15527\n",
      "UEHitrate: 0.00617  edgeHitrate 0.24385 sumHitrate 0.25002  privacy: 0.59137\n",
      "\n",
      "--Time: Sun Oct 10 03:45:05 2021   Index: 180000   Loss: 0.0 --\n",
      "Reward: [1.18188 0.      0.     ] total reward: 1.18188\n",
      "UEHitrate: 0.00599  edgeHitrate 0.24263 sumHitrate 0.24863  privacy: 0.57994\n",
      "\n",
      "--Time: Sun Oct 10 03:45:47 2021   Index: 190000   Loss: 0.0 --\n",
      "Reward: [1.20871 0.      0.     ] total reward: 1.20871\n",
      "UEHitrate: 0.00594  edgeHitrate 0.23951 sumHitrate 0.24545  privacy: 0.5685\n",
      "\n",
      "--Time: Sun Oct 10 03:46:29 2021   Index: 200000   Loss: 0.0 --\n",
      "Reward: [1.23425 0.      0.     ] total reward: 1.23425\n",
      "UEHitrate: 0.00581  edgeHitrate 0.23619 sumHitrate 0.242  privacy: 0.56287\n",
      "\n",
      "--Time: Sun Oct 10 03:47:12 2021   Index: 210000   Loss: 0.0 --\n",
      "Reward: [1.25859 0.      0.     ] total reward: 1.25859\n",
      "UEHitrate: 0.00576  edgeHitrate 0.23546 sumHitrate 0.24121  privacy: 0.55089\n",
      "\n",
      "--Time: Sun Oct 10 03:47:54 2021   Index: 220000   Loss: 0.0 --\n",
      "Reward: [1.28453 0.      0.     ] total reward: 1.28453\n",
      "UEHitrate: 0.00567  edgeHitrate 0.23309 sumHitrate 0.23876  privacy: 0.53894\n",
      "\n",
      "--Time: Sun Oct 10 03:48:38 2021   Index: 230000   Loss: 0.0 --\n",
      "Reward: [1.30912 0.      0.     ] total reward: 1.30912\n",
      "UEHitrate: 0.00562  edgeHitrate 0.2309 sumHitrate 0.23652  privacy: 0.52022\n",
      "\n",
      "--Time: Sun Oct 10 03:49:21 2021   Index: 240000   Loss: 0.0 --\n",
      "Reward: [1.33369 0.      0.     ] total reward: 1.33369\n",
      "UEHitrate: 0.00556  edgeHitrate 0.22837 sumHitrate 0.23393  privacy: 0.50834\n",
      "\n",
      "--Time: Sun Oct 10 03:50:02 2021   Index: 250000   Loss: 0.0 --\n",
      "Reward: [1.35867 0.      0.     ] total reward: 1.35867\n",
      "UEHitrate: 0.00555  edgeHitrate 0.22702 sumHitrate 0.23257  privacy: 0.49501\n",
      "\n",
      "--Time: Sun Oct 10 03:50:45 2021   Index: 260000   Loss: 0.0 --\n",
      "Reward: [1.38641 0.      0.     ] total reward: 1.38641\n",
      "UEHitrate: 0.00553  edgeHitrate 0.22532 sumHitrate 0.23085  privacy: 0.47845\n",
      "\n",
      "--Time: Sun Oct 10 03:51:27 2021   Index: 270000   Loss: 0.0 --\n",
      "Reward: [1.41453 0.      0.     ] total reward: 1.41453\n",
      "UEHitrate: 0.00553  edgeHitrate 0.22315 sumHitrate 0.22868  privacy: 0.46968\n",
      "\n",
      "--Time: Sun Oct 10 03:52:11 2021   Index: 280000   Loss: 0.0 --\n",
      "Reward: [1.44338 0.      0.     ] total reward: 1.44338\n",
      "UEHitrate: 0.00548  edgeHitrate 0.22285 sumHitrate 0.22833  privacy: 0.45501\n",
      "\n",
      "--Time: Sun Oct 10 03:52:57 2021   Index: 290000   Loss: 0.0 --\n",
      "Reward: [1.47049 0.      0.     ] total reward: 1.47049\n",
      "UEHitrate: 0.00543  edgeHitrate 0.22441 sumHitrate 0.22984  privacy: 0.44168\n",
      "\n",
      "--Time: Sun Oct 10 03:53:39 2021   Index: 300000   Loss: 0.0 --\n",
      "Reward: [1.49868 0.      0.     ] total reward: 1.49868\n",
      "UEHitrate: 0.00537  edgeHitrate 0.227 sumHitrate 0.23237  privacy: 0.42847\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 10 03:53:45 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [1.50187 0.      0.     ] total reward: 1.50187\n",
      "UEHitrate: 0.00538  edgeHitrate 0.22725 sumHitrate 0.23263  privacy: 0.42687\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_ep0_1009-15-40-22'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.44096, 0.45023, 0.39872, 0.36872, 0.34853, 0.33248,\n",
       "        0.31567, 0.30588, 0.29484, 0.28497, 0.27718, 0.26971, 0.26319,\n",
       "        0.26138, 0.2582 , 0.25253, 0.25002, 0.24863, 0.24545, 0.242  ,\n",
       "        0.24121, 0.23876, 0.23652, 0.23393, 0.23257, 0.23085, 0.22868,\n",
       "        0.22833, 0.22984, 0.23263]),\n",
       " array([0.     , 0.0173 , 0.01315, 0.01117, 0.01047, 0.00968, 0.0092 ,\n",
       "        0.00853, 0.00806, 0.00771, 0.00729, 0.00697, 0.00673, 0.00668,\n",
       "        0.00665, 0.00652, 0.00632, 0.00617, 0.00599, 0.00594, 0.00581,\n",
       "        0.00576, 0.00567, 0.00562, 0.00556, 0.00555, 0.00553, 0.00553,\n",
       "        0.00548, 0.00543, 0.00538]),\n",
       " array([0.     , 0.42366, 0.43708, 0.38755, 0.35824, 0.33885, 0.32328,\n",
       "        0.30714, 0.29782, 0.28713, 0.27768, 0.27021, 0.26298, 0.25651,\n",
       "        0.25473, 0.25168, 0.2462 , 0.24385, 0.24263, 0.23951, 0.23619,\n",
       "        0.23546, 0.23309, 0.2309 , 0.22837, 0.22702, 0.22532, 0.22315,\n",
       "        0.22285, 0.22441, 0.22725]))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4.02118, 1.38968, 1.21158, 1.06844, 0.98793, 0.93485, 0.89527,\n",
       "       0.86556, 0.8385 , 0.80671, 0.77223, 0.73948, 0.70591, 0.678  ,\n",
       "       0.64757, 0.62521, 0.60409, 0.59137, 0.57994, 0.5685 , 0.56287,\n",
       "       0.55089, 0.53894, 0.52022, 0.50834, 0.49501, 0.47845, 0.46968,\n",
       "       0.45501, 0.44168, 0.42687])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}