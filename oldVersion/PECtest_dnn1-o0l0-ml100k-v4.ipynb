{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2 增加神经网络输出\n",
    "#v3 修改reward为直接变化量\n",
    "#v4 删除e,S特征 \n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_o0l0_v4_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = '/home/ubuntu/data/dataset/ml-100k/'\n",
    "UIT = pd.read_csv(data_path + 'ml_header.trace')\n",
    "\n",
    "trainUIT = UIT[UIT['day']<18]\n",
    "validUIT = UIT[UIT['day']<24]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT.shape,validUIT.shape\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0,\"betal\":0}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 15\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    " \n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        #statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        statusFeature = torch.zeros(size=(3,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        #statusFeature[3] = self.e\n",
    "        #statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + torch.exp( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = self.ALPHAh * ( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "        if self.W[-1] not in np.argwhere(self.lastAction.numpy()): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            if  not train or (train and sample > eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                with torch.no_grad():\n",
    "                    Q_value = QNetwork(self.statusFeature)\n",
    "                    actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "                    #actionIndex = list(Q_value.squeeze(1).argsort(descending=True)[0:self.Bu])\n",
    "                QNetwork.train()\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "\n",
    "            self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "            self.action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                self.action[index] = 1\n",
    "            env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "        else:\n",
    "            self.action = self.lastAction # keep the cache and no request the new video\n",
    "\n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "\n",
    "        if train:\n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                        lastAction.to(device), \n",
    "                        self.statusFeature,\n",
    "                        torch.tensor([self.reward.float()]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "                        \n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    '''\n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    '''\n",
    "    #next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    #expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 22:58:31 2021 Episode: 0   Index: 65335   Loss: 0.1661 --\n",
      "Reward: [-2.39207  0.       0.     ] total reward: -2.39207\n",
      "UEHitrate: 0.01252  edgeHitrate 0.32284 sumHitrate 0.33536  privacy: 1.77304\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 23:00:57 2021 Episode: 0   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.36712  0.       0.     ] total reward: -0.36712\n",
      "UEHitrate: 0.0  edgeHitrate 0.6389 sumHitrate 0.6389  privacy: 0.91974\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v4_ep0_1010-23-00-57\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 23:02:48 2021 Episode: 1   Index: 65335   Loss: 0.14418 --\n",
      "Reward: [-2.04614  0.       0.     ] total reward: -2.04614\n",
      "UEHitrate: 0.0054  edgeHitrate 0.31396 sumHitrate 0.31936  privacy: 1.91198\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 23:05:11 2021 Episode: 1   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.54948  0.       0.     ] total reward: -0.54948\n",
      "UEHitrate: 9e-05  edgeHitrate 0.39881 sumHitrate 0.3989  privacy: 1.79489\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 23:07:15 2021 Episode: 2   Index: 65335   Loss: 0.1352 --\n",
      "Reward: [-2.03591  0.       0.     ] total reward: -2.03591\n",
      "UEHitrate: 0.00467  edgeHitrate 0.32025 sumHitrate 0.32492  privacy: 1.91538\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 23:09:39 2021 Episode: 2   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.35129  0.       0.     ] total reward: -0.35129\n",
      "UEHitrate: 0.00052  edgeHitrate 0.54777 sumHitrate 0.54829  privacy: 1.22703\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v4_ep2_1010-23-09-39\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 23:11:48 2021 Episode: 3   Index: 65335   Loss: 0.12948 --\n",
      "Reward: [-1.87903  0.       0.     ] total reward: -1.87903\n",
      "UEHitrate: 0.00491  edgeHitrate 0.32974 sumHitrate 0.33465  privacy: 2.01389\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 23:14:11 2021 Episode: 3   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.35127  0.       0.     ] total reward: -0.35127\n",
      "UEHitrate: 0.00102  edgeHitrate 0.50272 sumHitrate 0.50375  privacy: 1.22791\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v4_ep3_1010-23-14-11\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 23:16:28 2021 Episode: 4   Index: 65335   Loss: 0.1191 --\n",
      "Reward: [-2.59431  0.       0.     ] total reward: -2.59431\n",
      "UEHitrate: 0.01166  edgeHitrate 0.30112 sumHitrate 0.31278  privacy: 1.45888\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 23:19:01 2021 Episode: 4   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.18537  0.       0.     ] total reward: -3.18537\n",
      "UEHitrate: 0.01616  edgeHitrate 0.29604 sumHitrate 0.3122  privacy: 1.05441\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 23:21:19 2021 Episode: 5   Index: 65335   Loss: 0.14439 --\n",
      "Reward: [-3.24171  0.       0.     ] total reward: -3.24171\n",
      "UEHitrate: 0.00958  edgeHitrate 0.28839 sumHitrate 0.29797  privacy: 1.35897\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 23:23:48 2021 Episode: 5   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.21483  0.       0.     ] total reward: -3.21483\n",
      "UEHitrate: 0.0114  edgeHitrate 0.30245 sumHitrate 0.31385  privacy: 0.93029\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 23:26:08 2021 Episode: 6   Index: 65335   Loss: 0.15277 --\n",
      "Reward: [-3.27148  0.       0.     ] total reward: -3.27148\n",
      "UEHitrate: 0.00889  edgeHitrate 0.29025 sumHitrate 0.29915  privacy: 1.35279\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 23:29:07 2021 Episode: 6   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.21757  0.       0.     ] total reward: -3.21757\n",
      "UEHitrate: 0.00899  edgeHitrate 0.29465 sumHitrate 0.30363  privacy: 1.00083\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 23:31:27 2021 Episode: 7   Index: 65335   Loss: 0.15564 --\n",
      "Reward: [-3.32148  0.       0.     ] total reward: -3.32148\n",
      "UEHitrate: 0.00742  edgeHitrate 0.29318 sumHitrate 0.3006  privacy: 1.33486\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 23:34:01 2021 Episode: 7   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.20086  0.       0.     ] total reward: -3.20086\n",
      "UEHitrate: 0.00851  edgeHitrate 0.31361 sumHitrate 0.32211  privacy: 1.00113\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 23:36:27 2021 Episode: 8   Index: 65335   Loss: 0.16764 --\n",
      "Reward: [-3.32706  0.       0.     ] total reward: -3.32706\n",
      "UEHitrate: 0.00765  edgeHitrate 0.29982 sumHitrate 0.30747  privacy: 1.28313\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 23:39:01 2021 Episode: 8   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.23189  0.       0.     ] total reward: -3.23189\n",
      "UEHitrate: 0.00925  edgeHitrate 0.31149 sumHitrate 0.32073  privacy: 1.03266\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 23:41:26 2021 Episode: 9   Index: 65335   Loss: 0.16213 --\n",
      "Reward: [-3.17155  0.       0.     ] total reward: -3.17155\n",
      "UEHitrate: 0.00799  edgeHitrate 0.30559 sumHitrate 0.31358  privacy: 1.46052\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 23:43:58 2021 Episode: 9   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-2.69491  0.       0.     ] total reward: -2.69491\n",
      "UEHitrate: 0.00518  edgeHitrate 0.33157 sumHitrate 0.33675  privacy: 1.08461\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sun Oct 10 23:44:18 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-0.57847  0.       0.     ] total reward: -0.57847\n",
      "UEHitrate: 0.0021  edgeHitrate 0.5055 sumHitrate 0.5076  privacy: 1.5913\n",
      "\n",
      "--Time: Sun Oct 10 23:44:34 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-0.53273  0.       0.     ] total reward: -0.53273\n",
      "UEHitrate: 0.002  edgeHitrate 0.5081 sumHitrate 0.5101  privacy: 1.43225\n",
      "\n",
      "--Time: Sun Oct 10 23:44:51 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-0.41738  0.       0.     ] total reward: -0.41738\n",
      "UEHitrate: 0.0015  edgeHitrate 0.5138 sumHitrate 0.5153  privacy: 1.37789\n",
      "\n",
      "--Time: Sun Oct 10 23:45:08 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-0.39213  0.       0.     ] total reward: -0.39213\n",
      "UEHitrate: 0.00135  edgeHitrate 0.51658 sumHitrate 0.51792  privacy: 1.32652\n",
      "\n",
      "--Time: Sun Oct 10 23:45:25 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-0.36536  0.       0.     ] total reward: -0.36536\n",
      "UEHitrate: 0.00118  edgeHitrate 0.50796 sumHitrate 0.50914  privacy: 1.29165\n",
      "\n",
      "--Time: Sun Oct 10 23:45:42 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-0.36134  0.       0.     ] total reward: -0.36134\n",
      "UEHitrate: 0.00115  edgeHitrate 0.50782 sumHitrate 0.50897  privacy: 1.26989\n",
      "\n",
      "--Time: Sun Oct 10 23:45:59 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-0.36664  0.       0.     ] total reward: -0.36664\n",
      "UEHitrate: 0.00117  edgeHitrate 0.50493 sumHitrate 0.5061  privacy: 1.24792\n",
      "\n",
      "--Time: Sun Oct 10 23:46:16 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-0.3512  0.      0.    ] total reward: -0.3512\n",
      "UEHitrate: 0.00104  edgeHitrate 0.50246 sumHitrate 0.5035  privacy: 1.22997\n",
      "\n",
      "--Time: Sun Oct 10 23:46:33 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-0.32952  0.       0.     ] total reward: -0.32952\n",
      "UEHitrate: 0.00094  edgeHitrate 0.50951 sumHitrate 0.51046  privacy: 1.2159\n",
      "\n",
      "--Time: Sun Oct 10 23:46:50 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.31699  0.       0.     ] total reward: -0.31699\n",
      "UEHitrate: 0.00106  edgeHitrate 0.51036 sumHitrate 0.51142  privacy: 1.20007\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 10 23:46:51 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.31699  0.       0.     ] total reward: -0.31699\n",
      "UEHitrate: 0.00106  edgeHitrate 0.51036 sumHitrate 0.51142  privacy: 1.20007\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5076 , 0.5101 , 0.5153 , 0.51792, 0.50914, 0.50897, 0.5061 ,\n",
       "        0.5035 , 0.51046, 0.51142, 0.51142]),\n",
       " array([0.0021 , 0.002  , 0.0015 , 0.00135, 0.00118, 0.00115, 0.00117,\n",
       "        0.00104, 0.00094, 0.00106, 0.00106]),\n",
       " array([0.5055 , 0.5081 , 0.5138 , 0.51658, 0.50796, 0.50782, 0.50493,\n",
       "        0.50246, 0.50951, 0.51036, 0.51036]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5913 , 1.43225, 1.37789, 1.32652, 1.29165, 1.26989, 1.24792,\n",
       "       1.22997, 1.2159 , 1.20007, 1.20007])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v4_ep3_1010-23-14-11'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestPath = '/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v4_ep0_1010-23-00-57'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Mon Oct 11 15:08:04 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-0.56533  0.       0.     ] total reward: -0.56533\n",
      "UEHitrate: 0.0  edgeHitrate 0.6541 sumHitrate 0.6541  privacy: 1.20661\n",
      "\n",
      "--Time: Mon Oct 11 15:08:24 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-0.52987  0.       0.     ] total reward: -0.52987\n",
      "UEHitrate: 0.0  edgeHitrate 0.66895 sumHitrate 0.66895  privacy: 1.10211\n",
      "\n",
      "--Time: Mon Oct 11 15:08:45 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-0.41365  0.       0.     ] total reward: -0.41365\n",
      "UEHitrate: 0.0  edgeHitrate 0.6691 sumHitrate 0.6691  privacy: 1.04701\n",
      "\n",
      "--Time: Mon Oct 11 15:09:08 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-0.39273  0.       0.     ] total reward: -0.39273\n",
      "UEHitrate: 0.0  edgeHitrate 0.67015 sumHitrate 0.67015  privacy: 1.01568\n",
      "\n",
      "--Time: Mon Oct 11 15:09:32 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-0.37012  0.       0.     ] total reward: -0.37012\n",
      "UEHitrate: 0.0  edgeHitrate 0.66224 sumHitrate 0.66224  privacy: 0.99439\n",
      "\n",
      "--Time: Mon Oct 11 15:09:57 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-0.3702  0.      0.    ] total reward: -0.3702\n",
      "UEHitrate: 0.0  edgeHitrate 0.6604 sumHitrate 0.6604  privacy: 0.96747\n",
      "\n",
      "--Time: Mon Oct 11 15:10:21 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-0.37976  0.       0.     ] total reward: -0.37976\n",
      "UEHitrate: 0.0  edgeHitrate 0.64931 sumHitrate 0.64931  privacy: 0.94506\n",
      "\n",
      "--Time: Mon Oct 11 15:10:45 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-0.36659  0.       0.     ] total reward: -0.36659\n",
      "UEHitrate: 0.0  edgeHitrate 0.63889 sumHitrate 0.63889  privacy: 0.92289\n",
      "\n",
      "--Time: Mon Oct 11 15:11:09 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-0.34531  0.       0.     ] total reward: -0.34531\n",
      "UEHitrate: 0.0  edgeHitrate 0.64267 sumHitrate 0.64267  privacy: 0.87194\n",
      "\n",
      "--Time: Mon Oct 11 15:11:32 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.33449  0.       0.     ] total reward: -0.33449\n",
      "UEHitrate: 7e-05  edgeHitrate 0.63651 sumHitrate 0.63658  privacy: 0.72276\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Oct 11 15:11:32 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.33449  0.       0.     ] total reward: -0.33449\n",
      "UEHitrate: 7e-05  edgeHitrate 0.63651 sumHitrate 0.63658  privacy: 0.72276\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.6541 , 0.66895, 0.6691 , 0.67015, 0.66224, 0.6604 , 0.64931,\n",
       "        0.63889, 0.64267, 0.63658, 0.63658]),\n",
       " array([0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00, 0.e+00,\n",
       "        0.e+00, 7.e-05, 7.e-05]),\n",
       " array([0.6541 , 0.66895, 0.6691 , 0.67015, 0.66224, 0.6604 , 0.64931,\n",
       "        0.63889, 0.64267, 0.63651, 0.63651]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.20661, 1.10211, 1.04701, 1.01568, 0.99439, 0.96747, 0.94506,\n",
       "       0.92289, 0.87194, 0.72276, 0.72276])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v4_ep0_1010-23-00-57'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a71a010a29be3df77e49d8f42909021441fd02d28ccbb313c9d0d3f90d3253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
