{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,16)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.conn2 = nn.Linear(16,8)\n",
    "        self.bn2 = nn.BatchNorm1d(8)\n",
    "        self.conn3 = nn.Linear(8, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.bn1(self.conn1(x)))\n",
    "        x = F.relu(self.bn2(self.conn2(x)))\n",
    "        return self.conn3(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>video_type</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "      <th>level4</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3006</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>2232</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>2166</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>4578</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>1821</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79614</th>\n",
       "      <td>133</td>\n",
       "      <td>3911</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79615</th>\n",
       "      <td>133</td>\n",
       "      <td>1684</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79616</th>\n",
       "      <td>185</td>\n",
       "      <td>1552</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79617</th>\n",
       "      <td>61</td>\n",
       "      <td>4078</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79618</th>\n",
       "      <td>133</td>\n",
       "      <td>2568</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79619 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         u     i  day  video_type  level1  level2  level3  level4   time\n",
       "0        3  3006    0          11       0       0       0       0      0\n",
       "1       63  2232    0          11       0       0       0       0      0\n",
       "2       68  2166    0          11       0       0       0       0      0\n",
       "3       63  4578    0          11       0       0       0       0      0\n",
       "4       78  1821    0          47       0       0       0       0      0\n",
       "...    ...   ...  ...         ...     ...     ...     ...     ...    ...\n",
       "79614  133  3911   29          13       0       0       0       0  43197\n",
       "79615  133  1684   29          13       0       0       0       0  43198\n",
       "79616  185  1552   29          11       0       0       0       0  43198\n",
       "79617   61  4078   29          13       0       0       0       0  43199\n",
       "79618  133  2568   29          13       0       0       0       0  43199\n",
       "\n",
       "[79619 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R79619_U200_V5000/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=10,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "\n",
    "        self.p = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p) , \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                self.l_edge,\n",
    "                self.l_cp)\n",
    "\n",
    "    def reset(self):\n",
    "        self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "        self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "        self.e = np.zeros(shape=self.contentNum)\n",
    "        self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "        self.B = np.full(shape=self.userNum,fill_value=10,dtype=int)\n",
    "        self.pipe = collections.OrderedDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个神经网络单独作为一个reward进行训练\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']\n",
    "        self.EPS_START = 0.9\n",
    "        self.EPS_END = 0.1\n",
    "        self.EPS_DECAY = 10\n",
    "        \n",
    "        self.t = 0\n",
    "        \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e):\n",
    "\n",
    "        self.Rh =   self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)) - torch.log(lastru * lastp + (1-lastru) * (1-lastp)))\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        self.Rl =   self.BETAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        return  self.Rh\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "\n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.Bu,self.l_edge,self.l_cp,self.e)\n",
    "        \n",
    "        if train:\n",
    "            \n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                    lastAction.to(device), \n",
    "                    self.statusFeature,\n",
    "                    self.reward.float().to(device),\n",
    "                    torch.tensor([self.W[-1]]).to(device))\n",
    "        \n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) *  np.exp(-1. * self.t / self.EPS_DECAY)\n",
    "        self.t = self.t + 1\n",
    "        \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "        if  not train or (train and sample > eps_threshold):\n",
    "            QNetwork.eval()\n",
    "            with torch.no_grad():\n",
    "                Q_value = QNetwork(self.statusFeature)\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "            QNetwork.train()\n",
    "        else:\n",
    "            actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.63\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.masked_select(policy_net(state_batch),state_action_mask_bacth)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort()[0:10])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.masked_select(Q_value,action_mask).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELPATH =  './model_dict/dnn_l0_'\n",
    "rewardPara = {\"alpha\":1,\"betao\":5,\"betal\":0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: Tue Sep 14 00:47:20 2021 --Episode: 0   Index: 0   Reward: 0.0   Loss: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 00:50:25 2021 --Episode: 0   Index: 10000   Reward: 1.0417135812212377   Loss: 0.00011844962348165392\n",
      "UEHitrate: 0.0124987501249875  edgeHitrate 0.29217078292170784 sumHitrate 0.3046695330466953  privacy: tensor(2.1555, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 00:53:13 2021 --Episode: 0   Index: 20000   Reward: 0.48869786186548997   Loss: 6.725125172491725e-05\n",
      "UEHitrate: 0.015499225038748062  edgeHitrate 0.26538673066346685 sumHitrate 0.28088595570221486  privacy: tensor(2.0178, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 00:56:10 2021 --Episode: 0   Index: 30000   Reward: 0.3515378633093137   Loss: 4.633755886850308e-05\n",
      "UEHitrate: 0.015366154461517949  edgeHitrate 0.2626912436252125 sumHitrate 0.27805739808673047  privacy: tensor(1.8394, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 00:59:12 2021 --Episode: 0   Index: 40000   Reward: 0.28128021744851495   Loss: 3.589939218075795e-05\n",
      "UEHitrate: 0.015224619384515387  edgeHitrate 0.27921801954951125 sumHitrate 0.29444263893402667  privacy: tensor(1.7736, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:02:16 2021 --Episode: 0   Index: 50000   Reward: 0.22447834992634483   Loss: 2.921974088247478e-05\n",
      "UEHitrate: 0.015299694006119878  edgeHitrate 0.27687446251074976 sumHitrate 0.2921741565168697  privacy: tensor(1.7094, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 01:02:20 2021 --End episode: 0   Reward: 0.22320387183327622   Loss: 2.9091608298295988e-05\n",
      "UEHitrate: 0.01536715966319645  edgeHitrate 0.2765491569958397 sumHitrate 0.29191631665903617  privacy: tensor(1.7085, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 01:02:21 2021 --Episode: 1   Index: 0   Reward: 0.0   Loss: 1.941700975294225e-05\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:05:22 2021 --Episode: 1   Index: 10000   Reward: 0.6192417152096451   Loss: 5.344683432023858e-06\n",
      "UEHitrate: 0.014198580141985802  edgeHitrate 0.2405759424057594 sumHitrate 0.25477452254774524  privacy: tensor(2.2077, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:08:21 2021 --Episode: 1   Index: 20000   Reward: 0.31272149615415307   Loss: 5.359249063751043e-06\n",
      "UEHitrate: 0.011399430028498575  edgeHitrate 0.21308934553272335 sumHitrate 0.22448877556122193  privacy: tensor(2.0464, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:11:07 2021 --Episode: 1   Index: 30000   Reward: 0.20806483529385963   Loss: 4.662308935382597e-06\n",
      "UEHitrate: 0.010399653344888503  edgeHitrate 0.17276090796973434 sumHitrate 0.18316056131462286  privacy: tensor(1.9838, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:14:01 2021 --Episode: 1   Index: 40000   Reward: 0.16655799569467145   Loss: 4.148494822941402e-06\n",
      "UEHitrate: 0.010424739381515462  edgeHitrate 0.15794605134871628 sumHitrate 0.16837079073023176  privacy: tensor(1.9388, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:17:02 2021 --Episode: 1   Index: 50000   Reward: 0.13230125197575082   Loss: 3.738935927689854e-06\n",
      "UEHitrate: 0.010939781204375913  edgeHitrate 0.15731685366292675 sumHitrate 0.16825663486730266  privacy: tensor(1.8942, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 01:17:05 2021 --End episode: 1   Reward: 0.13203564690691685   Loss: 3.7316149437223743e-06\n",
      "UEHitrate: 0.010987917272130103  edgeHitrate 0.15733423572267452 sumHitrate 0.16832215299480463  privacy: tensor(1.8928, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 01:17:05 2021 --Episode: 2   Index: 0   Reward: 0.0   Loss: 4.833168350160122e-05\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:19:54 2021 --Episode: 2   Index: 10000   Reward: 0.5941034662752378   Loss: 4.651426198216479e-06\n",
      "UEHitrate: 0.007399260073992601  edgeHitrate 0.15248475152484753 sumHitrate 0.1598840115988401  privacy: tensor(2.2038, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:22:59 2021 --Episode: 2   Index: 20000   Reward: 0.29773886278874995   Loss: 4.950254380935245e-06\n",
      "UEHitrate: 0.007899605019749012  edgeHitrate 0.19549022548872558 sumHitrate 0.2033898305084746  privacy: tensor(2.0380, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:26:15 2021 --Episode: 2   Index: 30000   Reward: 0.20585976930275368   Loss: 4.370819594655063e-06\n",
      "UEHitrate: 0.010366321122629247  edgeHitrate 0.1914602846571781 sumHitrate 0.20182660577980735  privacy: tensor(1.9694, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:29:24 2021 --Episode: 2   Index: 40000   Reward: 0.16765519939913975   Loss: 4.0010357704238064e-06\n",
      "UEHitrate: 0.01307467313317167  edgeHitrate 0.18899527511812206 sumHitrate 0.2020699482512937  privacy: tensor(1.9274, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:32:36 2021 --Episode: 2   Index: 50000   Reward: 0.1317882617231005   Loss: 3.6510848949493085e-06\n",
      "UEHitrate: 0.013339733205335894  edgeHitrate 0.20333593328133437 sumHitrate 0.21667566648667028  privacy: tensor(1.8906, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 01:32:39 2021 --End episode: 2   Reward: 0.13140946973617104   Loss: 3.6448564129875387e-06\n",
      "UEHitrate: 0.013396500587216593  edgeHitrate 0.20371439377351355 sumHitrate 0.21711089436073014  privacy: tensor(1.8898, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 01:32:40 2021 --Episode: 3   Index: 0   Reward: 0.0   Loss: 9.547961781208869e-06\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:35:36 2021 --Episode: 3   Index: 10000   Reward: 0.5771775689983005   Loss: 4.481026280927813e-06\n",
      "UEHitrate: 0.007899210078992101  edgeHitrate 0.17678232176782321 sumHitrate 0.18468153184681532  privacy: tensor(2.1956, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:38:42 2021 --Episode: 3   Index: 20000   Reward: 0.2944777306557321   Loss: 4.8282034900749475e-06\n",
      "UEHitrate: 0.00839958002099895  edgeHitrate 0.21823908804559772 sumHitrate 0.22663866806659666  privacy: tensor(2.0443, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:41:57 2021 --Episode: 3   Index: 30000   Reward: 0.19219860464971703   Loss: 4.227703784258071e-06\n",
      "UEHitrate: 0.009666344455184828  edgeHitrate 0.2556248125062498 sumHitrate 0.2652911569614346  privacy: tensor(1.9765, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:44:59 2021 --Episode: 3   Index: 40000   Reward: 0.1555079075063057   Loss: 3.821854788753174e-06\n",
      "UEHitrate: 0.0112747181320467  edgeHitrate 0.28511787205319866 sumHitrate 0.2963925901852454  privacy: tensor(1.9349, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:48:03 2021 --Episode: 3   Index: 50000   Reward: 0.12326403134527163   Loss: 3.474681480680541e-06\n",
      "UEHitrate: 0.012319753604927902  edgeHitrate 0.2874142517149657 sumHitrate 0.2997340053198936  privacy: tensor(1.8978, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 01:48:07 2021 --End episode: 3   Reward: 0.12287567445712712   Loss: 3.4672636033577118e-06\n",
      "UEHitrate: 0.012341501283914246  edgeHitrate 0.28686028226207777 sumHitrate 0.299201783545992  privacy: tensor(1.8967, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 01:48:08 2021 --Episode: 4   Index: 0   Reward: 0.0   Loss: 1.0246390047541354e-05\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:51:12 2021 --Episode: 4   Index: 10000   Reward: 0.6133027152914817   Loss: 4.785679396750899e-06\n",
      "UEHitrate: 0.0082991700829917  edgeHitrate 0.1262873712628737 sumHitrate 0.13458654134586542  privacy: tensor(2.2037, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:54:28 2021 --Episode: 4   Index: 20000   Reward: 0.31137925363795604   Loss: 5.064655195165971e-06\n",
      "UEHitrate: 0.008999550022498875  edgeHitrate 0.14279286035698216 sumHitrate 0.15179241037948102  privacy: tensor(2.0546, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:57:33 2021 --Episode: 4   Index: 30000   Reward: 0.2077349166554952   Loss: 4.464088646413252e-06\n",
      "UEHitrate: 0.008799706676444119  edgeHitrate 0.13196226792440252 sumHitrate 0.14076197460084663  privacy: tensor(1.9794, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:00:35 2021 --Episode: 4   Index: 40000   Reward: 0.16800299251642495   Loss: 4.082252511350276e-06\n",
      "UEHitrate: 0.009099772505687357  edgeHitrate 0.1264718382040449 sumHitrate 0.13557161070973225  privacy: tensor(1.9338, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:03:38 2021 --Episode: 4   Index: 50000   Reward: 0.13414509126436913   Loss: 3.6773631034840614e-06\n",
      "UEHitrate: 0.009779804403911923  edgeHitrate 0.126337473250535 sumHitrate 0.13611727765444692  privacy: tensor(1.8876, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 02:03:42 2021 --End episode: 4   Reward: 0.1335042096897592   Loss: 3.6708382517158894e-06\n",
      "UEHitrate: 0.009813484085435038  edgeHitrate 0.12636104862949618 sumHitrate 0.1361745327149312  privacy: tensor(1.8870, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 02:03:43 2021 --Episode: 5   Index: 0   Reward: 0.0   Loss: 1.1239423656661529e-05\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:06:43 2021 --Episode: 5   Index: 10000   Reward: 0.6057007887559014   Loss: 4.627798023217335e-06\n",
      "UEHitrate: 0.006899310068993101  edgeHitrate 0.12468753124687532 sumHitrate 0.13158684131586842  privacy: tensor(2.2072, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:09:52 2021 --Episode: 5   Index: 20000   Reward: 0.31229753365461904   Loss: 4.987019815040431e-06\n",
      "UEHitrate: 0.007699615019249037  edgeHitrate 0.12524373781310935 sumHitrate 0.1329433528323584  privacy: tensor(2.0469, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:13:06 2021 --Episode: 5   Index: 30000   Reward: 0.20300753645073671   Loss: 4.402997261477562e-06\n",
      "UEHitrate: 0.00793306889770341  edgeHitrate 0.12049598346721777 sumHitrate 0.12842905236492116  privacy: tensor(1.9817, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:16:14 2021 --Episode: 5   Index: 40000   Reward: 0.16385021110192222   Loss: 3.932230937342894e-06\n",
      "UEHitrate: 0.008649783755406116  edgeHitrate 0.12397190070248244 sumHitrate 0.13262168445788855  privacy: tensor(1.9264, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:19:20 2021 --Episode: 5   Index: 50000   Reward: 0.13258047498221584   Loss: 3.6317928295885247e-06\n",
      "UEHitrate: 0.009659806803863923  edgeHitrate 0.13119737605247894 sumHitrate 0.14085718285634288  privacy: tensor(1.8823, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 02:19:25 2021 --End episode: 5   Reward: 0.1320147623913516   Loss: 3.625917901361278e-06\n",
      "UEHitrate: 0.00969405020204232  edgeHitrate 0.13133746043752612 sumHitrate 0.14103151063956845  privacy: tensor(1.8813, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 02:19:25 2021 --Episode: 6   Index: 0   Reward: 0.0   Loss: 3.0811195756541565e-05\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:22:24 2021 --Episode: 6   Index: 10000   Reward: 0.6055874291546753   Loss: 4.603349665877945e-06\n",
      "UEHitrate: 0.0071992800719928  edgeHitrate 0.12688731126887312 sumHitrate 0.13408659134086592  privacy: tensor(2.2141, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:25:23 2021 --Episode: 6   Index: 20000   Reward: 0.3136924660691251   Loss: 4.898538640591394e-06\n",
      "UEHitrate: 0.00759962001899905  edgeHitrate 0.1286435678216089 sumHitrate 0.13624318784060796  privacy: tensor(2.0463, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:28:25 2021 --Episode: 6   Index: 30000   Reward: 0.20786018800835512   Loss: 4.367169612661213e-06\n",
      "UEHitrate: 0.007866404453184895  edgeHitrate 0.12939568681043964 sumHitrate 0.13726209126362454  privacy: tensor(1.9744, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:31:40 2021 --Episode: 6   Index: 40000   Reward: 0.16904634220591747   Loss: 3.964376973759055e-06\n",
      "UEHitrate: 0.008924776880577986  edgeHitrate 0.12957176070598236 sumHitrate 0.13849653758656033  privacy: tensor(1.9306, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:34:51 2021 --Episode: 6   Index: 50000   Reward: 0.13460892647226677   Loss: 3.6158663160393377e-06\n",
      "UEHitrate: 0.009779804403911923  edgeHitrate 0.13089738205235896 sumHitrate 0.14067718645627086  privacy: tensor(1.8883, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 02:34:56 2021 --End episode: 6   Reward: 0.13422881549475152   Loss: 3.6090962689234166e-06\n",
      "UEHitrate: 0.009813484085435038  edgeHitrate 0.13113840396520493 sumHitrate 0.14095188805063996  privacy: tensor(1.8869, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 02:34:56 2021 --Episode: 7   Index: 0   Reward: 0.0   Loss: 3.0636445444542915e-05\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:38:03 2021 --Episode: 7   Index: 10000   Reward: 0.6284074844882817   Loss: 4.81574878287727e-06\n",
      "UEHitrate: 0.0072992700729927005  edgeHitrate 0.1314868513148685 sumHitrate 0.13878612138786123  privacy: tensor(2.2321, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:41:10 2021 --Episode: 7   Index: 20000   Reward: 0.32750275664482165   Loss: 5.109292339087163e-06\n",
      "UEHitrate: 0.008299585020748963  edgeHitrate 0.14124293785310735 sumHitrate 0.1495425228738563  privacy: tensor(2.0486, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:44:20 2021 --Episode: 7   Index: 30000   Reward: 0.2194392263224918   Loss: 4.545682990668125e-06\n",
      "UEHitrate: 0.010332988900369988  edgeHitrate 0.13956201459951334 sumHitrate 0.14989500349988333  privacy: tensor(1.9824, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:47:26 2021 --Episode: 7   Index: 40000   Reward: 0.1795157425741393   Loss: 4.093932806831062e-06\n",
      "UEHitrate: 0.011049723756906077  edgeHitrate 0.13687157821054474 sumHitrate 0.14792130196745082  privacy: tensor(1.9282, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:50:38 2021 --Episode: 7   Index: 50000   Reward: 0.14381270232345322   Loss: 3.733971284894405e-06\n",
      "UEHitrate: 0.011699766004679906  edgeHitrate 0.14465710685786284 sumHitrate 0.15635687286254274  privacy: tensor(1.8803, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 02:50:43 2021 --End episode: 7   Reward: 0.14312014486673574   Loss: 3.7293627970210546e-06\n",
      "UEHitrate: 0.011724426219718535  edgeHitrate 0.14469414973027847 sumHitrate 0.156418575949997  privacy: tensor(1.8797, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 02:50:43 2021 --Episode: 8   Index: 0   Reward: 0.0   Loss: 1.3506930372386705e-05\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:53:38 2021 --Episode: 8   Index: 10000   Reward: 0.5949724933945917   Loss: 4.694710686363697e-06\n",
      "UEHitrate: 0.007699230076992301  edgeHitrate 0.12808719128087193 sumHitrate 0.1357864213578642  privacy: tensor(2.2069, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:56:47 2021 --Episode: 8   Index: 20000   Reward: 0.30534397747366193   Loss: 4.9123907777002265e-06\n",
      "UEHitrate: 0.007649617519124044  edgeHitrate 0.13724313784310785 sumHitrate 0.1448927553622319  privacy: tensor(2.0402, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:00:03 2021 --Episode: 8   Index: 30000   Reward: 0.20364492467316583   Loss: 4.337658306553889e-06\n",
      "UEHitrate: 0.007899736675444151  edgeHitrate 0.13449551681610614 sumHitrate 0.14239525349155027  privacy: tensor(1.9719, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:03:04 2021 --Episode: 8   Index: 40000   Reward: 0.1666498387562948   Loss: 3.934880363824188e-06\n",
      "UEHitrate: 0.008424789380265493  edgeHitrate 0.13154671133221668 sumHitrate 0.13997150071248218  privacy: tensor(1.9284, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:06:06 2021 --Episode: 8   Index: 50000   Reward: 0.13161279058982572   Loss: 3.6092463151127783e-06\n",
      "UEHitrate: 0.009319813603727925  edgeHitrate 0.13915721685566287 sumHitrate 0.1484770304593908  privacy: tensor(1.8806, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 03:06:11 2021 --End episode: 8   Reward: 0.13127375096574737   Loss: 3.6035877959248054e-06\n",
      "UEHitrate: 0.009355654199096284  edgeHitrate 0.13906085156358858 sumHitrate 0.14841650576268486  privacy: tensor(1.8799, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 03:06:12 2021 --Episode: 9   Index: 0   Reward: 0.0   Loss: 3.119838947895914e-05\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:09:17 2021 --Episode: 9   Index: 10000   Reward: 0.6066797841305063   Loss: 4.596968358313235e-06\n",
      "UEHitrate: 0.007599240075992401  edgeHitrate 0.12428757124287572 sumHitrate 0.13188681131886812  privacy: tensor(2.1945, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:12:30 2021 --Episode: 9   Index: 20000   Reward: 0.31452319104621057   Loss: 4.935665864195084e-06\n",
      "UEHitrate: 0.009299535023248838  edgeHitrate 0.14059297035148244 sumHitrate 0.14989250537473126  privacy: tensor(2.0251, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:15:45 2021 --Episode: 9   Index: 30000   Reward: 0.2071011041898756   Loss: 4.4977605124743176e-06\n",
      "UEHitrate: 0.010599646678444052  edgeHitrate 0.13676210792973567 sumHitrate 0.14736175460817974  privacy: tensor(1.9678, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:18:57 2021 --Episode: 9   Index: 40000   Reward: 0.1666993226517462   Loss: 4.070593482922131e-06\n",
      "UEHitrate: 0.010424739381515462  edgeHitrate 0.1577460563485913 sumHitrate 0.16817079573010674  privacy: tensor(1.9208, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:21:42 2021 --Episode: 9   Index: 50000   Reward: 0.13373593502242656   Loss: 3.6948060880694137e-06\n",
      "UEHitrate: 0.010979780404391912  edgeHitrate 0.16099678006439871 sumHitrate 0.17197656046879062  privacy: tensor(1.8792, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 03:21:45 2021 --End episode: 9   Reward: 0.13343183863639826   Loss: 3.686501626262324e-06\n",
      "UEHitrate: 0.011007822919362222  edgeHitrate 0.16075800704659912 sumHitrate 0.17176582996596135  privacy: tensor(1.8781, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_episodes = 10\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  0\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = 0\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward += float(ue.reward.sum())\n",
    "        \n",
    "        if index % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if index % 10000 == 0:\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"Time:\",time.asctime( time.localtime(time.time())),\"--Episode:\",i_episode,\"  Index:\",index,\"  Reward:\",sumReward/(index+1),\"  Loss:\",float(loss/(index+1)))\n",
    "            print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1),\" privacy:\",psi/len(UEs))\n",
    "            \n",
    "            print()\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        \n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Time:\",time.asctime( time.localtime(time.time())),\"--End episode:\",i_episode,\"  Reward:\",sumReward/(index+1),\"  Loss:\",loss/(index+1))\n",
    "    print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1),\" privacy:\",psi/len(UEs))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "\n",
    "    if sumReward > bestReward:\n",
    "        bestReward = sumReward\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+'ep{}_'.format(bestEpisode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "    \n",
    "\n",
    "\n",
    "    env = ENV(userNum,contentNum)\n",
    "    UEs = {}\n",
    "    sumReward = 0\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: Tue Sep 14 03:21:45 2021 --Episode: 9   Index: 0  Reward: tensor(0., dtype=torch.float64)\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0\n",
      "\n",
      "Time: Tue Sep 14 03:22:09 2021 --Episode: 9   Index: 10000  Reward: tensor(0.2794, dtype=torch.float64)\n",
      "UEHitrate: 0.024997500249975  edgeHitrate 0.38806119388061194 sumHitrate 0.41305869413058693\n",
      "\n",
      "Time: Tue Sep 14 03:22:33 2021 --Episode: 9   Index: 20000  Reward: tensor(0.1680, dtype=torch.float64)\n",
      "UEHitrate: 0.030698465076746163  edgeHitrate 0.39473026348682566 sumHitrate 0.42542872856357183\n",
      "\n",
      "Time: Tue Sep 14 03:22:57 2021 --Episode: 9   Index: 30000  Reward: tensor(0.1277, dtype=torch.float64)\n",
      "UEHitrate: 0.031932268924369185  edgeHitrate 0.3646878437385421 sumHitrate 0.39662011266291125\n",
      "\n",
      "Time: Tue Sep 14 03:23:20 2021 --Episode: 9   Index: 40000  Reward: tensor(0.1194, dtype=torch.float64)\n",
      "UEHitrate: 0.03202419939501513  edgeHitrate 0.33611659708507285 sumHitrate 0.368140796480088\n",
      "\n",
      "Time: Tue Sep 14 03:23:43 2021 --Episode: 9   Index: 50000  Reward: tensor(0.1050, dtype=torch.float64)\n",
      "UEHitrate: 0.031799364012719745  edgeHitrate 0.31289374212515747 sumHitrate 0.34469310613787724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = 0\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for index,trace in trainUIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    sumReward += ue.reward.sum()\n",
    "    \n",
    "    if index % 10000 == 0:\n",
    "        print(\"Time:\",time.asctime( time.localtime(time.time())),\"--Episode:\",i_episode,\"  Index:\",index,\" Reward:\",sumReward/(index+1),)\n",
    "        print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tensor(176.8964, dtype=torch.float64)\n",
      "0 tensor(178.8415, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0382, dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    if u % 100 == 0:\n",
    "        print(u,psi)\n",
    "\n",
    "psi/contentNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
