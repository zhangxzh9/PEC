{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>1030</td>\n",
       "      <td>101001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11807</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15068</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>1035</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5375</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1030</td>\n",
       "      <td>10202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5992</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3468</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148        1030        101001     0   \n",
       "1       203  5779    0        0         7        1030         10203     0   \n",
       "2       208  4675    0        0        92        1035         10203     0   \n",
       "3       159   332    0        0        56        1030         10202     0   \n",
       "4        50   674    0        0       439        1030         10203     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831   29  2591880        34        1030         10203     0   \n",
       "300979  158  8448   29  2591880        34        1030         10203     0   \n",
       "300980  483  6463   29  2591940        35        1030         10203     0   \n",
       "300981  158  4715   29  2591940        34        1030         10203     0   \n",
       "300982  483  2021   29  2591940        34        1030         10203     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0      11807          1            2  \n",
       "1              0      15068          1            2  \n",
       "2              0       5375          1            2  \n",
       "3              0       5992          1            2  \n",
       "4              0       3468          1            2  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0      10010          1            2  \n",
       "300979         0      23340          1            2  \n",
       "300980         0      10010          1            2  \n",
       "300981         0      23340          1            2  \n",
       "300982         0      10010          1            2  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " 500,\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148        1030        101001     0   \n",
       " 1       203  5779    0        0         7        1030         10203     0   \n",
       " 2       208  4675    0        0        92        1035         10203     0   \n",
       " 3       159   332    0        0        56        1030         10202     0   \n",
       " 4        50   674    0        0       439        1030         10203     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 198170  264  7442   17  1555140        90        1035         10203     0   \n",
       " 198171   19  9362   17  1555140       424        1035         10203     0   \n",
       " 198172   82  9223   17  1555140        94        1037         10203     0   \n",
       " 198173   35  4164   17  1555140        22        1030         10203     0   \n",
       " 198174  239  5062   17  1555140        89        1035         10203     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0      11807          1            2  \n",
       " 1              0      15068          1            2  \n",
       " 2              0       5375          1            2  \n",
       " 3              0       5992          1            2  \n",
       " 4              0       3468          1            2  \n",
       " ...          ...        ...        ...          ...  \n",
       " 198170         0       7592          1            2  \n",
       " 198171         0       5938          1            2  \n",
       " 198172         0      11393          1            2  \n",
       " 198173         0       5866          1            2  \n",
       " 198174         0      23746          1            2  \n",
       " \n",
       " [198175 rows x 12 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=15,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() - torch.log(ru * p + (1-ru) * (1-p)).sum())\n",
    "        self.Rh = - self.ALPHAh *( torch.log(lastru * lastp + (1-lastru) * (1-lastp)).sum() - torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl *  ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "\n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "        \n",
    "        if train: \n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                    lastAction.to(device), \n",
    "                    self.statusFeature,\n",
    "                    torch.tensor([self.reward.float()]).to(device),\n",
    "                    torch.tensor([self.W[-1]]).to(device))\n",
    "        \n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "        \n",
    "\n",
    "        if  not train or (train and sample > eps_threshold):\n",
    "            QNetwork.eval()\n",
    "            with torch.no_grad():\n",
    "                Q_value = QNetwork(self.statusFeature)\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "            QNetwork.train()\n",
    "        else:\n",
    "            actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort()[0:10])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELPATH =  './model_dict/dnn_h1_reward2_'\n",
    "rewardPara = {\"alpha\":1,\"betao\":10,\"betal\":10}\n",
    "latency = [0.1,1,0.9]\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Mon Sep 20 14:51:38 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 14:55:22 2021 Episode: 0   Index: 20000   Loss: 0.53831 --\n",
      "Reward: [4.11621 0.14324 1.27074] total reward: 5.53019\n",
      "UEHitrate: 0.00675  edgeHitrate 0.14224 sumHitrate 0.14899  privacy: 3.38809\n",
      "\n",
      "--Time: Mon Sep 20 14:58:57 2021 Episode: 0   Index: 40000   Loss: 0.57557 --\n",
      "Reward: [3.14414 0.17617 1.26649] total reward: 4.5868\n",
      "UEHitrate: 0.00955  edgeHitrate 0.14207 sumHitrate 0.15162  privacy: 2.84787\n",
      "\n",
      "--Time: Mon Sep 20 15:02:30 2021 Episode: 0   Index: 60000   Loss: 0.57656 --\n",
      "Reward: [2.59006 0.19738 1.33288] total reward: 4.12032\n",
      "UEHitrate: 0.0106  edgeHitrate 0.15 sumHitrate 0.1606  privacy: 2.50023\n",
      "\n",
      "--Time: Mon Sep 20 15:05:45 2021 Episode: 0   Index: 80000   Loss: 0.57178 --\n",
      "Reward: [2.2299  0.19971 1.37361] total reward: 3.80322\n",
      "UEHitrate: 0.01157  edgeHitrate 0.15425 sumHitrate 0.16582  privacy: 2.26075\n",
      "\n",
      "--Time: Mon Sep 20 15:08:53 2021 Episode: 0   Index: 100000   Loss: 0.56804 --\n",
      "Reward: [1.89911 0.20941 1.41911] total reward: 3.52762\n",
      "UEHitrate: 0.01246  edgeHitrate 0.15929 sumHitrate 0.17175  privacy: 2.04416\n",
      "\n",
      "--Time: Mon Sep 20 15:12:11 2021 Episode: 0   Index: 120000   Loss: 0.56594 --\n",
      "Reward: [1.58776 0.22006 1.47104] total reward: 3.27885\n",
      "UEHitrate: 0.013  edgeHitrate 0.16509 sumHitrate 0.17809  privacy: 1.87466\n",
      "\n",
      "--Time: Mon Sep 20 15:15:11 2021 Episode: 0   Index: 140000   Loss: 0.56368 --\n",
      "Reward: [1.34257 0.23293 1.427  ] total reward: 3.00251\n",
      "UEHitrate: 0.01361  edgeHitrate 0.16053 sumHitrate 0.17414  privacy: 1.7321\n",
      "\n",
      "--Time: Mon Sep 20 15:18:30 2021 Episode: 0   Index: 160000   Loss: 0.55896 --\n",
      "Reward: [1.18144 0.24013 1.38239] total reward: 2.80396\n",
      "UEHitrate: 0.01402  edgeHitrate 0.15547 sumHitrate 0.16949  privacy: 1.61066\n",
      "\n",
      "--Time: Mon Sep 20 15:21:52 2021 Episode: 0   Index: 180000   Loss: 0.55424 --\n",
      "Reward: [1.02846 0.24633 1.36649] total reward: 2.64128\n",
      "UEHitrate: 0.01435  edgeHitrate 0.15369 sumHitrate 0.16804  privacy: 1.49928\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 15:25:02 2021 Episode: 0   Index: 198174   Loss: 0.55059 --\n",
      "Reward: [0.93986 0.25165 1.35176] total reward: 2.54327\n",
      "UEHitrate: 0.01489  edgeHitrate 0.15208 sumHitrate 0.16697  privacy: 1.41047\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward2_ep0_0920-15-25-02\n",
      "\n",
      "--Time: Mon Sep 20 15:25:02 2021 Episode: 1   Index: 0   Loss: 17.10127 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 15:28:27 2021 Episode: 1   Index: 20000   Loss: 0.46183 --\n",
      "Reward: [3.53061 0.21349 1.35578] total reward: 5.09988\n",
      "UEHitrate: 0.01195  edgeHitrate 0.15164 sumHitrate 0.16359  privacy: 3.36552\n",
      "\n",
      "--Time: Mon Sep 20 15:32:03 2021 Episode: 1   Index: 40000   Loss: 0.48901 --\n",
      "Reward: [2.68224 0.26824 1.28314] total reward: 4.23362\n",
      "UEHitrate: 0.01602  edgeHitrate 0.14417 sumHitrate 0.1602  privacy: 2.89268\n",
      "\n",
      "--Time: Mon Sep 20 15:35:18 2021 Episode: 1   Index: 60000   Loss: 0.49114 --\n",
      "Reward: [2.20745 0.29695 1.27948] total reward: 3.78388\n",
      "UEHitrate: 0.01847  edgeHitrate 0.14381 sumHitrate 0.16228  privacy: 2.57815\n",
      "\n",
      "--Time: Mon Sep 20 15:38:32 2021 Episode: 1   Index: 80000   Loss: 0.4919 --\n",
      "Reward: [1.89575 0.31051 1.26246] total reward: 3.46872\n",
      "UEHitrate: 0.01982  edgeHitrate 0.14215 sumHitrate 0.16197  privacy: 2.36162\n",
      "\n",
      "--Time: Mon Sep 20 15:42:23 2021 Episode: 1   Index: 100000   Loss: 0.49495 --\n",
      "Reward: [1.61182 0.31896 1.25738] total reward: 3.18816\n",
      "UEHitrate: 0.02065  edgeHitrate 0.14159 sumHitrate 0.16224  privacy: 2.15824\n",
      "\n",
      "--Time: Mon Sep 20 15:45:40 2021 Episode: 1   Index: 120000   Loss: 0.49793 --\n",
      "Reward: [1.34499 0.32743 1.25369] total reward: 2.92611\n",
      "UEHitrate: 0.02127  edgeHitrate 0.14116 sumHitrate 0.16243  privacy: 1.99972\n",
      "\n",
      "--Time: Mon Sep 20 15:49:11 2021 Episode: 1   Index: 140000   Loss: 0.50022 --\n",
      "Reward: [1.13559 0.33639 1.24983] total reward: 2.72181\n",
      "UEHitrate: 0.022  edgeHitrate 0.14071 sumHitrate 0.16271  privacy: 1.86575\n",
      "\n",
      "--Time: Mon Sep 20 15:52:42 2021 Episode: 1   Index: 160000   Loss: 0.49965 --\n",
      "Reward: [0.99741 0.34202 1.2421 ] total reward: 2.58153\n",
      "UEHitrate: 0.02242  edgeHitrate 0.13983 sumHitrate 0.16226  privacy: 1.74992\n",
      "\n",
      "--Time: Mon Sep 20 15:56:08 2021 Episode: 1   Index: 180000   Loss: 0.4987 --\n",
      "Reward: [0.86801 0.34715 1.25379] total reward: 2.46895\n",
      "UEHitrate: 0.02286  edgeHitrate 0.14107 sumHitrate 0.16392  privacy: 1.64169\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 15:59:29 2021 Episode: 1   Index: 198174   Loss: 0.49669 --\n",
      "Reward: [0.79197 0.35104 1.25271] total reward: 2.39572\n",
      "UEHitrate: 0.02334  edgeHitrate 0.14087 sumHitrate 0.1642  privacy: 1.55513\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 15:59:30 2021 Episode: 2   Index: 0   Loss: 17.18024 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 16:03:29 2021 Episode: 2   Index: 20000   Loss: 0.48519 --\n",
      "Reward: [2.95114 0.25594 1.47413] total reward: 4.68121\n",
      "UEHitrate: 0.0153  edgeHitrate 0.16639 sumHitrate 0.18169  privacy: 3.31271\n",
      "\n",
      "--Time: Mon Sep 20 16:07:33 2021 Episode: 2   Index: 40000   Loss: 0.51029 --\n",
      "Reward: [2.24598 0.30889 1.30947] total reward: 3.86434\n",
      "UEHitrate: 0.01957  edgeHitrate 0.14842 sumHitrate 0.168  privacy: 2.91449\n",
      "\n",
      "--Time: Mon Sep 20 16:11:31 2021 Episode: 2   Index: 60000   Loss: 0.51112 --\n",
      "Reward: [1.85894 0.34373 1.26388] total reward: 3.46655\n",
      "UEHitrate: 0.0217  edgeHitrate 0.14343 sumHitrate 0.16513  privacy: 2.6343\n",
      "\n",
      "--Time: Mon Sep 20 16:15:52 2021 Episode: 2   Index: 80000   Loss: 0.50861 --\n",
      "Reward: [1.60552 0.35661 1.23568] total reward: 3.19781\n",
      "UEHitrate: 0.02317  edgeHitrate 0.14027 sumHitrate 0.16345  privacy: 2.44062\n",
      "\n",
      "--Time: Mon Sep 20 16:19:58 2021 Episode: 2   Index: 100000   Loss: 0.50489 --\n",
      "Reward: [1.36542 0.36557 1.22696] total reward: 2.95794\n",
      "UEHitrate: 0.02391  edgeHitrate 0.13934 sumHitrate 0.16325  privacy: 2.25208\n",
      "\n",
      "--Time: Mon Sep 20 16:24:02 2021 Episode: 2   Index: 120000   Loss: 0.50198 --\n",
      "Reward: [1.13723 0.37349 1.22286] total reward: 2.73359\n",
      "UEHitrate: 0.02498  edgeHitrate 0.1388 sumHitrate 0.16378  privacy: 2.10749\n",
      "\n",
      "--Time: Mon Sep 20 16:28:27 2021 Episode: 2   Index: 140000   Loss: 0.49782 --\n",
      "Reward: [0.95742 0.38695 1.22978] total reward: 2.57415\n",
      "UEHitrate: 0.02598  edgeHitrate 0.13964 sumHitrate 0.16562  privacy: 1.98262\n",
      "\n",
      "--Time: Mon Sep 20 16:32:45 2021 Episode: 2   Index: 160000   Loss: 0.48937 --\n",
      "Reward: [0.83974 0.39314 1.23175] total reward: 2.46463\n",
      "UEHitrate: 0.02626  edgeHitrate 0.13983 sumHitrate 0.16609  privacy: 1.8743\n",
      "\n",
      "--Time: Mon Sep 20 16:36:55 2021 Episode: 2   Index: 180000   Loss: 0.47972 --\n",
      "Reward: [0.73166 0.38989 1.38394] total reward: 2.50549\n",
      "UEHitrate: 0.02597  edgeHitrate 0.15664 sumHitrate 0.18261  privacy: 1.77242\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 16:40:31 2021 Episode: 2   Index: 198174   Loss: 0.47005 --\n",
      "Reward: [0.66673 0.39225 1.5207 ] total reward: 2.57968\n",
      "UEHitrate: 0.02636  edgeHitrate 0.17185 sumHitrate 0.19821  privacy: 1.68915\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward2_ep2_0920-16-40-31\n",
      "\n",
      "--Time: Mon Sep 20 16:40:31 2021 Episode: 3   Index: 0   Loss: 11.73302 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 16:44:24 2021 Episode: 3   Index: 20000   Loss: 0.38035 --\n",
      "Reward: [2.60703 0.29429 2.05775] total reward: 4.95907\n",
      "UEHitrate: 0.01705  edgeHitrate 0.23079 sumHitrate 0.24784  privacy: 3.24085\n",
      "\n",
      "--Time: Mon Sep 20 16:48:11 2021 Episode: 3   Index: 40000   Loss: 0.41005 --\n",
      "Reward: [1.94967 0.35159 1.82088] total reward: 4.12215\n",
      "UEHitrate: 0.02132  edgeHitrate 0.20562 sumHitrate 0.22694  privacy: 2.90798\n",
      "\n",
      "--Time: Mon Sep 20 16:51:45 2021 Episode: 3   Index: 60000   Loss: 0.41936 --\n",
      "Reward: [1.6101  0.36391 1.66917] total reward: 3.64318\n",
      "UEHitrate: 0.02162  edgeHitrate 0.18846 sumHitrate 0.21008  privacy: 2.66379\n",
      "\n",
      "--Time: Mon Sep 20 16:55:39 2021 Episode: 3   Index: 80000   Loss: 0.42113 --\n",
      "Reward: [1.3875  0.36106 1.53279] total reward: 3.28136\n",
      "UEHitrate: 0.02135  edgeHitrate 0.17315 sumHitrate 0.1945  privacy: 2.49408\n",
      "\n",
      "--Time: Mon Sep 20 16:59:46 2021 Episode: 3   Index: 100000   Loss: 0.42141 --\n",
      "Reward: [1.17883 0.36363 1.44494] total reward: 2.98739\n",
      "UEHitrate: 0.0211  edgeHitrate 0.16335 sumHitrate 0.18445  privacy: 2.32268\n",
      "\n",
      "--Time: Mon Sep 20 17:03:57 2021 Episode: 3   Index: 120000   Loss: 0.42117 --\n",
      "Reward: [0.98171 0.36501 1.47974] total reward: 2.82645\n",
      "UEHitrate: 0.02153  edgeHitrate 0.16721 sumHitrate 0.18874  privacy: 2.19191\n",
      "\n",
      "--Time: Mon Sep 20 17:08:12 2021 Episode: 3   Index: 140000   Loss: 0.42129 --\n",
      "Reward: [0.82188 0.37803 1.61118] total reward: 2.81109\n",
      "UEHitrate: 0.02251  edgeHitrate 0.18213 sumHitrate 0.20463  privacy: 2.07912\n",
      "\n",
      "--Time: Mon Sep 20 17:12:44 2021 Episode: 3   Index: 160000   Loss: 0.41881 --\n",
      "Reward: [0.71911 0.38545 1.73114] total reward: 2.8357\n",
      "UEHitrate: 0.02315  edgeHitrate 0.19543 sumHitrate 0.21858  privacy: 1.98031\n",
      "\n",
      "--Time: Mon Sep 20 17:17:14 2021 Episode: 3   Index: 180000   Loss: 0.41641 --\n",
      "Reward: [0.6247  0.38559 1.89419] total reward: 2.90449\n",
      "UEHitrate: 0.02346  edgeHitrate 0.21356 sumHitrate 0.23702  privacy: 1.88359\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 17:21:16 2021 Episode: 3   Index: 198174   Loss: 0.41409 --\n",
      "Reward: [0.56982 0.39277 2.02853] total reward: 2.99112\n",
      "UEHitrate: 0.02451  edgeHitrate 0.22844 sumHitrate 0.25295  privacy: 1.80659\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward2_ep3_0920-17-21-16\n",
      "\n",
      "--Time: Mon Sep 20 17:21:16 2021 Episode: 4   Index: 0   Loss: 11.17369 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 17:25:35 2021 Episode: 4   Index: 20000   Loss: 0.39515 --\n",
      "Reward: [2.22695 0.31188 2.24494] total reward: 4.78377\n",
      "UEHitrate: 0.01895  edgeHitrate 0.25159 sumHitrate 0.27054  privacy: 3.1612\n",
      "\n",
      "--Time: Mon Sep 20 17:29:42 2021 Episode: 4   Index: 40000   Loss: 0.42564 --\n",
      "Reward: [1.66772 0.39494 2.0047 ] total reward: 4.06736\n",
      "UEHitrate: 0.02457  edgeHitrate 0.22522 sumHitrate 0.24979  privacy: 2.88834\n",
      "\n",
      "--Time: Mon Sep 20 17:33:44 2021 Episode: 4   Index: 60000   Loss: 0.44076 --\n",
      "Reward: [1.37398 0.39778 1.83207] total reward: 3.60383\n",
      "UEHitrate: 0.02457  edgeHitrate 0.20623 sumHitrate 0.2308  privacy: 2.67766\n",
      "\n",
      "--Time: Mon Sep 20 17:37:47 2021 Episode: 4   Index: 80000   Loss: 0.44744 --\n",
      "Reward: [1.18566 0.39498 1.67848] total reward: 3.25912\n",
      "UEHitrate: 0.02427  edgeHitrate 0.18952 sumHitrate 0.2138  privacy: 2.5319\n",
      "\n",
      "--Time: Mon Sep 20 17:41:57 2021 Episode: 4   Index: 100000   Loss: 0.4501 --\n",
      "Reward: [1.00485 0.39077 1.57408] total reward: 2.9697\n",
      "UEHitrate: 0.02354  edgeHitrate 0.17804 sumHitrate 0.20158  privacy: 2.37855\n",
      "\n",
      "--Time: Mon Sep 20 17:46:18 2021 Episode: 4   Index: 120000   Loss: 0.45031 --\n",
      "Reward: [0.83423 0.39216 1.57716] total reward: 2.80355\n",
      "UEHitrate: 0.02375  edgeHitrate 0.17855 sumHitrate 0.2023  privacy: 2.26288\n",
      "\n",
      "--Time: Mon Sep 20 17:50:43 2021 Episode: 4   Index: 140000   Loss: 0.45057 --\n",
      "Reward: [0.69917 0.39847 1.65824] total reward: 2.75587\n",
      "UEHitrate: 0.02414  edgeHitrate 0.18772 sumHitrate 0.21186  privacy: 2.16089\n",
      "\n",
      "--Time: Mon Sep 20 17:54:36 2021 Episode: 4   Index: 160000   Loss: 0.45049 --\n",
      "Reward: [0.61234 0.40342 1.76967] total reward: 2.78543\n",
      "UEHitrate: 0.02466  edgeHitrate 0.19992 sumHitrate 0.22457  privacy: 2.07065\n",
      "\n",
      "--Time: Mon Sep 20 17:58:56 2021 Episode: 4   Index: 180000   Loss: 0.44935 --\n",
      "Reward: [0.5333  0.40598 1.93989] total reward: 2.87917\n",
      "UEHitrate: 0.02497  edgeHitrate 0.21879 sumHitrate 0.24376  privacy: 1.98268\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 18:02:36 2021 Episode: 4   Index: 198174   Loss: 0.44747 --\n",
      "Reward: [0.48436 0.41532 2.07203] total reward: 2.97171\n",
      "UEHitrate: 0.02601  edgeHitrate 0.23334 sumHitrate 0.25936  privacy: 1.91177\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 18:02:36 2021 Episode: 5   Index: 0   Loss: 12.70999 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 18:06:31 2021 Episode: 5   Index: 20000   Loss: 0.40292 --\n",
      "Reward: [2.19867 0.33118 2.45418] total reward: 4.98403\n",
      "UEHitrate: 0.0213  edgeHitrate 0.27454 sumHitrate 0.29584  privacy: 3.14745\n",
      "\n",
      "--Time: Mon Sep 20 18:10:05 2021 Episode: 5   Index: 40000   Loss: 0.42373 --\n",
      "Reward: [1.57613 0.43764 2.1946 ] total reward: 4.20837\n",
      "UEHitrate: 0.02777  edgeHitrate 0.24702 sumHitrate 0.27479  privacy: 2.87773\n",
      "\n",
      "--Time: Mon Sep 20 18:13:35 2021 Episode: 5   Index: 60000   Loss: 0.43371 --\n",
      "Reward: [1.28523 0.45083 2.04417] total reward: 3.78023\n",
      "UEHitrate: 0.02858  edgeHitrate 0.23051 sumHitrate 0.2591  privacy: 2.68609\n",
      "\n",
      "--Time: Mon Sep 20 18:17:37 2021 Episode: 5   Index: 80000   Loss: 0.44005 --\n",
      "Reward: [1.1028  0.44898 2.00619] total reward: 3.55797\n",
      "UEHitrate: 0.02907  edgeHitrate 0.22618 sumHitrate 0.25526  privacy: 2.55358\n",
      "\n",
      "--Time: Mon Sep 20 18:21:24 2021 Episode: 5   Index: 100000   Loss: 0.44497 --\n",
      "Reward: [0.93003 0.45495 2.07367] total reward: 3.45865\n",
      "UEHitrate: 0.02976  edgeHitrate 0.23338 sumHitrate 0.26314  privacy: 2.41322\n",
      "\n",
      "--Time: Mon Sep 20 18:25:26 2021 Episode: 5   Index: 120000   Loss: 0.44485 --\n",
      "Reward: [0.77463 0.46438 2.20813] total reward: 3.44714\n",
      "UEHitrate: 0.03039  edgeHitrate 0.24869 sumHitrate 0.27908  privacy: 2.31011\n",
      "\n",
      "--Time: Mon Sep 20 18:29:25 2021 Episode: 5   Index: 140000   Loss: 0.44434 --\n",
      "Reward: [0.64838 0.48217 2.36968] total reward: 3.50023\n",
      "UEHitrate: 0.03156  edgeHitrate 0.26676 sumHitrate 0.29832  privacy: 2.21815\n",
      "\n",
      "--Time: Mon Sep 20 18:32:50 2021 Episode: 5   Index: 160000   Loss: 0.44292 --\n",
      "Reward: [0.56621 0.48964 2.50401] total reward: 3.55986\n",
      "UEHitrate: 0.03225  edgeHitrate 0.28158 sumHitrate 0.31383  privacy: 2.13789\n",
      "\n",
      "--Time: Mon Sep 20 18:35:50 2021 Episode: 5   Index: 180000   Loss: 0.44164 --\n",
      "Reward: [0.49075 0.48791 2.64424] total reward: 3.62289\n",
      "UEHitrate: 0.03241  edgeHitrate 0.297 sumHitrate 0.32941  privacy: 2.05603\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 18:38:42 2021 Episode: 5   Index: 198174   Loss: 0.43969 --\n",
      "Reward: [0.44714 0.49302 2.73935] total reward: 3.67952\n",
      "UEHitrate: 0.03308  edgeHitrate 0.30771 sumHitrate 0.34078  privacy: 1.99064\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward2_ep5_0920-18-38-42\n",
      "\n",
      "--Time: Mon Sep 20 18:38:43 2021 Episode: 6   Index: 0   Loss: 17.42857 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 18:41:45 2021 Episode: 6   Index: 20000   Loss: 0.38282 --\n",
      "Reward: [2.25768 0.37723 2.57747] total reward: 5.21238\n",
      "UEHitrate: 0.0234  edgeHitrate 0.28899 sumHitrate 0.31238  privacy: 3.23662\n",
      "\n",
      "--Time: Mon Sep 20 18:44:49 2021 Episode: 6   Index: 40000   Loss: 0.39175 --\n",
      "Reward: [1.57967 0.50539 2.1541 ] total reward: 4.23915\n",
      "UEHitrate: 0.03462  edgeHitrate 0.24254 sumHitrate 0.27717  privacy: 2.89652\n",
      "\n",
      "--Time: Mon Sep 20 18:47:56 2021 Episode: 6   Index: 60000   Loss: 0.39619 --\n",
      "Reward: [1.25228 0.55822 1.99377] total reward: 3.80427\n",
      "UEHitrate: 0.04055  edgeHitrate 0.22415 sumHitrate 0.2647  privacy: 2.6922\n",
      "\n",
      "--Time: Mon Sep 20 18:51:10 2021 Episode: 6   Index: 80000   Loss: 0.39882 --\n",
      "Reward: [1.05487 0.57144 2.00394] total reward: 3.63025\n",
      "UEHitrate: 0.04249  edgeHitrate 0.22536 sumHitrate 0.26785  privacy: 2.5655\n",
      "\n",
      "--Time: Mon Sep 20 18:54:28 2021 Episode: 6   Index: 100000   Loss: 0.40126 --\n",
      "Reward: [0.88994 0.58883 2.22226] total reward: 3.70104\n",
      "UEHitrate: 0.04398  edgeHitrate 0.24988 sumHitrate 0.29386  privacy: 2.43225\n",
      "\n",
      "--Time: Mon Sep 20 18:57:40 2021 Episode: 6   Index: 120000   Loss: 0.39534 --\n",
      "Reward: [0.73931 0.58141 2.4101 ] total reward: 3.73083\n",
      "UEHitrate: 0.04327  edgeHitrate 0.27115 sumHitrate 0.31442  privacy: 2.33659\n",
      "\n",
      "--Time: Mon Sep 20 19:01:26 2021 Episode: 6   Index: 140000   Loss: 0.3911 --\n",
      "Reward: [0.63913 0.56269 2.46598] total reward: 3.6678\n",
      "UEHitrate: 0.04126  edgeHitrate 0.27719 sumHitrate 0.31845  privacy: 2.21602\n",
      "\n",
      "--Time: Mon Sep 20 19:04:43 2021 Episode: 6   Index: 160000   Loss: 0.38617 --\n",
      "Reward: [0.55886 0.52079 2.48832] total reward: 3.56797\n",
      "UEHitrate: 0.03783  edgeHitrate 0.27917 sumHitrate 0.317  privacy: 2.12972\n",
      "\n",
      "--Time: Mon Sep 20 19:08:13 2021 Episode: 6   Index: 180000   Loss: 0.37947 --\n",
      "Reward: [0.48613 0.50606 2.59859] total reward: 3.59078\n",
      "UEHitrate: 0.0367  edgeHitrate 0.29126 sumHitrate 0.32796  privacy: 2.04915\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 19:11:23 2021 Episode: 6   Index: 198174   Loss: 0.37418 --\n",
      "Reward: [0.44341 0.49711 2.67559] total reward: 3.61611\n",
      "UEHitrate: 0.03637  edgeHitrate 0.29959 sumHitrate 0.33596  privacy: 1.98903\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 19:11:23 2021 Episode: 7   Index: 0   Loss: 9.82569 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 19:14:57 2021 Episode: 7   Index: 20000   Loss: 0.25093 --\n",
      "Reward: [1.60629 0.07945 0.68442] total reward: 2.37015\n",
      "UEHitrate: 0.00205  edgeHitrate 0.07645 sumHitrate 0.0785  privacy: 2.8395\n",
      "\n",
      "--Time: Mon Sep 20 19:18:23 2021 Episode: 7   Index: 40000   Loss: 0.24603 --\n",
      "Reward: [1.15148 0.09405 0.79198] total reward: 2.03751\n",
      "UEHitrate: 0.0034  edgeHitrate 0.08837 sumHitrate 0.09177  privacy: 2.71661\n",
      "\n",
      "--Time: Mon Sep 20 19:21:19 2021 Episode: 7   Index: 60000   Loss: 0.23831 --\n",
      "Reward: [0.94453 0.11836 1.06228] total reward: 2.12518\n",
      "UEHitrate: 0.00498  edgeHitrate 0.11881 sumHitrate 0.1238  privacy: 2.6023\n",
      "\n",
      "--Time: Mon Sep 20 19:24:04 2021 Episode: 7   Index: 80000   Loss: 0.23987 --\n",
      "Reward: [0.81551 0.12696 1.29475] total reward: 2.23722\n",
      "UEHitrate: 0.00599  edgeHitrate 0.14484 sumHitrate 0.15082  privacy: 2.52302\n",
      "\n",
      "--Time: Mon Sep 20 19:26:46 2021 Episode: 7   Index: 100000   Loss: 0.24232 --\n",
      "Reward: [0.68307 0.13075 1.38572] total reward: 2.19954\n",
      "UEHitrate: 0.00634  edgeHitrate 0.15499 sumHitrate 0.16133  privacy: 2.42217\n",
      "\n",
      "--Time: Mon Sep 20 19:29:28 2021 Episode: 7   Index: 120000   Loss: 0.24511 --\n",
      "Reward: [0.56716 0.13478 1.45956] total reward: 2.16151\n",
      "UEHitrate: 0.00647  edgeHitrate 0.16316 sumHitrate 0.16963  privacy: 2.35141\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f70ca416ccf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m25\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Perform one step of the optimization (on the policy network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# Update the target network, copying all weights and biases in DQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-dc4e7f9b8f99>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_state_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mnext_state_values\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mgetNextStatusQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontent_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Compute the expected Q values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-dc4e7f9b8f99>\u001b[0m in \u001b[0;36mgetNextStatusQ\u001b[0;34m(s_batch, c_batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mQ_value_sortindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mQ_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQ_value_sortindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0mQ_value_sortindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "TARGET_UPDATE = 1\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = trainUIT.shape[0]*5\n",
    "agentStep = 0\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "        agentStep += 1\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 20000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+'ep{}_'.format(bestEpisode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "        print(bestPath)\n",
    "    print()\n",
    "    env = ENV(userNum,contentNum,latency)\n",
    "    UEs = {}\n",
    "    sumReward = sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "TARGET_UPDATE = 1\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = trainUIT.shape[0]*5\n",
    "agentStep = 0\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "        agentStep += 1\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        if index % 64 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 10000 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 20000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+'ep{}_'.format(bestEpisode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "        print(bestPath)\n",
    "    print()\n",
    "    env = ENV(userNum,contentNum,latency)\n",
    "    UEs = {}\n",
    "    sumReward = sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
