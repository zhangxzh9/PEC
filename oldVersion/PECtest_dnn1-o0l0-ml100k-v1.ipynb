{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#v1 修改reward 直接用变化量作为reward\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    " \n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + torch.exp( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "        self.Rh = self.ALPHAh * ( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "        if self.W[-1] not in np.argwhere(self.lastAction.numpy()): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            if  not train or (train and sample > eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                with torch.no_grad():\n",
    "                    Q_value = QNetwork(self.statusFeature)\n",
    "                    #actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "                    actionIndex = list(Q_value.squeeze(1).argsort(descending=True)[0:self.Bu])\n",
    "                QNetwork.train()\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "\n",
    "            self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "            self.action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                self.action[index] = 1\n",
    "            env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "        else:\n",
    "            self.action = self.lastAction # keep the cache and no request the new video\n",
    "\n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "\n",
    "        if train:\n",
    "            #lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                        lastAction.to(device), \n",
    "                        self.statusFeature,\n",
    "                        torch.tensor([self.reward.float()]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "                        \n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    #state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    #state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    #next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    #expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "data_path = '/home/ubuntu/data/dataset/ml-100k/'\n",
    "UIT = pd.read_csv(data_path + 'ml_header.trace')\n",
    "\n",
    "trainUIT = UIT[UIT['day']<18]\n",
    "validUIT = UIT[UIT['day']<24]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT.shape,validUIT.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1682, 943, (65336, 6), (81114, 6))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_o0l0_v1_'\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0,\"betal\":0}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 15\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 1).to(device)\n",
    "target_net = DQN(5, 1).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 20:19:52 2021 Episode: 0   Index: 65335   Loss: 0.09681 --\n",
      "Reward: [-2.45507  0.       0.     ] total reward: -2.45507\n",
      "UEHitrate: 0.01445  edgeHitrate 0.37232 sumHitrate 0.38677  privacy: 1.73047\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 20:22:32 2021 Episode: 0   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.36718  0.       0.     ] total reward: -0.36718\n",
      "UEHitrate: 0.05325  edgeHitrate 0.59455 sumHitrate 0.64779  privacy: 0.97249\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_exp_ep0_1010-20-22-32\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 20:24:22 2021 Episode: 1   Index: 65335   Loss: 0.08296 --\n",
      "Reward: [-2.00837  0.       0.     ] total reward: -2.00837\n",
      "UEHitrate: 0.01555  edgeHitrate 0.3357 sumHitrate 0.35125  privacy: 1.93199\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 20:27:05 2021 Episode: 1   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.36651  0.       0.     ] total reward: -0.36651\n",
      "UEHitrate: 0.02103  edgeHitrate 0.61682 sumHitrate 0.63786  privacy: 1.01403\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_exp_ep1_1010-20-27-05\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 20:29:14 2021 Episode: 2   Index: 65335   Loss: 0.0635 --\n",
      "Reward: [-1.67575  0.       0.     ] total reward: -1.67575\n",
      "UEHitrate: 0.01861  edgeHitrate 0.34206 sumHitrate 0.36067  privacy: 2.03201\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 20:32:04 2021 Episode: 2   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.40067  0.       0.     ] total reward: -0.40067\n",
      "UEHitrate: 0.02028  edgeHitrate 0.62622 sumHitrate 0.6465  privacy: 1.03347\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 20:34:20 2021 Episode: 3   Index: 65335   Loss: 0.04787 --\n",
      "Reward: [-1.37145  0.       0.     ] total reward: -1.37145\n",
      "UEHitrate: 0.01581  edgeHitrate 0.36415 sumHitrate 0.37996  privacy: 2.0698\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 20:37:09 2021 Episode: 3   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.37201  0.       0.     ] total reward: -0.37201\n",
      "UEHitrate: 0.01629  edgeHitrate 0.62818 sumHitrate 0.64446  privacy: 1.01895\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 20:39:32 2021 Episode: 4   Index: 65335   Loss: 0.04147 --\n",
      "Reward: [-1.13412  0.       0.     ] total reward: -1.13412\n",
      "UEHitrate: 0.01524  edgeHitrate 0.39719 sumHitrate 0.41244  privacy: 2.03599\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 20:42:20 2021 Episode: 4   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.36902  0.       0.     ] total reward: -0.36902\n",
      "UEHitrate: 0.02074  edgeHitrate 0.61275 sumHitrate 0.63349  privacy: 0.98108\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 20:44:44 2021 Episode: 5   Index: 65335   Loss: 0.03375 --\n",
      "Reward: [-0.93391  0.       0.     ] total reward: -0.93391\n",
      "UEHitrate: 0.02398  edgeHitrate 0.38596 sumHitrate 0.40994  privacy: 1.96511\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 20:47:30 2021 Episode: 5   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.36475  0.       0.     ] total reward: -0.36475\n",
      "UEHitrate: 0.02752  edgeHitrate 0.62003 sumHitrate 0.64755  privacy: 1.05239\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_exp_ep5_1010-20-47-30\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 20:49:57 2021 Episode: 6   Index: 65335   Loss: 0.02953 --\n",
      "Reward: [-0.81444  0.       0.     ] total reward: -0.81444\n",
      "UEHitrate: 0.03572  edgeHitrate 0.40627 sumHitrate 0.44199  privacy: 1.87605\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 20:52:46 2021 Episode: 6   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.36744  0.       0.     ] total reward: -0.36744\n",
      "UEHitrate: 2e-05  edgeHitrate 0.63052 sumHitrate 0.63054  privacy: 0.91952\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 20:55:16 2021 Episode: 7   Index: 65335   Loss: 0.02507 --\n",
      "Reward: [-0.71962  0.       0.     ] total reward: -0.71962\n",
      "UEHitrate: 0.01391  edgeHitrate 0.46431 sumHitrate 0.47822  privacy: 1.68035\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 20:58:04 2021 Episode: 7   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.49344  0.       0.     ] total reward: -0.49344\n",
      "UEHitrate: 0.00415  edgeHitrate 0.66812 sumHitrate 0.67228  privacy: 0.93695\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 21:00:36 2021 Episode: 8   Index: 65335   Loss: 0.02308 --\n",
      "Reward: [-0.63799  0.       0.     ] total reward: -0.63799\n",
      "UEHitrate: 0.00398  edgeHitrate 0.4908 sumHitrate 0.49478  privacy: 1.5523\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 21:03:24 2021 Episode: 8   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.43464  0.       0.     ] total reward: -0.43464\n",
      "UEHitrate: 0.00344  edgeHitrate 0.64965 sumHitrate 0.65309  privacy: 0.91182\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 21:05:58 2021 Episode: 9   Index: 65335   Loss: 0.01797 --\n",
      "Reward: [-0.56902  0.       0.     ] total reward: -0.56902\n",
      "UEHitrate: 0.00037  edgeHitrate 0.48973 sumHitrate 0.4901  privacy: 1.45183\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 21:08:47 2021 Episode: 9   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.49694  0.       0.     ] total reward: -0.49694\n",
      "UEHitrate: 0.00275  edgeHitrate 0.6433 sumHitrate 0.64605  privacy: 0.93252\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#test\n",
    "policy_net = DQN(5, 1).to(device)\n",
    "target_net = DQN(5, 1).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sun Oct 10 21:09:08 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-0.56579  0.       0.     ] total reward: -0.56579\n",
      "UEHitrate: 0.0  edgeHitrate 0.6492 sumHitrate 0.6492  privacy: 1.20635\n",
      "\n",
      "--Time: Sun Oct 10 21:09:29 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-0.53033  0.       0.     ] total reward: -0.53033\n",
      "UEHitrate: 0.0005  edgeHitrate 0.6627 sumHitrate 0.6632  privacy: 1.10248\n",
      "\n",
      "--Time: Sun Oct 10 21:09:50 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-0.41467  0.       0.     ] total reward: -0.41467\n",
      "UEHitrate: 0.0159  edgeHitrate 0.64917 sumHitrate 0.66507  privacy: 1.08285\n",
      "\n",
      "--Time: Sun Oct 10 21:10:10 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-0.39572  0.       0.     ] total reward: -0.39572\n",
      "UEHitrate: 0.02408  edgeHitrate 0.64538 sumHitrate 0.66945  privacy: 1.07295\n",
      "\n",
      "--Time: Sun Oct 10 21:10:30 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-0.37229  0.       0.     ] total reward: -0.37229\n",
      "UEHitrate: 0.0273  edgeHitrate 0.63708 sumHitrate 0.66438  privacy: 1.06417\n",
      "\n",
      "--Time: Sun Oct 10 21:10:50 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-0.37114  0.       0.     ] total reward: -0.37114\n",
      "UEHitrate: 0.0276  edgeHitrate 0.63718 sumHitrate 0.66478  privacy: 1.06117\n",
      "\n",
      "--Time: Sun Oct 10 21:11:11 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-0.37908  0.       0.     ] total reward: -0.37908\n",
      "UEHitrate: 0.02751  edgeHitrate 0.62811 sumHitrate 0.65563  privacy: 1.05662\n",
      "\n",
      "--Time: Sun Oct 10 21:11:31 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-0.36441  0.       0.     ] total reward: -0.36441\n",
      "UEHitrate: 0.02708  edgeHitrate 0.6203 sumHitrate 0.64738  privacy: 1.05277\n",
      "\n",
      "--Time: Sun Oct 10 21:11:51 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-0.34237  0.       0.     ] total reward: -0.34237\n",
      "UEHitrate: 0.02686  edgeHitrate 0.62547 sumHitrate 0.65232  privacy: 1.0507\n",
      "\n",
      "--Time: Sun Oct 10 21:12:12 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.3299  0.      0.    ] total reward: -0.3299\n",
      "UEHitrate: 0.02631  edgeHitrate 0.62008 sumHitrate 0.64639  privacy: 1.04611\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 10 21:12:12 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.3299  0.      0.    ] total reward: -0.3299\n",
      "UEHitrate: 0.02631  edgeHitrate 0.62008 sumHitrate 0.64639  privacy: 1.04611\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.6492 , 0.6632 , 0.66507, 0.66945, 0.66438, 0.66478, 0.65563,\n",
       "        0.64738, 0.65232, 0.64639, 0.64639]),\n",
       " array([0.     , 0.0005 , 0.0159 , 0.02408, 0.0273 , 0.0276 , 0.02751,\n",
       "        0.02708, 0.02686, 0.02631, 0.02631]),\n",
       " array([0.6492 , 0.6627 , 0.64917, 0.64538, 0.63708, 0.63718, 0.62811,\n",
       "        0.6203 , 0.62547, 0.62008, 0.62008]))"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.20635, 1.10248, 1.08285, 1.07295, 1.06417, 1.06117, 1.05662,\n",
       "       1.05277, 1.0507 , 1.04611, 1.04611])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}