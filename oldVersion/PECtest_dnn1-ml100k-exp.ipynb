{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_path = '/home/ubuntu/data/dataset/ml-100k/'\n",
    "UIT = pd.read_csv(data_path + 'ml_header.trace')\n",
    "UIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>province</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>942</td>\n",
       "      <td>1337</td>\n",
       "      <td>29</td>\n",
       "      <td>2591999</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>942</td>\n",
       "      <td>1460</td>\n",
       "      <td>29</td>\n",
       "      <td>2591999</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>942</td>\n",
       "      <td>302</td>\n",
       "      <td>29</td>\n",
       "      <td>2591999</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>942</td>\n",
       "      <td>381</td>\n",
       "      <td>29</td>\n",
       "      <td>2591999</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>942</td>\n",
       "      <td>1467</td>\n",
       "      <td>29</td>\n",
       "      <td>2591999</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         u     i  day     time  province  city\n",
       "0        0     0    0        0         3    17\n",
       "1        0     1    0        2         3    17\n",
       "2        0     2    0        6         3    17\n",
       "3        0     3    0        9         3    17\n",
       "4        0     4    0       18         3    17\n",
       "...    ...   ...  ...      ...       ...   ...\n",
       "99995  942  1337   29  2591999         1    17\n",
       "99996  942  1460   29  2591999         1    17\n",
       "99997  942   302   29  2591999         1    17\n",
       "99998  942   381   29  2591999         1    17\n",
       "99999  942  1467   29  2591999         1    17\n",
       "\n",
       "[100000 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "validUIT = UIT[UIT['day']<24]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT,validUIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(1682,\n",
       " 943,\n",
       "          u     i  day     time  province  city\n",
       " 0        0     0    0        0         3    17\n",
       " 1        0     1    0        2         3    17\n",
       " 2        0     2    0        6         3    17\n",
       " 3        0     3    0        9         3    17\n",
       " 4        0     4    0       18         3    17\n",
       " ...    ...   ...  ...      ...       ...   ...\n",
       " 65331  383   995   17  1554979         4     0\n",
       " 65332  383   992   17  1554979         4     0\n",
       " 65333  383  1456   17  1554997         4     0\n",
       " 65334  383   309   17  1555010         4     0\n",
       " 65335  383  1337   17  1555033         4     0\n",
       " \n",
       " [65336 rows x 6 columns],\n",
       "          u     i  day     time  province  city\n",
       " 0        0     0    0        0         3    17\n",
       " 1        0     1    0        2         3    17\n",
       " 2        0     2    0        6         3    17\n",
       " 3        0     3    0        9         3    17\n",
       " 4        0     4    0       18         3    17\n",
       " ...    ...   ...  ...      ...       ...   ...\n",
       " 81109  470  1592   23  2072832         1    13\n",
       " 81110  470   309   23  2072832         1    13\n",
       " 81111  470   725   23  2072837         1    13\n",
       " 81112  470  1617   23  2072868         1    13\n",
       " 81113  470   984   23  2072904         1    13\n",
       " \n",
       " [81114 rows x 6 columns])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=15,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)).sum() - torch.log(v * p + (1-v) * (1-p)).sum())\n",
    "        self.Rh = self.ALPHAh * ( 1 / ( 1 + torch.exp( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "        if self.W[-1] not in np.argwhere(self.lastAction.numpy()): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            if  not train or (train and sample > eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                with torch.no_grad():\n",
    "                    Q_value = QNetwork(self.statusFeature)\n",
    "                    actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "                QNetwork.train()\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "\n",
    "            self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "            self.action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                self.action[index] = 1\n",
    "            env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "        else:\n",
    "            self.action = self.lastAction # keep the cache and no request the new video\n",
    "\n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "\n",
    "        if train:\n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                        lastAction.to(device), \n",
    "                        self.statusFeature,\n",
    "                        torch.tensor([self.reward.float()]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "                        \n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort()[0:10])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName+'dnn_exp_'\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "num_episodes = 10\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = trainUIT.shape[0]*5\n",
    "agentStep = 0\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "           edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 50000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "           edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "\n",
    "        if index % 50000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Fri Oct  8 16:57:26 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:02:16 2021 Episode: 0   Index: 50000   Loss: 0.03585 --\n",
      "Reward: [0.54585 0.00231 0.79115] total reward: 1.33931\n",
      "UEHitrate: 0.01106  edgeHitrate 0.30853 sumHitrate 0.31959  privacy: 2.2331\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct  8 17:03:47 2021 Episode: 0   Index: 65335   Loss: 0.03289 --\n",
      "Reward: [0.53798 0.00247 0.79045] total reward: 1.3309\n",
      "UEHitrate: 0.01194  edgeHitrate 0.30847 sumHitrate 0.32041  privacy: 1.70072\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 17:03:47 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:05:40 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.52493 0.0032  0.78718] total reward: 1.31532\n",
      "UEHitrate: 0.01602  edgeHitrate 0.63589 sumHitrate 0.65191  privacy: 1.19336\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Fri Oct  8 17:06:47 2021 Episode: 0   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.52011 0.00258 0.78966] total reward: 1.31236\n",
      "UEHitrate: 0.01292  edgeHitrate 0.61242 sumHitrate 0.62534  privacy: 1.1529\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_exp_ep0_1008-17-06-47\n",
      "\n",
      "--Time: Fri Oct  8 17:06:47 2021 Episode: 1   Index: 0   Loss: 0.69682 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:11:53 2021 Episode: 1   Index: 50000   Loss: 0.02647 --\n",
      "Reward: [0.54603 0.00139 0.79475] total reward: 1.34218\n",
      "UEHitrate: 0.00656  edgeHitrate 0.27499 sumHitrate 0.28155  privacy: 2.21346\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct  8 17:13:30 2021 Episode: 1   Index: 65335   Loss: 0.02486 --\n",
      "Reward: [0.53862 0.00149 0.79443] total reward: 1.33454\n",
      "UEHitrate: 0.00696  edgeHitrate 0.27403 sumHitrate 0.28099  privacy: 1.614\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 17:13:30 2021 Episode: 1   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:15:23 2021 Episode: 1   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54622 0.00163 0.79381] total reward: 1.34165\n",
      "UEHitrate: 0.00774  edgeHitrate 0.27323 sumHitrate 0.28097  privacy: 1.92001\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Fri Oct  8 17:16:28 2021 Episode: 1   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.53278 0.00156 0.79412] total reward: 1.32846\n",
      "UEHitrate: 0.00735  edgeHitrate 0.26231 sumHitrate 0.26966  privacy: 1.09798\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_exp_ep1_1008-17-16-28\n",
      "\n",
      "--Time: Fri Oct  8 17:16:28 2021 Episode: 2   Index: 0   Loss: 0.3193 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:21:46 2021 Episode: 2   Index: 50000   Loss: 0.01611 --\n",
      "Reward: [0.54437 0.00191 0.79299] total reward: 1.33928\n",
      "UEHitrate: 0.00876  edgeHitrate 0.29795 sumHitrate 0.30671  privacy: 2.20959\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct  8 17:23:25 2021 Episode: 2   Index: 65335   Loss: 0.01528 --\n",
      "Reward: [0.53684 0.00174 0.79358] total reward: 1.33216\n",
      "UEHitrate: 0.00802  edgeHitrate 0.28744 sumHitrate 0.29546  privacy: 1.6826\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 17:23:25 2021 Episode: 2   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:25:20 2021 Episode: 2   Index: 50000   Loss: 0.0 --\n",
      "Reward: [5.2724e-01 2.3000e-04 7.9914e-01] total reward: 1.32661\n",
      "UEHitrate: 0.00108  edgeHitrate 0.51229 sumHitrate 0.51337  privacy: 1.37464\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Fri Oct  8 17:26:27 2021 Episode: 2   Index: 81113   Loss: 0.0 --\n",
      "Reward: [5.2174e-01 2.6000e-04 7.9906e-01] total reward: 1.32107\n",
      "UEHitrate: 0.00117  edgeHitrate 0.48775 sumHitrate 0.48892  privacy: 1.28998\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 17:26:27 2021 Episode: 3   Index: 0   Loss: 0.32662 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:31:49 2021 Episode: 3   Index: 50000   Loss: 0.02148 --\n",
      "Reward: [0.54547 0.00172 0.79331] total reward: 1.34051\n",
      "UEHitrate: 0.00836  edgeHitrate 0.25859 sumHitrate 0.26695  privacy: 2.03241\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct  8 17:33:29 2021 Episode: 3   Index: 65335   Loss: 0.02035 --\n",
      "Reward: [0.53766 0.00169 0.79358] total reward: 1.33293\n",
      "UEHitrate: 0.00802  edgeHitrate 0.256 sumHitrate 0.26402  privacy: 1.50436\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 17:33:29 2021 Episode: 3   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:35:27 2021 Episode: 3   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.52694 0.00149 0.7941 ] total reward: 1.32252\n",
      "UEHitrate: 0.00738  edgeHitrate 0.54283 sumHitrate 0.55021  privacy: 1.35683\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Fri Oct  8 17:36:34 2021 Episode: 3   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.52111 0.00242 0.79046] total reward: 1.31399\n",
      "UEHitrate: 0.01192  edgeHitrate 0.58235 sumHitrate 0.59427  privacy: 1.2574\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 17:36:34 2021 Episode: 4   Index: 0   Loss: 0.48743 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:40:58 2021 Episode: 4   Index: 50000   Loss: 0.02238 --\n",
      "Reward: [0.54238 0.00176 0.79342] total reward: 1.33756\n",
      "UEHitrate: 0.00822  edgeHitrate 0.31539 sumHitrate 0.32361  privacy: 2.42159\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct  8 17:42:11 2021 Episode: 4   Index: 65335   Loss: 0.01911 --\n",
      "Reward: [0.53655 0.00169 0.79362] total reward: 1.33186\n",
      "UEHitrate: 0.00797  edgeHitrate 0.29977 sumHitrate 0.30775  privacy: 1.81842\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 17:42:11 2021 Episode: 4   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:43:46 2021 Episode: 4   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54521 0.00174 0.79334] total reward: 1.34029\n",
      "UEHitrate: 0.00832  edgeHitrate 0.26867 sumHitrate 0.27699  privacy: 1.9147\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Fri Oct  8 17:44:45 2021 Episode: 4   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.53284 0.00167 0.79364] total reward: 1.32815\n",
      "UEHitrate: 0.00795  edgeHitrate 0.26828 sumHitrate 0.27623  privacy: 0.988\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 17:44:45 2021 Episode: 5   Index: 0   Loss: 0.24903 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:48:58 2021 Episode: 5   Index: 50000   Loss: 0.00808 --\n",
      "Reward: [0.54583 0.0017  0.7937 ] total reward: 1.34123\n",
      "UEHitrate: 0.00788  edgeHitrate 0.26949 sumHitrate 0.27737  privacy: 1.99691\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct  8 17:50:44 2021 Episode: 5   Index: 65335   Loss: 0.00769 --\n",
      "Reward: [0.53822 0.00171 0.79374] total reward: 1.33367\n",
      "UEHitrate: 0.00782  edgeHitrate 0.26869 sumHitrate 0.27651  privacy: 1.41\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 17:50:44 2021 Episode: 5   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:52:29 2021 Episode: 5   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54549 0.00155 0.79418] total reward: 1.34121\n",
      "UEHitrate: 0.00728  edgeHitrate 0.27641 sumHitrate 0.28369  privacy: 1.91484\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Fri Oct  8 17:53:31 2021 Episode: 5   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.533   0.00181 0.79332] total reward: 1.32813\n",
      "UEHitrate: 0.00835  edgeHitrate 0.27989 sumHitrate 0.28824  privacy: 0.98761\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 17:53:31 2021 Episode: 6   Index: 0   Loss: 0.11068 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 17:59:07 2021 Episode: 6   Index: 50000   Loss: 0.00797 --\n",
      "Reward: [0.5418  0.00152 0.79411] total reward: 1.33743\n",
      "UEHitrate: 0.00736  edgeHitrate 0.32231 sumHitrate 0.32967  privacy: 2.354\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct  8 18:01:20 2021 Episode: 6   Index: 65335   Loss: 0.00783 --\n",
      "Reward: [0.53399 0.00158 0.79387] total reward: 1.32943\n",
      "UEHitrate: 0.00767  edgeHitrate 0.33816 sumHitrate 0.34583  privacy: 1.90603\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 18:01:20 2021 Episode: 6   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 18:03:51 2021 Episode: 6   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.52668 0.00268 0.78933] total reward: 1.31869\n",
      "UEHitrate: 0.01334  edgeHitrate 0.61225 sumHitrate 0.62559  privacy: 1.2406\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Fri Oct  8 18:05:19 2021 Episode: 6   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.52094 0.00363 0.78556] total reward: 1.31013\n",
      "UEHitrate: 0.01805  edgeHitrate 0.59533 sumHitrate 0.61338  privacy: 1.1602\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 18:05:19 2021 Episode: 7   Index: 0   Loss: 0.16261 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 18:12:19 2021 Episode: 7   Index: 50000   Loss: 0.00399 --\n",
      "Reward: [0.53801 0.00093 0.7964 ] total reward: 1.33534\n",
      "UEHitrate: 0.0045  edgeHitrate 0.30211 sumHitrate 0.30661  privacy: 2.42125\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct  8 18:14:18 2021 Episode: 7   Index: 65335   Loss: 0.00384 --\n",
      "Reward: [0.53184 0.00096 0.79634] total reward: 1.32914\n",
      "UEHitrate: 0.00458  edgeHitrate 0.31297 sumHitrate 0.31754  privacy: 2.06157\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 18:14:18 2021 Episode: 7   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 18:16:36 2021 Episode: 7   Index: 50000   Loss: 0.0 --\n",
      "Reward: [5.277e-01 6.300e-04 7.976e-01] total reward: 1.32593\n",
      "UEHitrate: 0.003  edgeHitrate 0.58211 sumHitrate 0.58511  privacy: 1.27186\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Fri Oct  8 18:18:09 2021 Episode: 7   Index: 81113   Loss: 0.0 --\n",
      "Reward: [5.2252e-01 5.8000e-04 7.9787e-01] total reward: 1.32098\n",
      "UEHitrate: 0.00266  edgeHitrate 0.5466 sumHitrate 0.54926  privacy: 1.22326\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 18:18:09 2021 Episode: 8   Index: 0   Loss: 0.1353 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 18:25:24 2021 Episode: 8   Index: 50000   Loss: 0.00684 --\n",
      "Reward: [0.53715 0.00084 0.79685] total reward: 1.33484\n",
      "UEHitrate: 0.00394  edgeHitrate 0.27281 sumHitrate 0.27675  privacy: 2.35381\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct  8 18:27:28 2021 Episode: 8   Index: 65335   Loss: 0.00633 --\n",
      "Reward: [0.53133 0.00085 0.79679] total reward: 1.32897\n",
      "UEHitrate: 0.00401  edgeHitrate 0.27369 sumHitrate 0.2777  privacy: 2.05048\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 18:27:28 2021 Episode: 8   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 18:30:10 2021 Episode: 8   Index: 50000   Loss: 0.0 --\n",
      "Reward: [5.2753e-01 5.7000e-04 7.9784e-01] total reward: 1.32594\n",
      "UEHitrate: 0.0027  edgeHitrate 0.58143 sumHitrate 0.58413  privacy: 1.27225\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Fri Oct  8 18:31:45 2021 Episode: 8   Index: 81113   Loss: 0.0 --\n",
      "Reward: [5.2242e-01 5.4000e-04 7.9798e-01] total reward: 1.32094\n",
      "UEHitrate: 0.00253  edgeHitrate 0.54606 sumHitrate 0.54859  privacy: 1.2236\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 18:31:45 2021 Episode: 9   Index: 0   Loss: 0.18198 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 18:39:11 2021 Episode: 9   Index: 50000   Loss: 0.00512 --\n",
      "Reward: [5.3633e-01 5.8000e-04 7.9773e-01] total reward: 1.33464\n",
      "UEHitrate: 0.00284  edgeHitrate 0.33727 sumHitrate 0.34011  privacy: 2.30441\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Fri Oct  8 18:41:26 2021 Episode: 9   Index: 65335   Loss: 0.00494 --\n",
      "Reward: [5.3070e-01 6.5000e-04 7.9744e-01] total reward: 1.3288\n",
      "UEHitrate: 0.0032  edgeHitrate 0.33279 sumHitrate 0.33599  privacy: 2.03022\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Fri Oct  8 18:41:26 2021 Episode: 9   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 18:43:57 2021 Episode: 9   Index: 50000   Loss: 0.0 --\n",
      "Reward: [5.2750e-01 5.6000e-04 7.9795e-01] total reward: 1.32602\n",
      "UEHitrate: 0.00256  edgeHitrate 0.58139 sumHitrate 0.58395  privacy: 1.27274\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Fri Oct  8 18:45:22 2021 Episode: 9   Index: 81113   Loss: 0.0 --\n",
      "Reward: [5.2248e-01 5.1000e-04 7.9820e-01] total reward: 1.32119\n",
      "UEHitrate: 0.00226  edgeHitrate 0.54637 sumHitrate 0.54862  privacy: 1.22832\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Fri Oct  8 18:45:27 2021   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0.8] total reward: 1.3\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 11.83096\n",
      "\n",
      "--Time: Fri Oct  8 18:45:57 2021   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.6317  0.00198 0.79272] total reward: 1.4264\n",
      "UEHitrate: 0.0075  edgeHitrate 0.30817 sumHitrate 0.31567  privacy: 7.14748\n",
      "\n",
      "--Time: Fri Oct  8 18:46:24 2021   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.58415 0.00214 0.79192] total reward: 1.37821\n",
      "UEHitrate: 0.00915  edgeHitrate 0.32443 sumHitrate 0.33358  privacy: 4.63978\n",
      "\n",
      "--Time: Fri Oct  8 18:46:52 2021   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.56617 0.0019  0.79283] total reward: 1.36089\n",
      "UEHitrate: 0.0087  edgeHitrate 0.30822 sumHitrate 0.31692  privacy: 3.22957\n",
      "\n",
      "--Time: Fri Oct  8 18:47:24 2021   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.55397 0.00173 0.7934 ] total reward: 1.3491\n",
      "UEHitrate: 0.00807  edgeHitrate 0.28432 sumHitrate 0.29239  privacy: 2.42544\n",
      "\n",
      "--Time: Fri Oct  8 18:47:53 2021   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54622 0.00163 0.79381] total reward: 1.34165\n",
      "UEHitrate: 0.0078  edgeHitrate 0.27319 sumHitrate 0.28099  privacy: 1.92001\n",
      "\n",
      "--Time: Fri Oct  8 18:48:23 2021   Index: 60000   Loss: 0.0 --\n",
      "Reward: [0.54081 0.00158 0.79404] total reward: 1.33644\n",
      "UEHitrate: 0.00782  edgeHitrate 0.26746 sumHitrate 0.27528  privacy: 1.48247\n",
      "\n",
      "--Time: Fri Oct  8 18:48:51 2021   Index: 70000   Loss: 0.0 --\n",
      "Reward: [0.53673 0.00159 0.794  ] total reward: 1.33232\n",
      "UEHitrate: 0.00797  edgeHitrate 0.26814 sumHitrate 0.27611  privacy: 1.22291\n",
      "\n",
      "--Time: Fri Oct  8 18:49:19 2021   Index: 80000   Loss: 0.0 --\n",
      "Reward: [0.53312 0.00156 0.79413] total reward: 1.32881\n",
      "UEHitrate: 0.00774  edgeHitrate 0.26265 sumHitrate 0.27038  privacy: 1.11063\n",
      "\n",
      "--Time: Fri Oct  8 18:49:46 2021   Index: 90000   Loss: 0.0 --\n",
      "Reward: [0.5298  0.00151 0.79428] total reward: 1.32559\n",
      "UEHitrate: 0.00746  edgeHitrate 0.25814 sumHitrate 0.2656  privacy: 1.00506\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Fri Oct  8 18:50:13 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [0.527   0.0015  0.79432] total reward: 1.32282\n",
      "UEHitrate: 0.00741  edgeHitrate 0.25439 sumHitrate 0.2618  privacy: 0.94161\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/ml100k/dnn_exp_ep1_1008-17-16-28'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.31567, 0.33358, 0.31692, 0.29239, 0.28099, 0.27528,\n",
       "        0.27611, 0.27038, 0.2656 , 0.2618 ]),\n",
       " array([0.     , 0.0075 , 0.00915, 0.0087 , 0.00807, 0.0078 , 0.00782,\n",
       "        0.00797, 0.00774, 0.00746, 0.00741]),\n",
       " array([0.     , 0.30817, 0.32443, 0.30822, 0.28432, 0.27319, 0.26746,\n",
       "        0.26814, 0.26265, 0.25814, 0.25439]))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([11.83096,  7.14748,  4.63978,  3.22957,  2.42544,  1.92001,\n",
       "        1.48247,  1.22291,  1.11063,  1.00506,  0.94161])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}