{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "validUIT = UIT[UIT['day']<24]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    " \n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)).sum() - torch.log(v * p + (1-v) * (1-p)).sum())\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + torch.exp( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "\n",
    "        if self.W[-1] not in np.argwhere(self.lastAction.numpy()): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            if  not train or (train and sample > eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                with torch.no_grad():\n",
    "                    Q_value = QNetwork(self.statusFeature)\n",
    "                    actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                QNetwork.train()\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "\n",
    "            self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "            self.action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                self.action[index] = 1\n",
    "            env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "        else:\n",
    "            self.action = self.lastAction # keep the cache and no request the new video\n",
    "\n",
    "        if train:\n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                        lastAction.to(device), \n",
    "                        self.statusFeature,\n",
    "                        torch.tensor([self.reward.float()]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "                        \n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName+'dnn_h0_'\n",
    "\n",
    "rewardPara = {\"alpha\":0,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 15\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 50000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "\n",
    "        if index % 50000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sat Oct  9 14:21:28 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 14:32:19 2021 Episode: 0   Index: 50000   Loss: 0.31197 --\n",
      "Reward: [0.      0.00501 0.06955] total reward: 0.07456\n",
      "UEHitrate: 0.00618  edgeHitrate 0.08694 sumHitrate 0.09312  privacy: 2.59481\n",
      "\n",
      "--Time: Sat Oct  9 14:43:59 2021 Episode: 0   Index: 100000   Loss: 0.29233 --\n",
      "Reward: [0.     0.0047 0.0641] total reward: 0.06881\n",
      "UEHitrate: 0.00575  edgeHitrate 0.08013 sumHitrate 0.08588  privacy: 1.92324\n",
      "\n",
      "--Time: Sat Oct  9 14:55:34 2021 Episode: 0   Index: 150000   Loss: 0.27922 --\n",
      "Reward: [0.      0.0052  0.06263] total reward: 0.06783\n",
      "UEHitrate: 0.00638  edgeHitrate 0.07829 sumHitrate 0.08467  privacy: 1.49123\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 15:07:40 2021 Episode: 0   Index: 198174   Loss: 0.26846 --\n",
      "Reward: [0.      0.00549 0.06238] total reward: 0.06787\n",
      "UEHitrate: 0.00658  edgeHitrate 0.07797 sumHitrate 0.08455  privacy: 1.17509\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 15:07:40 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 15:13:48 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.00459 0.22585] total reward: 0.23044\n",
      "UEHitrate: 0.01  edgeHitrate 0.28231 sumHitrate 0.29231  privacy: 1.36964\n",
      "\n",
      "--Time: Sat Oct  9 15:19:52 2021 Episode: 0   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.00428 0.19306] total reward: 0.19735\n",
      "UEHitrate: 0.00846  edgeHitrate 0.24133 sumHitrate 0.24979  privacy: 1.06873\n",
      "\n",
      "--Time: Sat Oct  9 15:25:53 2021 Episode: 0   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.00447 0.17909] total reward: 0.18356\n",
      "UEHitrate: 0.00808  edgeHitrate 0.22386 sumHitrate 0.23194  privacy: 0.85929\n",
      "\n",
      "--Time: Sat Oct  9 15:31:14 2021 Episode: 0   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.00464 0.17359] total reward: 0.17822\n",
      "UEHitrate: 0.00786  edgeHitrate 0.21698 sumHitrate 0.22485  privacy: 0.72365\n",
      "\n",
      "--Time: Sat Oct  9 15:36:16 2021 Episode: 0   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.00471 0.17186] total reward: 0.17656\n",
      "UEHitrate: 0.00773  edgeHitrate 0.21482 sumHitrate 0.22255  privacy: 0.57054\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 15:36:37 2021 Episode: 0   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.00468 0.17158] total reward: 0.17626\n",
      "UEHitrate: 0.00768  edgeHitrate 0.21447 sumHitrate 0.22215  privacy: 0.55921\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h0_ep0_1009-15-36-37\n",
      "\n",
      "--Time: Sat Oct  9 15:36:38 2021 Episode: 1   Index: 0   Loss: 8.49433 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 15:48:55 2021 Episode: 1   Index: 50000   Loss: 0.19716 --\n",
      "Reward: [0.      0.00502 0.08616] total reward: 0.09118\n",
      "UEHitrate: 0.00752  edgeHitrate 0.1077 sumHitrate 0.11522  privacy: 2.35111\n",
      "\n",
      "--Time: Sat Oct  9 16:01:21 2021 Episode: 1   Index: 100000   Loss: 0.18986 --\n",
      "Reward: [0.      0.00475 0.07825] total reward: 0.08299\n",
      "UEHitrate: 0.00657  edgeHitrate 0.09781 sumHitrate 0.10438  privacy: 1.64067\n",
      "\n",
      "--Time: Sat Oct  9 16:14:27 2021 Episode: 1   Index: 150000   Loss: 0.18057 --\n",
      "Reward: [0.      0.00514 0.07413] total reward: 0.07926\n",
      "UEHitrate: 0.00675  edgeHitrate 0.09266 sumHitrate 0.09941  privacy: 1.38206\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 16:27:39 2021 Episode: 1   Index: 198174   Loss: 0.17433 --\n",
      "Reward: [0.      0.00558 0.07246] total reward: 0.07804\n",
      "UEHitrate: 0.00718  edgeHitrate 0.09057 sumHitrate 0.09775  privacy: 1.21558\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 16:27:39 2021 Episode: 1   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 16:33:21 2021 Episode: 1   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.0041  0.21872] total reward: 0.22281\n",
      "UEHitrate: 0.00776  edgeHitrate 0.27339 sumHitrate 0.28115  privacy: 2.30389\n",
      "\n",
      "--Time: Sat Oct  9 16:39:25 2021 Episode: 1   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.00392 0.22106] total reward: 0.22498\n",
      "UEHitrate: 0.00691  edgeHitrate 0.27633 sumHitrate 0.28324  privacy: 1.97889\n",
      "\n",
      "--Time: Sat Oct  9 16:44:39 2021 Episode: 1   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.00419 0.23317] total reward: 0.23736\n",
      "UEHitrate: 0.00699  edgeHitrate 0.29146 sumHitrate 0.29845  privacy: 1.8351\n",
      "\n",
      "--Time: Sat Oct  9 16:50:03 2021 Episode: 1   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.0044  0.24178] total reward: 0.24619\n",
      "UEHitrate: 0.00703  edgeHitrate 0.30223 sumHitrate 0.30926  privacy: 1.69625\n",
      "\n",
      "--Time: Sat Oct  9 16:55:16 2021 Episode: 1   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.00502 0.23802] total reward: 0.24304\n",
      "UEHitrate: 0.00859  edgeHitrate 0.29753 sumHitrate 0.30612  privacy: 1.49224\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 16:55:41 2021 Episode: 1   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.00503 0.23766] total reward: 0.24269\n",
      "UEHitrate: 0.00862  edgeHitrate 0.29707 sumHitrate 0.30569  privacy: 1.48362\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h0_ep1_1009-16-55-41\n",
      "\n",
      "--Time: Sat Oct  9 16:55:42 2021 Episode: 2   Index: 0   Loss: 6.73957 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 17:08:30 2021 Episode: 2   Index: 50000   Loss: 0.22898 --\n",
      "Reward: [0.      0.00444 0.07745] total reward: 0.08189\n",
      "UEHitrate: 0.00532  edgeHitrate 0.09682 sumHitrate 0.10214  privacy: 2.74357\n",
      "\n",
      "--Time: Sat Oct  9 17:21:59 2021 Episode: 2   Index: 100000   Loss: 0.21867 --\n",
      "Reward: [0.      0.0044  0.07054] total reward: 0.07494\n",
      "UEHitrate: 0.00529  edgeHitrate 0.08817 sumHitrate 0.09346  privacy: 2.30001\n",
      "\n",
      "--Time: Sat Oct  9 17:35:10 2021 Episode: 2   Index: 150000   Loss: 0.20443 --\n",
      "Reward: [0.      0.00718 0.07694] total reward: 0.08412\n",
      "UEHitrate: 0.01146  edgeHitrate 0.09617 sumHitrate 0.10763  privacy: 2.03993\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 17:48:12 2021 Episode: 2   Index: 198174   Loss: 0.1922 --\n",
      "Reward: [0.      0.00865 0.08088] total reward: 0.08954\n",
      "UEHitrate: 0.01469  edgeHitrate 0.1011 sumHitrate 0.1158  privacy: 1.84748\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 17:48:13 2021 Episode: 2   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 17:52:32 2021 Episode: 2   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.01561 0.27524] total reward: 0.29085\n",
      "UEHitrate: 0.04652  edgeHitrate 0.34405 sumHitrate 0.39057  privacy: 1.00881\n",
      "\n",
      "--Time: Sat Oct  9 17:57:51 2021 Episode: 2   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.01609 0.2562 ] total reward: 0.27228\n",
      "UEHitrate: 0.04687  edgeHitrate 0.32025 sumHitrate 0.36712  privacy: 1.01044\n",
      "\n",
      "--Time: Sat Oct  9 18:03:18 2021 Episode: 2   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.01654 0.24741] total reward: 0.26395\n",
      "UEHitrate: 0.04739  edgeHitrate 0.30926 sumHitrate 0.35665  privacy: 1.00123\n",
      "\n",
      "--Time: Sat Oct  9 18:08:36 2021 Episode: 2   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.01658 0.24323] total reward: 0.25982\n",
      "UEHitrate: 0.04599  edgeHitrate 0.30404 sumHitrate 0.35003  privacy: 0.99666\n",
      "\n",
      "--Time: Sat Oct  9 18:13:35 2021 Episode: 2   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.01669 0.23762] total reward: 0.25431\n",
      "UEHitrate: 0.04547  edgeHitrate 0.29702 sumHitrate 0.34249  privacy: 0.9952\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 18:13:59 2021 Episode: 2   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.01663 0.23706] total reward: 0.25368\n",
      "UEHitrate: 0.04524  edgeHitrate 0.29632 sumHitrate 0.34156  privacy: 0.99515\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h0_ep2_1009-18-13-59\n",
      "\n",
      "--Time: Sat Oct  9 18:13:59 2021 Episode: 3   Index: 0   Loss: 6.26559 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 18:26:33 2021 Episode: 3   Index: 50000   Loss: 0.19373 --\n",
      "Reward: [0.      0.00902 0.11222] total reward: 0.12124\n",
      "UEHitrate: 0.019  edgeHitrate 0.14028 sumHitrate 0.15928  privacy: 2.7268\n",
      "\n",
      "--Time: Sat Oct  9 18:38:50 2021 Episode: 3   Index: 100000   Loss: 0.18779 --\n",
      "Reward: [0.     0.01   0.1059] total reward: 0.1159\n",
      "UEHitrate: 0.02114  edgeHitrate 0.13238 sumHitrate 0.15352  privacy: 2.28868\n",
      "\n",
      "--Time: Sat Oct  9 18:51:30 2021 Episode: 3   Index: 150000   Loss: 0.17433 --\n",
      "Reward: [0.      0.00887 0.09761] total reward: 0.10648\n",
      "UEHitrate: 0.01708  edgeHitrate 0.12201 sumHitrate 0.13909  privacy: 1.82334\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 19:05:13 2021 Episode: 3   Index: 198174   Loss: 0.16295 --\n",
      "Reward: [0.      0.00835 0.09373] total reward: 0.10208\n",
      "UEHitrate: 0.01518  edgeHitrate 0.11716 sumHitrate 0.13235  privacy: 1.44455\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 19:05:13 2021 Episode: 3   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 19:11:01 2021 Episode: 3   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.01542 0.28257] total reward: 0.29799\n",
      "UEHitrate: 0.0458  edgeHitrate 0.35321 sumHitrate 0.39901  privacy: 0.89524\n",
      "\n",
      "--Time: Sat Oct  9 19:16:29 2021 Episode: 3   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.01531 0.2611 ] total reward: 0.27641\n",
      "UEHitrate: 0.04449  edgeHitrate 0.32638 sumHitrate 0.37087  privacy: 0.85035\n",
      "\n",
      "--Time: Sat Oct  9 19:21:41 2021 Episode: 3   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.01518 0.24982] total reward: 0.26499\n",
      "UEHitrate: 0.04213  edgeHitrate 0.31227 sumHitrate 0.3544  privacy: 0.78463\n",
      "\n",
      "--Time: Sat Oct  9 19:26:59 2021 Episode: 3   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.01492 0.24247] total reward: 0.25739\n",
      "UEHitrate: 0.03939  edgeHitrate 0.30309 sumHitrate 0.34248  privacy: 0.76325\n",
      "\n",
      "--Time: Sat Oct  9 19:31:56 2021 Episode: 3   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.01488 0.2357 ] total reward: 0.25057\n",
      "UEHitrate: 0.03835  edgeHitrate 0.29462 sumHitrate 0.33297  privacy: 0.7609\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 19:32:23 2021 Episode: 3   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.01482 0.2351 ] total reward: 0.24992\n",
      "UEHitrate: 0.03815  edgeHitrate 0.29388 sumHitrate 0.33203  privacy: 0.76142\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 19:32:23 2021 Episode: 4   Index: 0   Loss: 5.03357 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 19:45:35 2021 Episode: 4   Index: 50000   Loss: 0.17 --\n",
      "Reward: [0.      0.00776 0.12221] total reward: 0.12997\n",
      "UEHitrate: 0.01426  edgeHitrate 0.15276 sumHitrate 0.16702  privacy: 2.56457\n",
      "\n",
      "--Time: Sat Oct  9 19:58:17 2021 Episode: 4   Index: 100000   Loss: 0.15992 --\n",
      "Reward: [0.      0.00615 0.10135] total reward: 0.1075\n",
      "UEHitrate: 0.00994  edgeHitrate 0.12669 sumHitrate 0.13663  privacy: 2.12486\n",
      "\n",
      "--Time: Sat Oct  9 20:11:22 2021 Episode: 4   Index: 150000   Loss: 0.14877 --\n",
      "Reward: [0.      0.006   0.09685] total reward: 0.10286\n",
      "UEHitrate: 0.00889  edgeHitrate 0.12107 sumHitrate 0.12996  privacy: 1.81002\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 20:25:37 2021 Episode: 4   Index: 198174   Loss: 0.141 --\n",
      "Reward: [0.      0.00596 0.09593] total reward: 0.10189\n",
      "UEHitrate: 0.00839  edgeHitrate 0.11991 sumHitrate 0.12831  privacy: 1.43418\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 20:25:37 2021 Episode: 4   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 20:31:28 2021 Episode: 4   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.01442 0.28654] total reward: 0.30096\n",
      "UEHitrate: 0.04172  edgeHitrate 0.35817 sumHitrate 0.39989  privacy: 0.80991\n",
      "\n",
      "--Time: Sat Oct  9 20:37:05 2021 Episode: 4   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.0128  0.25904] total reward: 0.27184\n",
      "UEHitrate: 0.03417  edgeHitrate 0.3238 sumHitrate 0.35797  privacy: 0.68744\n",
      "\n",
      "--Time: Sat Oct  9 20:42:42 2021 Episode: 4   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.01259 0.24607] total reward: 0.25866\n",
      "UEHitrate: 0.03203  edgeHitrate 0.30759 sumHitrate 0.33962  privacy: 0.60127\n",
      "\n",
      "--Time: Sat Oct  9 20:47:49 2021 Episode: 4   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.01257 0.23899] total reward: 0.25156\n",
      "UEHitrate: 0.03024  edgeHitrate 0.29874 sumHitrate 0.32898  privacy: 0.5457\n",
      "\n",
      "--Time: Sat Oct  9 20:52:53 2021 Episode: 4   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.01232 0.23284] total reward: 0.24516\n",
      "UEHitrate: 0.02886  edgeHitrate 0.29105 sumHitrate 0.31991  privacy: 0.4844\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 20:53:16 2021 Episode: 4   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.01226 0.23216] total reward: 0.24442\n",
      "UEHitrate: 0.02866  edgeHitrate 0.29021 sumHitrate 0.31887  privacy: 0.4825\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 20:53:16 2021 Episode: 5   Index: 0   Loss: 4.5012 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 21:05:44 2021 Episode: 5   Index: 50000   Loss: 0.15238 --\n",
      "Reward: [0.      0.00474 0.11963] total reward: 0.12437\n",
      "UEHitrate: 0.00704  edgeHitrate 0.14954 sumHitrate 0.15658  privacy: 2.1381\n",
      "\n",
      "--Time: Sat Oct  9 21:18:26 2021 Episode: 5   Index: 100000   Loss: 0.14515 --\n",
      "Reward: [0.      0.00444 0.10457] total reward: 0.10901\n",
      "UEHitrate: 0.00613  edgeHitrate 0.13071 sumHitrate 0.13684  privacy: 1.68397\n",
      "\n",
      "--Time: Sat Oct  9 21:31:25 2021 Episode: 5   Index: 150000   Loss: 0.1386 --\n",
      "Reward: [0.      0.00481 0.10049] total reward: 0.1053\n",
      "UEHitrate: 0.00629  edgeHitrate 0.12561 sumHitrate 0.1319  privacy: 1.51772\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 21:45:51 2021 Episode: 5   Index: 198174   Loss: 0.13335 --\n",
      "Reward: [0.      0.00505 0.0992 ] total reward: 0.10425\n",
      "UEHitrate: 0.00637  edgeHitrate 0.124 sumHitrate 0.13036  privacy: 1.41961\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 21:45:51 2021 Episode: 5   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 21:52:01 2021 Episode: 5   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.01267 0.28739] total reward: 0.30006\n",
      "UEHitrate: 0.03392  edgeHitrate 0.35923 sumHitrate 0.39315  privacy: 0.80697\n",
      "\n",
      "--Time: Sat Oct  9 21:57:34 2021 Episode: 5   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.01113 0.25669] total reward: 0.26782\n",
      "UEHitrate: 0.02774  edgeHitrate 0.32086 sumHitrate 0.3486  privacy: 0.71067\n",
      "\n",
      "--Time: Sat Oct  9 22:03:26 2021 Episode: 5   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.01073 0.24467] total reward: 0.2554\n",
      "UEHitrate: 0.02531  edgeHitrate 0.30584 sumHitrate 0.33115  privacy: 0.59956\n",
      "\n",
      "--Time: Sat Oct  9 22:08:31 2021 Episode: 5   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.01069 0.23884] total reward: 0.24953\n",
      "UEHitrate: 0.02335  edgeHitrate 0.29855 sumHitrate 0.3219  privacy: 0.51077\n",
      "\n",
      "--Time: Sat Oct  9 22:13:04 2021 Episode: 5   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.01054 0.23351] total reward: 0.24404\n",
      "UEHitrate: 0.02236  edgeHitrate 0.29188 sumHitrate 0.31425  privacy: 0.44413\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 22:13:23 2021 Episode: 5   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.01049 0.23309] total reward: 0.24358\n",
      "UEHitrate: 0.0222  edgeHitrate 0.29136 sumHitrate 0.31356  privacy: 0.43881\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 22:13:23 2021 Episode: 6   Index: 0   Loss: 3.82632 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 22:26:04 2021 Episode: 6   Index: 50000   Loss: 0.14659 --\n",
      "Reward: [0.      0.00464 0.13017] total reward: 0.13481\n",
      "UEHitrate: 0.00726  edgeHitrate 0.16272 sumHitrate 0.16998  privacy: 1.976\n",
      "\n",
      "--Time: Sat Oct  9 22:39:04 2021 Episode: 6   Index: 100000   Loss: 0.14014 --\n",
      "Reward: [0.      0.00492 0.11104] total reward: 0.11596\n",
      "UEHitrate: 0.00722  edgeHitrate 0.1388 sumHitrate 0.14602  privacy: 1.74782\n",
      "\n",
      "--Time: Sat Oct  9 22:52:37 2021 Episode: 6   Index: 150000   Loss: 0.12894 --\n",
      "Reward: [0.      0.00515 0.11035] total reward: 0.1155\n",
      "UEHitrate: 0.00723  edgeHitrate 0.13793 sumHitrate 0.14516  privacy: 1.64048\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 23:07:15 2021 Episode: 6   Index: 198174   Loss: 0.12074 --\n",
      "Reward: [0.      0.00534 0.10999] total reward: 0.11532\n",
      "UEHitrate: 0.0072  edgeHitrate 0.13748 sumHitrate 0.14469  privacy: 1.23751\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 23:07:15 2021 Episode: 6   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 23:13:06 2021 Episode: 6   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.0044  0.19736] total reward: 0.20175\n",
      "UEHitrate: 0.00862  edgeHitrate 0.2467 sumHitrate 0.25531  privacy: 2.14409\n",
      "\n",
      "--Time: Sat Oct  9 23:18:01 2021 Episode: 6   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.00409 0.18824] total reward: 0.19233\n",
      "UEHitrate: 0.00736  edgeHitrate 0.2353 sumHitrate 0.24266  privacy: 1.71939\n",
      "\n",
      "--Time: Sat Oct  9 23:23:11 2021 Episode: 6   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.00427 0.19939] total reward: 0.20365\n",
      "UEHitrate: 0.00732  edgeHitrate 0.24923 sumHitrate 0.25655  privacy: 1.53519\n",
      "\n",
      "--Time: Sat Oct  9 23:28:16 2021 Episode: 6   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.00443 0.20811] total reward: 0.21254\n",
      "UEHitrate: 0.00725  edgeHitrate 0.26014 sumHitrate 0.26739  privacy: 1.37904\n",
      "\n",
      "--Time: Sat Oct  9 23:33:07 2021 Episode: 6   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.00455 0.20315] total reward: 0.2077\n",
      "UEHitrate: 0.0074  edgeHitrate 0.25394 sumHitrate 0.26134  privacy: 1.03885\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 23:33:30 2021 Episode: 6   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.00452 0.20245] total reward: 0.20697\n",
      "UEHitrate: 0.00736  edgeHitrate 0.25306 sumHitrate 0.26042  privacy: 1.01929\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 23:33:30 2021 Episode: 7   Index: 0   Loss: 3.28373 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 23:46:48 2021 Episode: 7   Index: 50000   Loss: 0.12641 --\n",
      "Reward: [0.      0.00443 0.12865] total reward: 0.13309\n",
      "UEHitrate: 0.00632  edgeHitrate 0.16082 sumHitrate 0.16714  privacy: 2.51398\n",
      "\n",
      "--Time: Sat Oct  9 23:59:53 2021 Episode: 7   Index: 100000   Loss: 0.11873 --\n",
      "Reward: [0.      0.00438 0.12682] total reward: 0.13121\n",
      "UEHitrate: 0.0066  edgeHitrate 0.15853 sumHitrate 0.16513  privacy: 1.7776\n",
      "\n",
      "--Time: Sun Oct 10 00:12:53 2021 Episode: 7   Index: 150000   Loss: 0.11103 --\n",
      "Reward: [0.      0.00492 0.12867] total reward: 0.13359\n",
      "UEHitrate: 0.00744  edgeHitrate 0.16084 sumHitrate 0.16828  privacy: 1.30339\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 00:27:16 2021 Episode: 7   Index: 198174   Loss: 0.10542 --\n",
      "Reward: [0.      0.00509 0.12792] total reward: 0.13301\n",
      "UEHitrate: 0.00739  edgeHitrate 0.15989 sumHitrate 0.16728  privacy: 0.96668\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 00:27:16 2021 Episode: 7   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 00:32:31 2021 Episode: 7   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.00497 0.29879] total reward: 0.30376\n",
      "UEHitrate: 0.01324  edgeHitrate 0.37349 sumHitrate 0.38673  privacy: 1.33884\n",
      "\n",
      "--Time: Sun Oct 10 00:38:15 2021 Episode: 7   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.00443 0.27915] total reward: 0.28358\n",
      "UEHitrate: 0.01008  edgeHitrate 0.34894 sumHitrate 0.35902  privacy: 1.20892\n",
      "\n",
      "--Time: Sun Oct 10 00:43:22 2021 Episode: 7   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.00449 0.27645] total reward: 0.28094\n",
      "UEHitrate: 0.00905  edgeHitrate 0.34556 sumHitrate 0.35461  privacy: 1.16101\n",
      "\n",
      "--Time: Sun Oct 10 00:48:28 2021 Episode: 7   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.00456 0.2775 ] total reward: 0.28206\n",
      "UEHitrate: 0.00854  edgeHitrate 0.34688 sumHitrate 0.35542  privacy: 1.11861\n",
      "\n",
      "--Time: Sun Oct 10 00:52:45 2021 Episode: 7   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.0046  0.27405] total reward: 0.27865\n",
      "UEHitrate: 0.00825  edgeHitrate 0.34257 sumHitrate 0.35081  privacy: 1.09852\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 00:53:04 2021 Episode: 7   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.00457 0.27346] total reward: 0.27803\n",
      "UEHitrate: 0.00819  edgeHitrate 0.34182 sumHitrate 0.35001  privacy: 1.09726\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h0_ep7_1010-00-53-04\n",
      "\n",
      "--Time: Sun Oct 10 00:53:05 2021 Episode: 8   Index: 0   Loss: 2.98262 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 01:06:25 2021 Episode: 8   Index: 50000   Loss: 0.13888 --\n",
      "Reward: [0.      0.00516 0.18206] total reward: 0.18722\n",
      "UEHitrate: 0.0113  edgeHitrate 0.22758 sumHitrate 0.23888  privacy: 1.5772\n",
      "\n",
      "--Time: Sun Oct 10 01:18:47 2021 Episode: 8   Index: 100000   Loss: 0.13608 --\n",
      "Reward: [0.      0.00472 0.18454] total reward: 0.18926\n",
      "UEHitrate: 0.00983  edgeHitrate 0.23068 sumHitrate 0.24051  privacy: 1.13395\n",
      "\n",
      "--Time: Sun Oct 10 01:32:46 2021 Episode: 8   Index: 150000   Loss: 0.13269 --\n",
      "Reward: [0.      0.00503 0.1778 ] total reward: 0.18283\n",
      "UEHitrate: 0.00927  edgeHitrate 0.22225 sumHitrate 0.23152  privacy: 0.89198\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 01:47:01 2021 Episode: 8   Index: 198174   Loss: 0.12791 --\n",
      "Reward: [0.      0.00514 0.1627 ] total reward: 0.16784\n",
      "UEHitrate: 0.00867  edgeHitrate 0.20338 sumHitrate 0.21205  privacy: 1.15426\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 01:47:01 2021 Episode: 8   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 01:52:05 2021 Episode: 8   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.00461 0.21232] total reward: 0.21692\n",
      "UEHitrate: 0.01008  edgeHitrate 0.26539 sumHitrate 0.27547  privacy: 2.13841\n",
      "\n",
      "--Time: Sun Oct 10 01:57:53 2021 Episode: 8   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.00427 0.19384] total reward: 0.19811\n",
      "UEHitrate: 0.00879  edgeHitrate 0.2423 sumHitrate 0.25109  privacy: 1.60913\n",
      "\n",
      "--Time: Sun Oct 10 02:03:05 2021 Episode: 8   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.00449 0.18609] total reward: 0.19058\n",
      "UEHitrate: 0.00849  edgeHitrate 0.23262 sumHitrate 0.24111  privacy: 1.28877\n",
      "\n",
      "--Time: Sun Oct 10 02:07:57 2021 Episode: 8   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.00466 0.18267] total reward: 0.18732\n",
      "UEHitrate: 0.00828  edgeHitrate 0.22833 sumHitrate 0.23662  privacy: 1.02495\n",
      "\n",
      "--Time: Sun Oct 10 02:12:28 2021 Episode: 8   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.00473 0.18041] total reward: 0.18514\n",
      "UEHitrate: 0.00816  edgeHitrate 0.22552 sumHitrate 0.23367  privacy: 0.76002\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 02:12:50 2021 Episode: 8   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.00471 0.18004] total reward: 0.18474\n",
      "UEHitrate: 0.00811  edgeHitrate 0.22504 sumHitrate 0.23315  privacy: 0.74375\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 02:12:51 2021 Episode: 9   Index: 0   Loss: 2.78443 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 02:26:39 2021 Episode: 9   Index: 50000   Loss: 0.13602 --\n",
      "Reward: [0.      0.0044  0.14501] total reward: 0.1494\n",
      "UEHitrate: 0.00646  edgeHitrate 0.18126 sumHitrate 0.18772  privacy: 2.51875\n",
      "\n",
      "--Time: Sun Oct 10 02:39:50 2021 Episode: 9   Index: 100000   Loss: 0.12276 --\n",
      "Reward: [0.      0.00415 0.13847] total reward: 0.14262\n",
      "UEHitrate: 0.00594  edgeHitrate 0.17309 sumHitrate 0.17903  privacy: 1.79366\n",
      "\n",
      "--Time: Sun Oct 10 02:54:12 2021 Episode: 9   Index: 150000   Loss: 0.11506 --\n",
      "Reward: [0.      0.00457 0.13768] total reward: 0.14225\n",
      "UEHitrate: 0.00641  edgeHitrate 0.1721 sumHitrate 0.17851  privacy: 1.32406\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 03:08:17 2021 Episode: 9   Index: 198174   Loss: 0.10956 --\n",
      "Reward: [0.      0.00483 0.13706] total reward: 0.14189\n",
      "UEHitrate: 0.0066  edgeHitrate 0.17132 sumHitrate 0.17792  privacy: 0.99236\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 03:08:17 2021 Episode: 9   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 03:14:17 2021 Episode: 9   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.00446 0.20113] total reward: 0.20559\n",
      "UEHitrate: 0.00948  edgeHitrate 0.25141 sumHitrate 0.26089  privacy: 2.13853\n",
      "\n",
      "--Time: Sun Oct 10 03:20:34 2021 Episode: 9   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.00413 0.1854 ] total reward: 0.18953\n",
      "UEHitrate: 0.00786  edgeHitrate 0.23175 sumHitrate 0.23961  privacy: 1.62457\n",
      "\n",
      "--Time: Sun Oct 10 03:26:31 2021 Episode: 9   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.00438 0.18131] total reward: 0.18569\n",
      "UEHitrate: 0.00771  edgeHitrate 0.22663 sumHitrate 0.23434  privacy: 1.36137\n",
      "\n",
      "--Time: Sun Oct 10 03:32:31 2021 Episode: 9   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.00454 0.18321] total reward: 0.18776\n",
      "UEHitrate: 0.00762  edgeHitrate 0.22901 sumHitrate 0.23663  privacy: 1.14665\n",
      "\n",
      "--Time: Sun Oct 10 03:38:38 2021 Episode: 9   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.00465 0.18321] total reward: 0.18785\n",
      "UEHitrate: 0.00767  edgeHitrate 0.22901 sumHitrate 0.23668  privacy: 0.83859\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 03:39:00 2021 Episode: 9   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.00463 0.18296] total reward: 0.18759\n",
      "UEHitrate: 0.00763  edgeHitrate 0.2287 sumHitrate 0.23634  privacy: 0.81912\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h0_ep7_1010-00-53-04'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sun Oct 10 03:39:06 2021   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 03:40:05 2021   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.      0.00624 0.31101] total reward: 0.31725\n",
      "UEHitrate: 0.0224  edgeHitrate 0.39016 sumHitrate 0.41256  privacy: 1.42067\n",
      "\n",
      "--Time: Sun Oct 10 03:41:00 2021   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.      0.00538 0.34182] total reward: 0.3472\n",
      "UEHitrate: 0.0151  edgeHitrate 0.42823 sumHitrate 0.44333  privacy: 1.3737\n",
      "\n",
      "--Time: Sun Oct 10 03:41:58 2021   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.      0.00515 0.3205 ] total reward: 0.32565\n",
      "UEHitrate: 0.01277  edgeHitrate 0.40159 sumHitrate 0.41435  privacy: 1.36136\n",
      "\n",
      "--Time: Sun Oct 10 03:42:52 2021   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.      0.00484 0.30741] total reward: 0.31226\n",
      "UEHitrate: 0.0114  edgeHitrate 0.38552 sumHitrate 0.39692  privacy: 1.37976\n",
      "\n",
      "--Time: Sun Oct 10 03:43:57 2021   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.00497 0.29879] total reward: 0.30376\n",
      "UEHitrate: 0.0108  edgeHitrate 0.37485 sumHitrate 0.38565  privacy: 1.33884\n",
      "\n",
      "--Time: Sun Oct 10 03:44:58 2021   Index: 60000   Loss: 0.0 --\n",
      "Reward: [0.      0.00495 0.2927 ] total reward: 0.29766\n",
      "UEHitrate: 0.01008  edgeHitrate 0.36719 sumHitrate 0.37728  privacy: 1.32744\n",
      "\n",
      "--Time: Sun Oct 10 03:45:59 2021   Index: 70000   Loss: 0.0 --\n",
      "Reward: [0.      0.00478 0.28468] total reward: 0.28946\n",
      "UEHitrate: 0.00953  edgeHitrate 0.35714 sumHitrate 0.36667  privacy: 1.29705\n",
      "\n",
      "--Time: Sun Oct 10 03:46:56 2021   Index: 80000   Loss: 0.0 --\n",
      "Reward: [0.      0.00462 0.28458] total reward: 0.2892\n",
      "UEHitrate: 0.00882  edgeHitrate 0.35688 sumHitrate 0.36571  privacy: 1.26682\n",
      "\n",
      "--Time: Sun Oct 10 03:47:58 2021   Index: 90000   Loss: 0.0 --\n",
      "Reward: [0.      0.00456 0.28121] total reward: 0.28577\n",
      "UEHitrate: 0.00829  edgeHitrate 0.35258 sumHitrate 0.36087  privacy: 1.24273\n",
      "\n",
      "--Time: Sun Oct 10 03:49:01 2021   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.00443 0.27915] total reward: 0.28358\n",
      "UEHitrate: 0.00778  edgeHitrate 0.35 sumHitrate 0.35778  privacy: 1.20892\n",
      "\n",
      "--Time: Sun Oct 10 03:50:02 2021   Index: 110000   Loss: 0.0 --\n",
      "Reward: [0.      0.00439 0.27739] total reward: 0.28177\n",
      "UEHitrate: 0.00739  edgeHitrate 0.3479 sumHitrate 0.35529  privacy: 1.18989\n",
      "\n",
      "--Time: Sun Oct 10 03:51:06 2021   Index: 120000   Loss: 0.0 --\n",
      "Reward: [0.      0.00436 0.27526] total reward: 0.27961\n",
      "UEHitrate: 0.00712  edgeHitrate 0.34515 sumHitrate 0.35226  privacy: 1.18219\n",
      "\n",
      "--Time: Sun Oct 10 03:52:08 2021   Index: 130000   Loss: 0.0 --\n",
      "Reward: [0.      0.00448 0.2721 ] total reward: 0.27657\n",
      "UEHitrate: 0.00699  edgeHitrate 0.34111 sumHitrate 0.34811  privacy: 1.17322\n",
      "\n",
      "--Time: Sun Oct 10 03:53:12 2021   Index: 140000   Loss: 0.0 --\n",
      "Reward: [0.      0.00451 0.27412] total reward: 0.27863\n",
      "UEHitrate: 0.00683  edgeHitrate 0.34369 sumHitrate 0.35052  privacy: 1.1707\n",
      "\n",
      "--Time: Sun Oct 10 03:54:05 2021   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.00449 0.27645] total reward: 0.28094\n",
      "UEHitrate: 0.00669  edgeHitrate 0.3466 sumHitrate 0.35329  privacy: 1.16101\n",
      "\n",
      "--Time: Sun Oct 10 03:54:40 2021   Index: 160000   Loss: 0.0 --\n",
      "Reward: [0.      0.00459 0.27438] total reward: 0.27897\n",
      "UEHitrate: 0.00654  edgeHitrate 0.34403 sumHitrate 0.35057  privacy: 1.15532\n",
      "\n",
      "--Time: Sun Oct 10 03:55:15 2021   Index: 170000   Loss: 0.0 --\n",
      "Reward: [0.      0.00457 0.27629] total reward: 0.28086\n",
      "UEHitrate: 0.00636  edgeHitrate 0.34642 sumHitrate 0.35278  privacy: 1.14767\n",
      "\n",
      "--Time: Sun Oct 10 03:55:49 2021   Index: 180000   Loss: 0.0 --\n",
      "Reward: [0.      0.00458 0.27788] total reward: 0.28246\n",
      "UEHitrate: 0.00618  edgeHitrate 0.34844 sumHitrate 0.35462  privacy: 1.13825\n",
      "\n",
      "--Time: Sun Oct 10 03:56:21 2021   Index: 190000   Loss: 0.0 --\n",
      "Reward: [0.      0.00459 0.27767] total reward: 0.28226\n",
      "UEHitrate: 0.00611  edgeHitrate 0.34811 sumHitrate 0.35422  privacy: 1.12528\n",
      "\n",
      "--Time: Sun Oct 10 03:56:51 2021   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.00456 0.2775 ] total reward: 0.28206\n",
      "UEHitrate: 0.00598  edgeHitrate 0.34795 sumHitrate 0.35393  privacy: 1.11861\n",
      "\n",
      "--Time: Sun Oct 10 03:57:20 2021   Index: 210000   Loss: 0.0 --\n",
      "Reward: [0.      0.00457 0.27825] total reward: 0.28283\n",
      "UEHitrate: 0.00592  edgeHitrate 0.3489 sumHitrate 0.35482  privacy: 1.11247\n",
      "\n",
      "--Time: Sun Oct 10 03:57:50 2021   Index: 220000   Loss: 0.0 --\n",
      "Reward: [0.      0.00459 0.27673] total reward: 0.28132\n",
      "UEHitrate: 0.00584  edgeHitrate 0.34698 sumHitrate 0.35282  privacy: 1.10771\n",
      "\n",
      "--Time: Sun Oct 10 03:58:20 2021   Index: 230000   Loss: 0.0 --\n",
      "Reward: [0.      0.0046  0.27564] total reward: 0.28025\n",
      "UEHitrate: 0.00579  edgeHitrate 0.34556 sumHitrate 0.35135  privacy: 1.10504\n",
      "\n",
      "--Time: Sun Oct 10 03:58:49 2021   Index: 240000   Loss: 0.0 --\n",
      "Reward: [0.      0.00462 0.27418] total reward: 0.27879\n",
      "UEHitrate: 0.00572  edgeHitrate 0.34369 sumHitrate 0.34941  privacy: 1.10065\n",
      "\n",
      "--Time: Sun Oct 10 03:59:18 2021   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.0046  0.27405] total reward: 0.27865\n",
      "UEHitrate: 0.0057  edgeHitrate 0.34358 sumHitrate 0.34928  privacy: 1.09852\n",
      "\n",
      "--Time: Sun Oct 10 03:59:48 2021   Index: 260000   Loss: 0.0 --\n",
      "Reward: [0.      0.00458 0.27264] total reward: 0.27721\n",
      "UEHitrate: 0.00566  edgeHitrate 0.34179 sumHitrate 0.34745  privacy: 1.09662\n",
      "\n",
      "--Time: Sun Oct 10 04:00:17 2021   Index: 270000   Loss: 0.0 --\n",
      "Reward: [0.      0.00461 0.27118] total reward: 0.2758\n",
      "UEHitrate: 0.00565  edgeHitrate 0.33995 sumHitrate 0.34561  privacy: 1.09426\n",
      "\n",
      "--Time: Sun Oct 10 04:00:46 2021   Index: 280000   Loss: 0.0 --\n",
      "Reward: [0.      0.00462 0.27121] total reward: 0.27583\n",
      "UEHitrate: 0.0056  edgeHitrate 0.33999 sumHitrate 0.34559  privacy: 1.09203\n",
      "\n",
      "--Time: Sun Oct 10 04:01:16 2021   Index: 290000   Loss: 0.0 --\n",
      "Reward: [0.      0.0046  0.27277] total reward: 0.27737\n",
      "UEHitrate: 0.00555  edgeHitrate 0.34198 sumHitrate 0.34753  privacy: 1.08807\n",
      "\n",
      "--Time: Sun Oct 10 04:01:45 2021   Index: 300000   Loss: 0.0 --\n",
      "Reward: [0.      0.00456 0.27521] total reward: 0.27978\n",
      "UEHitrate: 0.00549  edgeHitrate 0.34501 sumHitrate 0.3505  privacy: 1.08353\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 10 04:01:48 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [0.      0.00456 0.27544] total reward: 0.28\n",
      "UEHitrate: 0.00549  edgeHitrate 0.34529 sumHitrate 0.35077  privacy: 1.08207\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h0_ep7_1010-00-53-04'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.41256, 0.44333, 0.41435, 0.39692, 0.38565, 0.37728,\n",
       "        0.36667, 0.36571, 0.36087, 0.35778, 0.35529, 0.35226, 0.34811,\n",
       "        0.35052, 0.35329, 0.35057, 0.35278, 0.35462, 0.35422, 0.35393,\n",
       "        0.35482, 0.35282, 0.35135, 0.34941, 0.34928, 0.34745, 0.34561,\n",
       "        0.34559, 0.34753, 0.35077]),\n",
       " array([0.     , 0.0224 , 0.0151 , 0.01277, 0.0114 , 0.0108 , 0.01008,\n",
       "        0.00953, 0.00882, 0.00829, 0.00778, 0.00739, 0.00712, 0.00699,\n",
       "        0.00683, 0.00669, 0.00654, 0.00636, 0.00618, 0.00611, 0.00598,\n",
       "        0.00592, 0.00584, 0.00579, 0.00572, 0.0057 , 0.00566, 0.00565,\n",
       "        0.0056 , 0.00555, 0.00549]),\n",
       " array([0.     , 0.39016, 0.42823, 0.40159, 0.38552, 0.37485, 0.36719,\n",
       "        0.35714, 0.35688, 0.35258, 0.35   , 0.3479 , 0.34515, 0.34111,\n",
       "        0.34369, 0.3466 , 0.34403, 0.34642, 0.34844, 0.34811, 0.34795,\n",
       "        0.3489 , 0.34698, 0.34556, 0.34369, 0.34358, 0.34179, 0.33995,\n",
       "        0.33999, 0.34198, 0.34529]))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4.02118, 1.42067, 1.3737 , 1.36136, 1.37976, 1.33884, 1.32744,\n",
       "       1.29705, 1.26682, 1.24273, 1.20892, 1.18989, 1.18219, 1.17322,\n",
       "       1.1707 , 1.16101, 1.15532, 1.14767, 1.13825, 1.12528, 1.11861,\n",
       "       1.11247, 1.10771, 1.10504, 1.10065, 1.09852, 1.09662, 1.09426,\n",
       "       1.09203, 1.08807, 1.08207])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h0_ep7_1010-00-53-04'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}