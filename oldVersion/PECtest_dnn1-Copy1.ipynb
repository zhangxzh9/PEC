{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>1030</td>\n",
       "      <td>101001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11807</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15068</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>1035</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5375</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1030</td>\n",
       "      <td>10202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5992</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3468</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148        1030        101001     0   \n",
       "1       203  5779    0        0         7        1030         10203     0   \n",
       "2       208  4675    0        0        92        1035         10203     0   \n",
       "3       159   332    0        0        56        1030         10202     0   \n",
       "4        50   674    0        0       439        1030         10203     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831   29  2591880        34        1030         10203     0   \n",
       "300979  158  8448   29  2591880        34        1030         10203     0   \n",
       "300980  483  6463   29  2591940        35        1030         10203     0   \n",
       "300981  158  4715   29  2591940        34        1030         10203     0   \n",
       "300982  483  2021   29  2591940        34        1030         10203     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0      11807          1            2  \n",
       "1              0      15068          1            2  \n",
       "2              0       5375          1            2  \n",
       "3              0       5992          1            2  \n",
       "4              0       3468          1            2  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0      10010          1            2  \n",
       "300979         0      23340          1            2  \n",
       "300980         0      10010          1            2  \n",
       "300981         0      23340          1            2  \n",
       "300982         0      10010          1            2  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " 500,\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148        1030        101001     0   \n",
       " 1       203  5779    0        0         7        1030         10203     0   \n",
       " 2       208  4675    0        0        92        1035         10203     0   \n",
       " 3       159   332    0        0        56        1030         10202     0   \n",
       " 4        50   674    0        0       439        1030         10203     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 198170  264  7442   17  1555140        90        1035         10203     0   \n",
       " 198171   19  9362   17  1555140       424        1035         10203     0   \n",
       " 198172   82  9223   17  1555140        94        1037         10203     0   \n",
       " 198173   35  4164   17  1555140        22        1030         10203     0   \n",
       " 198174  239  5062   17  1555140        89        1035         10203     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0      11807          1            2  \n",
       " 1              0      15068          1            2  \n",
       " 2              0       5375          1            2  \n",
       " 3              0       5992          1            2  \n",
       " 4              0       3468          1            2  \n",
       " ...          ...        ...        ...          ...  \n",
       " 198170         0       7592          1            2  \n",
       " 198171         0       5938          1            2  \n",
       " 198172         0      11393          1            2  \n",
       " 198173         0       5866          1            2  \n",
       " 198174         0      23746          1            2  \n",
       " \n",
       " [198175 rows x 12 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=15,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() - torch.log(ru * p + (1-ru) * (1-p)).sum())\n",
    "        self.Rh =   self.ALPHAh *( torch.log(lastru * lastp + (1-lastru) * (1-lastp)).sum() - torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl *  ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "\n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "        \n",
    "        if train: \n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                    lastAction.to(device), \n",
    "                    self.statusFeature,\n",
    "                    torch.tensor([self.reward.float()]).to(device),\n",
    "                    torch.tensor([self.W[-1]]).to(device))\n",
    "        \n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "        \n",
    "\n",
    "        if  not train or (train and sample > eps_threshold):\n",
    "            QNetwork.eval()\n",
    "            with torch.no_grad():\n",
    "                Q_value = QNetwork(self.statusFeature)\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "            QNetwork.train()\n",
    "        else:\n",
    "            actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort()[0:10])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELPATH =  './model_dict/dnn_h1_reward_'\n",
    "rewardPara = {\"alpha\":1,\"betao\":10,\"betal\":10}\n",
    "latency = [0.1,1,0.9]\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Mon Sep 20 11:18:09 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 11:20:39 2021 Episode: 0   Index: 20000   Loss: 0.48118 --\n",
      "Reward: [-4.71013  0.08715  1.01425] total reward: -3.60874\n",
      "UEHitrate: 0.00315  edgeHitrate 0.11314 sumHitrate 0.11629  privacy: 3.21989\n",
      "\n",
      "--Time: Mon Sep 20 11:23:11 2021 Episode: 0   Index: 40000   Loss: 0.53658 --\n",
      "Reward: [-3.65692  0.0966   1.04577] total reward: -2.51454\n",
      "UEHitrate: 0.0043  edgeHitrate 0.1163 sumHitrate 0.1206  privacy: 2.6674\n",
      "\n",
      "--Time: Mon Sep 20 11:25:32 2021 Episode: 0   Index: 60000   Loss: 0.54554 --\n",
      "Reward: [-3.03497  0.10378  0.95593] total reward: -1.97525\n",
      "UEHitrate: 0.00457  edgeHitrate 0.10636 sumHitrate 0.11093  privacy: 2.33183\n",
      "\n",
      "--Time: Mon Sep 20 11:27:56 2021 Episode: 0   Index: 80000   Loss: 0.54169 --\n",
      "Reward: [-2.61399  0.10345  0.89403] total reward: -1.61652\n",
      "UEHitrate: 0.00461  edgeHitrate 0.09949 sumHitrate 0.1041  privacy: 2.09791\n",
      "\n",
      "--Time: Mon Sep 20 11:30:18 2021 Episode: 0   Index: 100000   Loss: 0.53451 --\n",
      "Reward: [-2.24029  0.10254  0.85544] total reward: -1.28231\n",
      "UEHitrate: 0.00475  edgeHitrate 0.09501 sumHitrate 0.09976  privacy: 1.87831\n",
      "\n",
      "--Time: Mon Sep 20 11:32:43 2021 Episode: 0   Index: 120000   Loss: 0.52953 --\n",
      "Reward: [-1.88163  0.10572  0.83152] total reward: -0.94439\n",
      "UEHitrate: 0.00458  edgeHitrate 0.0925 sumHitrate 0.09708  privacy: 1.69564\n",
      "\n",
      "--Time: Mon Sep 20 11:35:07 2021 Episode: 0   Index: 140000   Loss: 0.51946 --\n",
      "Reward: [-1.5922   0.11114  0.81231] total reward: -0.66875\n",
      "UEHitrate: 0.00463  edgeHitrate 0.09051 sumHitrate 0.09514  privacy: 1.53703\n",
      "\n",
      "--Time: Mon Sep 20 11:37:40 2021 Episode: 0   Index: 160000   Loss: 0.50704 --\n",
      "Reward: [-1.40528  0.11715  0.80983] total reward: -0.4783\n",
      "UEHitrate: 0.00466  edgeHitrate 0.09034 sumHitrate 0.095  privacy: 1.37779\n",
      "\n",
      "--Time: Mon Sep 20 11:40:15 2021 Episode: 0   Index: 180000   Loss: 0.4959 --\n",
      "Reward: [-1.22967  0.12757  0.90394] total reward: -0.19816\n",
      "UEHitrate: 0.00523  edgeHitrate 0.10092 sumHitrate 0.10615  privacy: 1.25453\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 11:42:23 2021 Episode: 0   Index: 198174   Loss: 0.48455 --\n",
      "Reward: [-1.12014  0.14246  1.02033] total reward: 0.04265\n",
      "UEHitrate: 0.00661  edgeHitrate 0.11398 sumHitrate 0.12058  privacy: 1.18667\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward_ep0_0920-11-42-23\n",
      "\n",
      "--Time: Mon Sep 20 11:42:23 2021 Episode: 1   Index: 0   Loss: 8.18278 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 11:44:48 2021 Episode: 1   Index: 20000   Loss: 0.42844 --\n",
      "Reward: [-3.58075  0.08535  0.88376] total reward: -2.61165\n",
      "UEHitrate: 0.0031  edgeHitrate 0.0982 sumHitrate 0.10129  privacy: 3.37011\n",
      "\n",
      "--Time: Mon Sep 20 11:47:11 2021 Episode: 1   Index: 40000   Loss: 0.46592 --\n",
      "Reward: [-2.71062  0.09685  0.76138] total reward: -1.8524\n",
      "UEHitrate: 0.00345  edgeHitrate 0.08465 sumHitrate 0.0881  privacy: 2.89195\n",
      "\n",
      "--Time: Mon Sep 20 11:49:36 2021 Episode: 1   Index: 60000   Loss: 0.46721 --\n",
      "Reward: [-2.22585  0.10345  0.71264] total reward: -1.40976\n",
      "UEHitrate: 0.00372  edgeHitrate 0.07928 sumHitrate 0.083  privacy: 2.57667\n",
      "\n",
      "--Time: Mon Sep 20 11:52:02 2021 Episode: 1   Index: 80000   Loss: 0.4615 --\n",
      "Reward: [-1.9093   0.10072  0.7192 ] total reward: -1.08937\n",
      "UEHitrate: 0.00359  edgeHitrate 0.0801 sumHitrate 0.08369  privacy: 2.36003\n",
      "\n",
      "--Time: Mon Sep 20 11:54:29 2021 Episode: 1   Index: 100000   Loss: 0.45769 --\n",
      "Reward: [-1.6227   0.10226  0.72053] total reward: -0.79991\n",
      "UEHitrate: 0.00369  edgeHitrate 0.08034 sumHitrate 0.08403  privacy: 2.15668\n",
      "\n",
      "--Time: Mon Sep 20 11:56:55 2021 Episode: 1   Index: 120000   Loss: 0.45441 --\n",
      "Reward: [-1.35467  0.10527  0.69674] total reward: -0.55266\n",
      "UEHitrate: 0.0038  edgeHitrate 0.0777 sumHitrate 0.0815  privacy: 1.99824\n",
      "\n",
      "--Time: Mon Sep 20 11:59:19 2021 Episode: 1   Index: 140000   Loss: 0.45343 --\n",
      "Reward: [-1.16472  0.11021  0.67416] total reward: -0.38036\n",
      "UEHitrate: 0.00396  edgeHitrate 0.07527 sumHitrate 0.07924  privacy: 1.79777\n",
      "\n",
      "--Time: Mon Sep 20 12:01:55 2021 Episode: 1   Index: 160000   Loss: 0.45192 --\n",
      "Reward: [-1.03782  0.11439  0.65835] total reward: -0.26508\n",
      "UEHitrate: 0.00402  edgeHitrate 0.07346 sumHitrate 0.07748  privacy: 1.62015\n",
      "\n",
      "--Time: Mon Sep 20 12:04:21 2021 Episode: 1   Index: 180000   Loss: 0.44891 --\n",
      "Reward: [-0.90977  0.11696  0.65245] total reward: -0.14036\n",
      "UEHitrate: 0.00403  edgeHitrate 0.07284 sumHitrate 0.07687  privacy: 1.45996\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 12:06:33 2021 Episode: 1   Index: 198174   Loss: 0.44141 --\n",
      "Reward: [-0.82638  0.11677  0.64788] total reward: -0.06173\n",
      "UEHitrate: 0.00411  edgeHitrate 0.07235 sumHitrate 0.07645  privacy: 1.37153\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 12:06:33 2021 Episode: 2   Index: 0   Loss: 9.42989 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 12:12:18 2021 Episode: 2   Index: 20000   Loss: 0.46409 --\n",
      "Reward: [-4.66514  0.08175  0.73436] total reward: -3.84903\n",
      "UEHitrate: 0.0026  edgeHitrate 0.0822 sumHitrate 0.0848  privacy: 3.37844\n",
      "\n",
      "--Time: Mon Sep 20 12:15:35 2021 Episode: 2   Index: 40000   Loss: 0.52011 --\n",
      "Reward: [-3.62448  0.09087  0.68286] total reward: -2.85075\n",
      "UEHitrate: 0.00315  edgeHitrate 0.0765 sumHitrate 0.07965  privacy: 2.77\n",
      "\n",
      "--Time: Mon Sep 20 12:18:40 2021 Episode: 2   Index: 60000   Loss: 0.53745 --\n",
      "Reward: [-3.04601  0.10196  0.65714] total reward: -2.2869\n",
      "UEHitrate: 0.00333  edgeHitrate 0.07383 sumHitrate 0.07717  privacy: 2.37707\n",
      "\n",
      "--Time: Mon Sep 20 12:21:39 2021 Episode: 2   Index: 80000   Loss: 0.53639 --\n",
      "Reward: [-2.57957  0.0993   0.70537] total reward: -1.7749\n",
      "UEHitrate: 0.00342  edgeHitrate 0.07906 sumHitrate 0.08249  privacy: 2.14423\n",
      "\n",
      "--Time: Mon Sep 20 12:24:15 2021 Episode: 2   Index: 100000   Loss: 0.53306 --\n",
      "Reward: [-2.13739  0.10185  0.85391] total reward: -1.18163\n",
      "UEHitrate: 0.00373  edgeHitrate 0.09562 sumHitrate 0.09935  privacy: 1.97563\n",
      "\n",
      "--Time: Mon Sep 20 12:26:52 2021 Episode: 2   Index: 120000   Loss: 0.53092 --\n",
      "Reward: [-1.76565  0.10942  1.09117] total reward: -0.56506\n",
      "UEHitrate: 0.00423  edgeHitrate 0.122 sumHitrate 0.12623  privacy: 1.85081\n",
      "\n",
      "--Time: Mon Sep 20 12:29:23 2021 Episode: 2   Index: 140000   Loss: 0.52397 --\n",
      "Reward: [-1.48423  0.11429  1.05473] total reward: -0.31521\n",
      "UEHitrate: 0.00435  edgeHitrate 0.11796 sumHitrate 0.12231  privacy: 1.74376\n",
      "\n",
      "--Time: Mon Sep 20 12:31:58 2021 Episode: 2   Index: 160000   Loss: 0.51226 --\n",
      "Reward: [-1.29881  0.11847  0.99613] total reward: -0.18421\n",
      "UEHitrate: 0.00432  edgeHitrate 0.11142 sumHitrate 0.11574  privacy: 1.65068\n",
      "\n",
      "--Time: Mon Sep 20 12:34:47 2021 Episode: 2   Index: 180000   Loss: 0.5045 --\n",
      "Reward: [-1.1368   0.12244  1.07574] total reward: 0.06138\n",
      "UEHitrate: 0.00451  edgeHitrate 0.12027 sumHitrate 0.12478  privacy: 1.56327\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 12:37:24 2021 Episode: 2   Index: 198174   Loss: 0.49751 --\n",
      "Reward: [-1.03813  0.12344  1.09549] total reward: 0.1808\n",
      "UEHitrate: 0.00458  edgeHitrate 0.12247 sumHitrate 0.12705  privacy: 1.49161\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward_ep2_0920-12-37-24\n",
      "\n",
      "--Time: Mon Sep 20 12:37:24 2021 Episode: 3   Index: 0   Loss: 9.8144 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 12:40:21 2021 Episode: 3   Index: 20000   Loss: 0.44205 --\n",
      "Reward: [-2.68384  0.09135  0.72401] total reward: -1.86848\n",
      "UEHitrate: 0.003  edgeHitrate 0.0806 sumHitrate 0.0836  privacy: 3.27126\n",
      "\n",
      "--Time: Mon Sep 20 12:43:20 2021 Episode: 3   Index: 40000   Loss: 0.47803 --\n",
      "Reward: [-2.1994   0.0973   0.64753] total reward: -1.45457\n",
      "UEHitrate: 0.00317  edgeHitrate 0.07222 sumHitrate 0.0754  privacy: 2.90884\n",
      "\n",
      "--Time: Mon Sep 20 12:46:14 2021 Episode: 3   Index: 60000   Loss: 0.53691 --\n",
      "Reward: [-2.1245   0.1022   0.63764] total reward: -1.38466\n",
      "UEHitrate: 0.0033  edgeHitrate 0.07112 sumHitrate 0.07442  privacy: 2.52589\n",
      "\n",
      "--Time: Mon Sep 20 12:49:15 2021 Episode: 3   Index: 80000   Loss: 0.5757 --\n",
      "Reward: [-2.00333  0.09802  0.60895] total reward: -1.29635\n",
      "UEHitrate: 0.00322  edgeHitrate 0.06787 sumHitrate 0.0711  privacy: 2.23182\n",
      "\n",
      "--Time: Mon Sep 20 12:52:09 2021 Episode: 3   Index: 100000   Loss: 0.60777 --\n",
      "Reward: [-1.64438  0.10286  0.96587] total reward: -0.57565\n",
      "UEHitrate: 0.00387  edgeHitrate 0.10757 sumHitrate 0.11144  privacy: 2.06206\n",
      "\n",
      "--Time: Mon Sep 20 12:55:01 2021 Episode: 3   Index: 120000   Loss: 0.62949 --\n",
      "Reward: [-1.3579   0.10885  1.09559] total reward: -0.15346\n",
      "UEHitrate: 0.00412  edgeHitrate 0.12224 sumHitrate 0.12636  privacy: 1.90339\n",
      "\n",
      "--Time: Mon Sep 20 12:57:50 2021 Episode: 3   Index: 140000   Loss: 0.64282 --\n",
      "Reward: [-1.15267  0.11468  1.0528 ] total reward: 0.01481\n",
      "UEHitrate: 0.00425  edgeHitrate 0.11762 sumHitrate 0.12187  privacy: 1.69852\n",
      "\n",
      "--Time: Mon Sep 20 13:00:48 2021 Episode: 3   Index: 160000   Loss: 0.65339 --\n",
      "Reward: [-1.04693  0.11894  1.01727] total reward: 0.08928\n",
      "UEHitrate: 0.00432  edgeHitrate 0.11361 sumHitrate 0.11792  privacy: 1.49509\n",
      "\n",
      "--Time: Mon Sep 20 13:03:43 2021 Episode: 3   Index: 180000   Loss: 0.66014 --\n",
      "Reward: [-0.92309  0.12148  1.00024] total reward: 0.19863\n",
      "UEHitrate: 0.00434  edgeHitrate 0.11172 sumHitrate 0.11607  privacy: 1.28798\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 13:06:17 2021 Episode: 3   Index: 198174   Loss: 0.66455 --\n",
      "Reward: [-0.85868  0.12193  0.97909] total reward: 0.24233\n",
      "UEHitrate: 0.00441  edgeHitrate 0.10939 sumHitrate 0.11379  privacy: 1.12389\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward_ep3_0920-13-06-17\n",
      "\n",
      "--Time: Mon Sep 20 13:06:17 2021 Episode: 4   Index: 0   Loss: 18.27742 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 13:09:07 2021 Episode: 4   Index: 20000   Loss: 0.70037 --\n",
      "Reward: [-2.49782  0.07925  0.78656] total reward: -1.63201\n",
      "UEHitrate: 0.00215  edgeHitrate 0.0876 sumHitrate 0.08975  privacy: 3.10438\n",
      "\n",
      "--Time: Mon Sep 20 13:11:59 2021 Episode: 4   Index: 40000   Loss: 0.75175 --\n",
      "Reward: [-2.68376  0.09982  1.25614] total reward: -1.3278\n",
      "UEHitrate: 0.00402  edgeHitrate 0.13997 sumHitrate 0.144  privacy: 2.35262\n",
      "\n",
      "--Time: Mon Sep 20 13:14:53 2021 Episode: 4   Index: 60000   Loss: 0.77922 --\n",
      "Reward: [-2.55978  0.10296  1.13173] total reward: -1.32508\n",
      "UEHitrate: 0.00422  edgeHitrate 0.12608 sumHitrate 0.1303  privacy: 2.04895\n",
      "\n",
      "--Time: Mon Sep 20 13:17:45 2021 Episode: 4   Index: 80000   Loss: 0.78118 --\n",
      "Reward: [-2.40796  0.09886  0.99426] total reward: -1.31484\n",
      "UEHitrate: 0.00395  edgeHitrate 0.11082 sumHitrate 0.11477  privacy: 1.86823\n",
      "\n",
      "--Time: Mon Sep 20 13:20:40 2021 Episode: 4   Index: 100000   Loss: 0.77472 --\n",
      "Reward: [-2.06178  0.09874  0.89954] total reward: -1.0635\n",
      "UEHitrate: 0.0039  edgeHitrate 0.10025 sumHitrate 0.10415  privacy: 1.72886\n",
      "\n",
      "--Time: Mon Sep 20 13:23:35 2021 Episode: 4   Index: 120000   Loss: 0.76413 --\n",
      "Reward: [-1.70192  0.10162  0.83549] total reward: -0.76481\n",
      "UEHitrate: 0.00385  edgeHitrate 0.09315 sumHitrate 0.097  privacy: 1.62538\n",
      "\n",
      "--Time: Mon Sep 20 13:26:30 2021 Episode: 4   Index: 140000   Loss: 0.74688 --\n",
      "Reward: [-1.40882  0.10738  0.78557] total reward: -0.51587\n",
      "UEHitrate: 0.00401  edgeHitrate 0.08761 sumHitrate 0.09162  privacy: 1.49675\n",
      "\n",
      "--Time: Mon Sep 20 13:29:34 2021 Episode: 4   Index: 160000   Loss: 0.73294 --\n",
      "Reward: [-1.23719  0.11111  0.74705] total reward: -0.37903\n",
      "UEHitrate: 0.00404  edgeHitrate 0.08328 sumHitrate 0.08732  privacy: 1.35341\n",
      "\n",
      "--Time: Mon Sep 20 13:32:33 2021 Episode: 4   Index: 180000   Loss: 0.71913 --\n",
      "Reward: [-1.07153  0.11285  0.72265] total reward: -0.23603\n",
      "UEHitrate: 0.00401  edgeHitrate 0.08058 sumHitrate 0.08459  privacy: 1.20846\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 13:35:09 2021 Episode: 4   Index: 198174   Loss: 0.70939 --\n",
      "Reward: [-0.97244  0.11309  0.70488] total reward: -0.15447\n",
      "UEHitrate: 0.00404  edgeHitrate 0.07858 sumHitrate 0.08262  privacy: 1.08668\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 13:35:09 2021 Episode: 5   Index: 0   Loss: 10.37019 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 13:38:02 2021 Episode: 5   Index: 20000   Loss: 0.70527 --\n",
      "Reward: [-2.9808   0.26014  2.38083] total reward: -0.33983\n",
      "UEHitrate: 0.0147  edgeHitrate 0.26499 sumHitrate 0.27969  privacy: 2.72659\n",
      "\n",
      "--Time: Mon Sep 20 13:40:55 2021 Episode: 5   Index: 40000   Loss: 0.78497 --\n",
      "Reward: [-2.97173  0.1939   1.85935] total reward: -0.91848\n",
      "UEHitrate: 0.01005  edgeHitrate 0.20749 sumHitrate 0.21754  privacy: 2.21394\n",
      "\n",
      "--Time: Mon Sep 20 13:43:47 2021 Episode: 5   Index: 60000   Loss: 0.79717 --\n",
      "Reward: [-2.72462  0.1674   1.49863] total reward: -1.0586\n",
      "UEHitrate: 0.00817  edgeHitrate 0.16715 sumHitrate 0.17531  privacy: 1.97356\n",
      "\n",
      "--Time: Mon Sep 20 13:46:39 2021 Episode: 5   Index: 80000   Loss: 0.79001 --\n",
      "Reward: [-2.53567  0.14647  1.25627] total reward: -1.13293\n",
      "UEHitrate: 0.00696  edgeHitrate 0.14006 sumHitrate 0.14702  privacy: 1.83417\n",
      "\n",
      "--Time: Mon Sep 20 13:49:32 2021 Episode: 5   Index: 100000   Loss: 0.77658 --\n",
      "Reward: [-2.17604  0.13679  1.1159 ] total reward: -0.92335\n",
      "UEHitrate: 0.00627  edgeHitrate 0.12435 sumHitrate 0.13062  privacy: 1.61833\n",
      "\n",
      "--Time: Mon Sep 20 13:52:25 2021 Episode: 5   Index: 120000   Loss: 0.76187 --\n",
      "Reward: [-1.8421   0.13319  1.01519] total reward: -0.69372\n",
      "UEHitrate: 0.00581  edgeHitrate 0.11322 sumHitrate 0.11903  privacy: 1.46752\n",
      "\n",
      "--Time: Mon Sep 20 13:55:19 2021 Episode: 5   Index: 140000   Loss: 0.74842 --\n",
      "Reward: [-1.53451  0.13366  0.93554] total reward: -0.46531\n",
      "UEHitrate: 0.0057  edgeHitrate 0.10436 sumHitrate 0.11006  privacy: 1.36409\n",
      "\n",
      "--Time: Mon Sep 20 13:58:09 2021 Episode: 5   Index: 160000   Loss: 0.73599 --\n",
      "Reward: [-1.36171  0.13446  0.87508] total reward: -0.35217\n",
      "UEHitrate: 0.00551  edgeHitrate 0.09766 sumHitrate 0.10317  privacy: 1.27006\n",
      "\n",
      "--Time: Mon Sep 20 14:01:05 2021 Episode: 5   Index: 180000   Loss: 0.72379 --\n",
      "Reward: [-1.22904  0.13403  0.83285] total reward: -0.26217\n",
      "UEHitrate: 0.00532  edgeHitrate 0.09296 sumHitrate 0.09828  privacy: 1.16059\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 14:03:49 2021 Episode: 5   Index: 198174   Loss: 0.71341 --\n",
      "Reward: [-1.09524  0.13235  0.8027 ] total reward: -0.16018\n",
      "UEHitrate: 0.00523  edgeHitrate 0.08967 sumHitrate 0.0949  privacy: 1.03952\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 14:03:49 2021 Episode: 6   Index: 0   Loss: 21.85564 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 14:07:10 2021 Episode: 6   Index: 20000   Loss: 0.80912 --\n",
      "Reward: [-3.25896  0.11364  2.06585] total reward: -1.07947\n",
      "UEHitrate: 0.0058  edgeHitrate 0.23009 sumHitrate 0.23589  privacy: 2.7364\n",
      "\n",
      "--Time: Mon Sep 20 14:10:22 2021 Episode: 6   Index: 40000   Loss: 0.84393 --\n",
      "Reward: [-3.05428  0.11635  1.47686] total reward: -1.46107\n",
      "UEHitrate: 0.00535  edgeHitrate 0.16485 sumHitrate 0.1702  privacy: 2.18261\n",
      "\n",
      "--Time: Mon Sep 20 14:15:03 2021 Episode: 6   Index: 60000   Loss: 0.84849 --\n",
      "Reward: [-2.79651  0.11581  1.22128] total reward: -1.45942\n",
      "UEHitrate: 0.00503  edgeHitrate 0.13618 sumHitrate 0.14121  privacy: 1.93761\n",
      "\n",
      "--Time: Mon Sep 20 14:22:30 2021 Episode: 6   Index: 80000   Loss: 0.82849 --\n",
      "Reward: [-2.58486  0.10756  1.0495 ] total reward: -1.4278\n",
      "UEHitrate: 0.0046  edgeHitrate 0.11697 sumHitrate 0.12157  privacy: 1.77319\n",
      "\n",
      "--Time: Mon Sep 20 14:30:00 2021 Episode: 6   Index: 100000   Loss: 0.80645 --\n",
      "Reward: [-2.23706  0.1053   0.93959] total reward: -1.19217\n",
      "UEHitrate: 0.0044  edgeHitrate 0.10469 sumHitrate 0.10909  privacy: 1.62125\n",
      "\n",
      "--Time: Mon Sep 20 14:37:30 2021 Episode: 6   Index: 120000   Loss: 0.78533 --\n",
      "Reward: [-1.8816   0.10651  0.86287] total reward: -0.91223\n",
      "UEHitrate: 0.00431  edgeHitrate 0.09617 sumHitrate 0.10048  privacy: 1.48793\n",
      "\n",
      "--Time: Mon Sep 20 14:45:09 2021 Episode: 6   Index: 140000   Loss: 0.76361 --\n",
      "Reward: [-1.5751   0.11073  0.80479] total reward: -0.65958\n",
      "UEHitrate: 0.00438  edgeHitrate 0.08977 sumHitrate 0.09415  privacy: 1.38983\n",
      "\n",
      "--Time: Mon Sep 20 14:52:03 2021 Episode: 6   Index: 160000   Loss: 0.74314 --\n",
      "Reward: [-1.39387  0.11422  0.75751] total reward: -0.52213\n",
      "UEHitrate: 0.00436  edgeHitrate 0.08449 sumHitrate 0.08884  privacy: 1.27637\n",
      "\n",
      "--Time: Mon Sep 20 14:55:43 2021 Episode: 6   Index: 180000   Loss: 0.72551 --\n",
      "Reward: [-1.20843  0.11606  0.7278 ] total reward: -0.36457\n",
      "UEHitrate: 0.00426  edgeHitrate 0.08118 sumHitrate 0.08544  privacy: 1.13673\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 14:58:43 2021 Episode: 6   Index: 198174   Loss: 0.71182 --\n",
      "Reward: [-1.08675  0.1163   0.72341] total reward: -0.24705\n",
      "UEHitrate: 0.00427  edgeHitrate 0.08067 sumHitrate 0.08494  privacy: 1.01479\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 14:58:43 2021 Episode: 7   Index: 0   Loss: 11.36195 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 15:02:18 2021 Episode: 7   Index: 20000   Loss: 0.71256 --\n",
      "Reward: [-1.89168  0.0891   1.20639] total reward: -0.59619\n",
      "UEHitrate: 0.00325  edgeHitrate 0.13474 sumHitrate 0.13799  privacy: 2.88716\n",
      "\n",
      "--Time: Mon Sep 20 15:05:59 2021 Episode: 7   Index: 40000   Loss: 0.73101 --\n",
      "Reward: [-1.31857  0.10392  1.32342] total reward: 0.10877\n",
      "UEHitrate: 0.00422  edgeHitrate 0.14752 sumHitrate 0.15175  privacy: 2.73423\n",
      "\n",
      "--Time: Mon Sep 20 15:09:53 2021 Episode: 7   Index: 60000   Loss: 0.70938 --\n",
      "Reward: [-1.14272  0.11221  1.44733] total reward: 0.41682\n",
      "UEHitrate: 0.00443  edgeHitrate 0.16145 sumHitrate 0.16588  privacy: 2.66233\n",
      "\n",
      "--Time: Mon Sep 20 15:13:35 2021 Episode: 7   Index: 80000   Loss: 0.69867 --\n",
      "Reward: [-1.09865  0.10744  1.50219] total reward: 0.51098\n",
      "UEHitrate: 0.00454  edgeHitrate 0.16731 sumHitrate 0.17185  privacy: 2.52789\n",
      "\n",
      "--Time: Mon Sep 20 15:17:27 2021 Episode: 7   Index: 100000   Loss: 0.69658 --\n",
      "Reward: [-0.96901  0.10893  1.55176] total reward: 0.69169\n",
      "UEHitrate: 0.00464  edgeHitrate 0.17296 sumHitrate 0.1776  privacy: 2.33255\n",
      "\n",
      "--Time: Mon Sep 20 15:21:20 2021 Episode: 7   Index: 120000   Loss: 0.69724 --\n",
      "Reward: [-0.85531  0.11373  1.60739] total reward: 0.86581\n",
      "UEHitrate: 0.00463  edgeHitrate 0.17935 sumHitrate 0.18398  privacy: 2.10254\n",
      "\n",
      "--Time: Mon Sep 20 15:25:32 2021 Episode: 7   Index: 140000   Loss: 0.68391 --\n",
      "Reward: [-0.7082   0.12443  1.82499] total reward: 1.24122\n",
      "UEHitrate: 0.00539  edgeHitrate 0.20353 sumHitrate 0.20893  privacy: 1.92464\n",
      "\n",
      "--Time: Mon Sep 20 15:29:26 2021 Episode: 7   Index: 160000   Loss: 0.67128 --\n",
      "Reward: [-0.63249  0.12951  1.82541] total reward: 1.32243\n",
      "UEHitrate: 0.00549  edgeHitrate 0.20357 sumHitrate 0.20906  privacy: 1.81509\n",
      "\n",
      "--Time: Mon Sep 20 15:33:23 2021 Episode: 7   Index: 180000   Loss: 0.65568 --\n",
      "Reward: [-0.53161  0.13323  1.83039] total reward: 1.43201\n",
      "UEHitrate: 0.00558  edgeHitrate 0.20415 sumHitrate 0.20973  privacy: 1.75568\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 15:37:15 2021 Episode: 7   Index: 198174   Loss: 0.625 --\n",
      "Reward: [-0.48734  0.13369  1.78878] total reward: 1.43513\n",
      "UEHitrate: 0.00567  edgeHitrate 0.1995 sumHitrate 0.20517  privacy: 1.71498\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward_ep7_0920-15-37-15\n",
      "\n",
      "--Time: Mon Sep 20 15:37:15 2021 Episode: 8   Index: 0   Loss: 8.83485 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 15:41:05 2021 Episode: 8   Index: 20000   Loss: 0.45616 --\n",
      "Reward: [-5.05737  0.10204  1.0732 ] total reward: -3.88213\n",
      "UEHitrate: 0.00385  edgeHitrate 0.11964 sumHitrate 0.12349  privacy: 3.22828\n",
      "\n",
      "--Time: Mon Sep 20 15:45:03 2021 Episode: 8   Index: 40000   Loss: 0.50458 --\n",
      "Reward: [-3.78811  0.09937  0.8102 ] total reward: -2.87853\n",
      "UEHitrate: 0.0037  edgeHitrate 0.09025 sumHitrate 0.09395  privacy: 2.75734\n",
      "\n",
      "--Time: Mon Sep 20 15:49:05 2021 Episode: 8   Index: 60000   Loss: 0.53118 --\n",
      "Reward: [-3.15144  0.10248  0.72239] total reward: -2.32657\n",
      "UEHitrate: 0.00362  edgeHitrate 0.08048 sumHitrate 0.0841  privacy: 2.34318\n",
      "\n",
      "--Time: Mon Sep 20 15:53:18 2021 Episode: 8   Index: 80000   Loss: 0.54678 --\n",
      "Reward: [-2.80291  0.09729  0.67409] total reward: -2.03153\n",
      "UEHitrate: 0.00344  edgeHitrate 0.07509 sumHitrate 0.07852  privacy: 2.0432\n",
      "\n",
      "--Time: Mon Sep 20 15:57:33 2021 Episode: 8   Index: 100000   Loss: 0.55619 --\n",
      "Reward: [-2.4123   0.09847  0.66662] total reward: -1.64721\n",
      "UEHitrate: 0.00351  edgeHitrate 0.07432 sumHitrate 0.07783  privacy: 1.77643\n",
      "\n",
      "--Time: Mon Sep 20 16:02:05 2021 Episode: 8   Index: 120000   Loss: 0.55579 --\n",
      "Reward: [-2.03846  0.10138  0.65924] total reward: -1.27783\n",
      "UEHitrate: 0.00358  edgeHitrate 0.07357 sumHitrate 0.07715  privacy: 1.56205\n",
      "\n",
      "--Time: Mon Sep 20 16:06:36 2021 Episode: 8   Index: 140000   Loss: 0.56742 --\n",
      "Reward: [-1.72184  0.10724  0.64825] total reward: -0.96635\n",
      "UEHitrate: 0.00379  edgeHitrate 0.07244 sumHitrate 0.07623  privacy: 1.38007\n",
      "\n",
      "--Time: Mon Sep 20 16:10:54 2021 Episode: 8   Index: 160000   Loss: 0.57645 --\n",
      "Reward: [-1.515    0.11109  0.6503 ] total reward: -0.75361\n",
      "UEHitrate: 0.00388  edgeHitrate 0.07264 sumHitrate 0.07652  privacy: 1.21824\n",
      "\n",
      "--Time: Mon Sep 20 16:14:41 2021 Episode: 8   Index: 180000   Loss: 0.57906 --\n",
      "Reward: [-1.31202  0.11494  0.7279 ] total reward: -0.46918\n",
      "UEHitrate: 0.00401  edgeHitrate 0.08133 sumHitrate 0.08534  privacy: 1.07467\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 16:18:14 2021 Episode: 8   Index: 198174   Loss: 0.5754 --\n",
      "Reward: [-1.19525  0.1259   0.73739] total reward: -0.33195\n",
      "UEHitrate: 0.00499  edgeHitrate 0.08246 sumHitrate 0.08745  privacy: 0.96904\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 16:18:15 2021 Episode: 9   Index: 0   Loss: 9.46438 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 16:21:58 2021 Episode: 9   Index: 20000   Loss: 0.85493 --\n",
      "Reward: [-4.78572  0.0839   0.68082] total reward: -4.02101\n",
      "UEHitrate: 0.00225  edgeHitrate 0.0761 sumHitrate 0.07835  privacy: 3.3508\n",
      "\n",
      "--Time: Mon Sep 20 16:26:05 2021 Episode: 9   Index: 40000   Loss: 0.92453 --\n",
      "Reward: [-3.77227  0.09062  0.62638] total reward: -3.05526\n",
      "UEHitrate: 0.0028  edgeHitrate 0.0701 sumHitrate 0.0729  privacy: 2.72938\n",
      "\n",
      "--Time: Mon Sep 20 16:29:44 2021 Episode: 9   Index: 60000   Loss: 0.9301 --\n",
      "Reward: [-3.21522  0.09735  0.61649] total reward: -2.50139\n",
      "UEHitrate: 0.003  edgeHitrate 0.06898 sumHitrate 0.07198  privacy: 2.3191\n",
      "\n",
      "--Time: Mon Sep 20 16:34:04 2021 Episode: 9   Index: 80000   Loss: 0.9213 --\n",
      "Reward: [-2.86331  0.09986  0.60592] total reward: -2.15753\n",
      "UEHitrate: 0.00345  edgeHitrate 0.06774 sumHitrate 0.07119  privacy: 2.02654\n",
      "\n",
      "--Time: Mon Sep 20 16:37:39 2021 Episode: 9   Index: 100000   Loss: 0.84339 --\n",
      "Reward: [-2.29077  0.19498  0.80198] total reward: -1.29381\n",
      "UEHitrate: 0.01208  edgeHitrate 0.08977 sumHitrate 0.10185  privacy: 1.91347\n",
      "\n",
      "--Time: Mon Sep 20 16:41:20 2021 Episode: 9   Index: 120000   Loss: 0.77614 --\n",
      "Reward: [-2.01665  0.18909  0.88777] total reward: -0.93979\n",
      "UEHitrate: 0.01149  edgeHitrate 0.09926 sumHitrate 0.11075  privacy: 1.62705\n",
      "\n",
      "--Time: Mon Sep 20 16:44:36 2021 Episode: 9   Index: 140000   Loss: 0.73178 --\n",
      "Reward: [-1.72378  0.20945  0.98993] total reward: -0.5244\n",
      "UEHitrate: 0.01306  edgeHitrate 0.11079 sumHitrate 0.12386  privacy: 1.39018\n",
      "\n",
      "--Time: Mon Sep 20 16:48:26 2021 Episode: 9   Index: 160000   Loss: 0.68167 --\n",
      "Reward: [-1.5015   0.24534  1.06998] total reward: -0.18618\n",
      "UEHitrate: 0.01595  edgeHitrate 0.11962 sumHitrate 0.13557  privacy: 1.28999\n",
      "\n",
      "--Time: Mon Sep 20 16:52:24 2021 Episode: 9   Index: 180000   Loss: 0.64628 --\n",
      "Reward: [-1.31549  0.23346  1.03979] total reward: -0.04223\n",
      "UEHitrate: 0.01476  edgeHitrate 0.11619 sumHitrate 0.13094  privacy: 1.133\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 16:55:41 2021 Episode: 9   Index: 198174   Loss: 0.61421 --\n",
      "Reward: [-1.19656  0.22264  0.98658] total reward: 0.01267\n",
      "UEHitrate: 0.0138  edgeHitrate 0.11023 sumHitrate 0.12403  privacy: 1.09084\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "TARGET_UPDATE = 1\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = trainUIT.shape[0]*5\n",
    "agentStep = 0\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "        agentStep += 1\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 20000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+'ep{}_'.format(bestEpisode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "        print(bestPath)\n",
    "    print()\n",
    "    env = ENV(userNum,contentNum,latency)\n",
    "    UEs = {}\n",
    "    sumReward = sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Mon Sep 20 16:55:46 2021   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 16:56:32 2021   Index: 10000   Loss: 0.0 --\n",
      "Reward: [-5.48366  0.06589  0.60384] total reward: -4.81393\n",
      "UEHitrate: 0.0021  edgeHitrate 0.06749 sumHitrate 0.06959  privacy: 3.83595\n",
      "\n",
      "--Time: Mon Sep 20 16:57:16 2021   Index: 20000   Loss: 0.0 --\n",
      "Reward: [-4.86256  0.08015  0.63582] total reward: -4.1466\n",
      "UEHitrate: 0.0021  edgeHitrate 0.07095 sumHitrate 0.07305  privacy: 3.3345\n",
      "\n",
      "--Time: Mon Sep 20 16:58:10 2021   Index: 30000   Loss: 0.0 --\n",
      "Reward: [-4.2262   0.09073  0.59308] total reward: -3.54239\n",
      "UEHitrate: 0.0026  edgeHitrate 0.0663 sumHitrate 0.0689  privacy: 2.97243\n",
      "\n",
      "--Time: Mon Sep 20 16:59:05 2021   Index: 40000   Loss: 0.0 --\n",
      "Reward: [-3.68458  0.0952   0.82303] total reward: -2.76635\n",
      "UEHitrate: 0.00322  edgeHitrate 0.09185 sumHitrate 0.09507  privacy: 2.77774\n",
      "\n",
      "--Time: Mon Sep 20 17:00:04 2021   Index: 50000   Loss: 0.0 --\n",
      "Reward: [-2.97662  0.1083   1.34835] total reward: -1.51997\n",
      "UEHitrate: 0.0047  edgeHitrate 0.15024 sumHitrate 0.15494  privacy: 2.66236\n",
      "\n",
      "--Time: Mon Sep 20 17:01:01 2021   Index: 60000   Loss: 0.0 --\n",
      "Reward: [-2.53254  0.11715  1.75797] total reward: -0.65742\n",
      "UEHitrate: 0.00538  edgeHitrate 0.19585 sumHitrate 0.20123  privacy: 2.58154\n",
      "\n",
      "--Time: Mon Sep 20 17:01:58 2021   Index: 70000   Loss: 0.0 --\n",
      "Reward: [-2.19906  0.11807  2.07241] total reward: -0.00858\n",
      "UEHitrate: 0.00583  edgeHitrate 0.23063 sumHitrate 0.23645  privacy: 2.50596\n",
      "\n",
      "--Time: Mon Sep 20 17:02:57 2021   Index: 80000   Loss: 0.0 --\n",
      "Reward: [-1.9515   0.12297  2.37451] total reward: 0.54598\n",
      "UEHitrate: 0.00607  edgeHitrate 0.2644 sumHitrate 0.27047  privacy: 2.4339\n",
      "\n",
      "--Time: Mon Sep 20 17:04:02 2021   Index: 90000   Loss: 0.0 --\n",
      "Reward: [-1.75458  0.12774  2.57387] total reward: 0.94703\n",
      "UEHitrate: 0.00644  edgeHitrate 0.28671 sumHitrate 0.29315  privacy: 2.36836\n",
      "\n",
      "--Time: Mon Sep 20 17:04:54 2021   Index: 100000   Loss: 0.0 --\n",
      "Reward: [-1.58668  0.12809  2.71086] total reward: 1.25228\n",
      "UEHitrate: 0.00677  edgeHitrate 0.30182 sumHitrate 0.30859  privacy: 2.30683\n",
      "\n",
      "--Time: Mon Sep 20 17:05:49 2021   Index: 110000   Loss: 0.0 --\n",
      "Reward: [-1.43172  0.13082  2.78629] total reward: 1.48539\n",
      "UEHitrate: 0.00689  edgeHitrate 0.31024 sumHitrate 0.31713  privacy: 2.26529\n",
      "\n",
      "--Time: Mon Sep 20 17:06:37 2021   Index: 120000   Loss: 0.0 --\n",
      "Reward: [-1.30332  0.1324   2.80985] total reward: 1.63893\n",
      "UEHitrate: 0.00705  edgeHitrate 0.3128 sumHitrate 0.31985  privacy: 2.23962\n",
      "\n",
      "--Time: Mon Sep 20 17:07:28 2021   Index: 130000   Loss: 0.0 --\n",
      "Reward: [-1.20521  0.13616  2.80078] total reward: 1.73173\n",
      "UEHitrate: 0.00734  edgeHitrate 0.31166 sumHitrate 0.319  privacy: 2.21136\n",
      "\n",
      "--Time: Mon Sep 20 17:08:14 2021   Index: 140000   Loss: 0.0 --\n",
      "Reward: [-1.10721  0.13886  2.75874] total reward: 1.79039\n",
      "UEHitrate: 0.00737  edgeHitrate 0.30707 sumHitrate 0.31444  privacy: 2.18797\n",
      "\n",
      "--Time: Mon Sep 20 17:09:04 2021   Index: 150000   Loss: 0.0 --\n",
      "Reward: [-1.03502  0.13939  2.7259 ] total reward: 1.83027\n",
      "UEHitrate: 0.00744  edgeHitrate 0.3034 sumHitrate 0.31084  privacy: 2.17053\n",
      "\n",
      "--Time: Mon Sep 20 17:09:56 2021   Index: 160000   Loss: 0.0 --\n",
      "Reward: [-0.97051  0.14263  2.71   ] total reward: 1.88212\n",
      "UEHitrate: 0.00745  edgeHitrate 0.30163 sumHitrate 0.30908  privacy: 2.15014\n",
      "\n",
      "--Time: Mon Sep 20 17:10:46 2021   Index: 170000   Loss: 0.0 --\n",
      "Reward: [-0.91521  0.1425   2.66737] total reward: 1.89467\n",
      "UEHitrate: 0.00746  edgeHitrate 0.29686 sumHitrate 0.30432  privacy: 2.13218\n",
      "\n",
      "--Time: Mon Sep 20 17:11:43 2021   Index: 180000   Loss: 0.0 --\n",
      "Reward: [-0.86244  0.14321  2.61814] total reward: 1.89891\n",
      "UEHitrate: 0.00734  edgeHitrate 0.29144 sumHitrate 0.29878  privacy: 2.11075\n",
      "\n",
      "--Time: Mon Sep 20 17:12:37 2021   Index: 190000   Loss: 0.0 --\n",
      "Reward: [-0.81681  0.14321  2.57432] total reward: 1.90072\n",
      "UEHitrate: 0.00731  edgeHitrate 0.28661 sumHitrate 0.29392  privacy: 2.09595\n",
      "\n",
      "--Time: Mon Sep 20 17:13:33 2021   Index: 200000   Loss: 0.0 --\n",
      "Reward: [-0.77603  0.14299  2.53803] total reward: 1.90499\n",
      "UEHitrate: 0.00722  edgeHitrate 0.28257 sumHitrate 0.2898  privacy: 2.08144\n",
      "\n",
      "--Time: Mon Sep 20 17:14:31 2021   Index: 210000   Loss: 0.0 --\n",
      "Reward: [-0.73667  0.14349  2.49809] total reward: 1.90491\n",
      "UEHitrate: 0.00721  edgeHitrate 0.27807 sumHitrate 0.28528  privacy: 2.06399\n",
      "\n",
      "--Time: Mon Sep 20 17:15:30 2021   Index: 220000   Loss: 0.0 --\n",
      "Reward: [-0.70293  0.14379  2.47585] total reward: 1.9167\n",
      "UEHitrate: 0.00718  edgeHitrate 0.27564 sumHitrate 0.28282  privacy: 2.0494\n",
      "\n",
      "--Time: Mon Sep 20 17:16:29 2021   Index: 230000   Loss: 0.0 --\n",
      "Reward: [-0.67198  0.14352  2.44627] total reward: 1.91781\n",
      "UEHitrate: 0.00717  edgeHitrate 0.27231 sumHitrate 0.27947  privacy: 2.03482\n",
      "\n",
      "--Time: Mon Sep 20 17:17:34 2021   Index: 240000   Loss: 0.0 --\n",
      "Reward: [-0.64378  0.14486  2.4223 ] total reward: 1.92338\n",
      "UEHitrate: 0.00722  edgeHitrate 0.26963 sumHitrate 0.27685  privacy: 2.02284\n",
      "\n",
      "--Time: Mon Sep 20 17:18:34 2021   Index: 250000   Loss: 0.0 --\n",
      "Reward: [-0.61733  0.14495  2.4004 ] total reward: 1.92801\n",
      "UEHitrate: 0.00718  edgeHitrate 0.26722 sumHitrate 0.2744  privacy: 2.01197\n",
      "\n",
      "--Time: Mon Sep 20 17:19:27 2021   Index: 260000   Loss: 0.0 --\n",
      "Reward: [-0.59336  0.14466  2.38478] total reward: 1.93608\n",
      "UEHitrate: 0.0072  edgeHitrate 0.26545 sumHitrate 0.27265  privacy: 2.00202\n",
      "\n",
      "--Time: Mon Sep 20 17:20:21 2021   Index: 270000   Loss: 0.0 --\n",
      "Reward: [-0.57039  0.14522  2.36059] total reward: 1.93542\n",
      "UEHitrate: 0.00724  edgeHitrate 0.26274 sumHitrate 0.26998  privacy: 1.99091\n",
      "\n",
      "--Time: Mon Sep 20 17:21:19 2021   Index: 280000   Loss: 0.0 --\n",
      "Reward: [-0.55113  0.14531  2.33552] total reward: 1.9297\n",
      "UEHitrate: 0.0072  edgeHitrate 0.25994 sumHitrate 0.26714  privacy: 1.98072\n",
      "\n",
      "--Time: Mon Sep 20 17:22:21 2021   Index: 290000   Loss: 0.0 --\n",
      "Reward: [-0.53229  0.14466  2.30955] total reward: 1.92192\n",
      "UEHitrate: 0.00714  edgeHitrate 0.25706 sumHitrate 0.2642  privacy: 1.97226\n",
      "\n",
      "--Time: Mon Sep 20 17:23:15 2021   Index: 300000   Loss: 0.0 --\n",
      "Reward: [-0.5142   0.14347  2.27855] total reward: 1.90782\n",
      "UEHitrate: 0.00705  edgeHitrate 0.25362 sumHitrate 0.26067  privacy: 1.96574\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 17:23:19 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-0.51234  0.14324  2.27524] total reward: 1.90614\n",
      "UEHitrate: 0.00705  edgeHitrate 0.25324 sumHitrate 0.26029  privacy: 1.96511\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Mon Sep 20 17:23:19 2021 Episode: 0   Index: 0   Loss: 7.98768 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 17:25:31 2021 Episode: 0   Index: 20000   Loss: 0.31862 --\n",
      "Reward: [-4.59599  0.0901   0.79646] total reward: -3.70944\n",
      "UEHitrate: 0.00205  edgeHitrate 0.0892 sumHitrate 0.09125  privacy: 3.39814\n",
      "\n",
      "--Time: Mon Sep 20 17:27:38 2021 Episode: 0   Index: 40000   Loss: 0.333 --\n",
      "Reward: [-3.51379  0.10105  0.9272 ] total reward: -2.48554\n",
      "UEHitrate: 0.00335  edgeHitrate 0.10365 sumHitrate 0.107  privacy: 2.81475\n",
      "\n",
      "--Time: Mon Sep 20 17:29:43 2021 Episode: 0   Index: 60000   Loss: 0.31758 --\n",
      "Reward: [-2.85226  0.11405  1.17943] total reward: -1.55878\n",
      "UEHitrate: 0.00443  edgeHitrate 0.1317 sumHitrate 0.13613  privacy: 2.4606\n",
      "\n",
      "--Time: Mon Sep 20 17:31:51 2021 Episode: 0   Index: 80000   Loss: 0.30044 --\n",
      "Reward: [-2.42193  0.11396  1.31578] total reward: -0.99219\n",
      "UEHitrate: 0.00464  edgeHitrate 0.14709 sumHitrate 0.15172  privacy: 2.22235\n",
      "\n",
      "--Time: Mon Sep 20 17:34:03 2021 Episode: 0   Index: 100000   Loss: 0.28441 --\n",
      "Reward: [-2.05496  0.11487  1.37852] total reward: -0.56157\n",
      "UEHitrate: 0.00476  edgeHitrate 0.1539 sumHitrate 0.15866  privacy: 2.0074\n",
      "\n",
      "--Time: Mon Sep 20 17:36:00 2021 Episode: 0   Index: 120000   Loss: 0.27152 --\n",
      "Reward: [-1.71984  0.11772  1.38156] total reward: -0.22055\n",
      "UEHitrate: 0.00497  edgeHitrate 0.15417 sumHitrate 0.15915  privacy: 1.84061\n",
      "\n",
      "--Time: Mon Sep 20 17:38:02 2021 Episode: 0   Index: 140000   Loss: 0.2589 --\n",
      "Reward: [-1.45546  0.12287  1.33848] total reward: 0.00589\n",
      "UEHitrate: 0.00518  edgeHitrate 0.14938 sumHitrate 0.15456  privacy: 1.70073\n",
      "\n",
      "--Time: Mon Sep 20 17:39:58 2021 Episode: 0   Index: 160000   Loss: 0.24691 --\n",
      "Reward: [-1.27976  0.12571  1.29925] total reward: 0.14521\n",
      "UEHitrate: 0.00524  edgeHitrate 0.1449 sumHitrate 0.15014  privacy: 1.58209\n",
      "\n",
      "--Time: Mon Sep 20 17:41:50 2021 Episode: 0   Index: 180000   Loss: 0.23587 --\n",
      "Reward: [-1.11752  0.12741  1.24854] total reward: 0.25843\n",
      "UEHitrate: 0.00516  edgeHitrate 0.1393 sumHitrate 0.14446  privacy: 1.473\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 17:43:31 2021 Episode: 0   Index: 198174   Loss: 0.22719 --\n",
      "Reward: [-1.02014  0.12653  1.19385] total reward: 0.30025\n",
      "UEHitrate: 0.00515  edgeHitrate 0.13322 sumHitrate 0.13836  privacy: 1.38502\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward_ep0_0920-17-43-31\n",
      "\n",
      "--Time: Mon Sep 20 17:43:31 2021 Episode: 1   Index: 0   Loss: 10.64334 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 17:45:16 2021 Episode: 1   Index: 20000   Loss: 0.31917 --\n",
      "Reward: [-4.62438  0.08035  0.75821] total reward: -3.78582\n",
      "UEHitrate: 0.0028  edgeHitrate 0.08425 sumHitrate 0.08705  privacy: 3.38882\n",
      "\n",
      "--Time: Mon Sep 20 17:47:02 2021 Episode: 1   Index: 40000   Loss: 0.33141 --\n",
      "Reward: [-3.53817  0.10382  1.01112] total reward: -2.42322\n",
      "UEHitrate: 0.00407  edgeHitrate 0.11295 sumHitrate 0.11702  privacy: 2.81331\n",
      "\n",
      "--Time: Mon Sep 20 17:48:55 2021 Episode: 1   Index: 60000   Loss: 0.31052 --\n",
      "Reward: [-2.80875  0.11985  1.46953] total reward: -1.21937\n",
      "UEHitrate: 0.00548  edgeHitrate 0.16401 sumHitrate 0.1695  privacy: 2.48065\n",
      "\n",
      "--Time: Mon Sep 20 17:50:44 2021 Episode: 1   Index: 80000   Loss: 0.29036 --\n",
      "Reward: [-2.34353  0.12047  1.65272] total reward: -0.57034\n",
      "UEHitrate: 0.00556  edgeHitrate 0.18431 sumHitrate 0.18987  privacy: 2.26449\n",
      "\n",
      "--Time: Mon Sep 20 17:52:58 2021 Episode: 1   Index: 100000   Loss: 0.27434 --\n",
      "Reward: [-1.97563  0.12363  1.73293] total reward: -0.11907\n",
      "UEHitrate: 0.00593  edgeHitrate 0.19313 sumHitrate 0.19906  privacy: 2.06601\n",
      "\n",
      "--Time: Mon Sep 20 17:54:54 2021 Episode: 1   Index: 120000   Loss: 0.2616 --\n",
      "Reward: [-1.65085  0.12667  1.73271] total reward: 0.20853\n",
      "UEHitrate: 0.00602  edgeHitrate 0.19313 sumHitrate 0.19915  privacy: 1.91391\n",
      "\n",
      "--Time: Mon Sep 20 17:56:48 2021 Episode: 1   Index: 140000   Loss: 0.24987 --\n",
      "Reward: [-1.39546  0.13104  1.66274] total reward: 0.39832\n",
      "UEHitrate: 0.00609  edgeHitrate 0.18534 sumHitrate 0.19143  privacy: 1.78549\n",
      "\n",
      "--Time: Mon Sep 20 17:58:42 2021 Episode: 1   Index: 160000   Loss: 0.23852 --\n",
      "Reward: [-1.2241   0.13456  1.60733] total reward: 0.51779\n",
      "UEHitrate: 0.00606  edgeHitrate 0.17917 sumHitrate 0.18522  privacy: 1.67614\n",
      "\n",
      "--Time: Mon Sep 20 18:00:43 2021 Episode: 1   Index: 180000   Loss: 0.22771 --\n",
      "Reward: [-1.07325  0.1366   1.55044] total reward: 0.6138\n",
      "UEHitrate: 0.00594  edgeHitrate 0.17288 sumHitrate 0.17882  privacy: 1.57359\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 18:02:42 2021 Episode: 1   Index: 198174   Loss: 0.21919 --\n",
      "Reward: [-0.97811  0.13659  1.49759] total reward: 0.65606\n",
      "UEHitrate: 0.00593  edgeHitrate 0.167 sumHitrate 0.17294  privacy: 1.49133\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward_ep1_0920-18-02-42\n",
      "\n",
      "--Time: Mon Sep 20 18:02:42 2021 Episode: 2   Index: 0   Loss: 7.17779 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 18:04:56 2021 Episode: 2   Index: 20000   Loss: 0.3203 --\n",
      "Reward: [-4.6555   0.0879   0.75326] total reward: -3.81435\n",
      "UEHitrate: 0.0026  edgeHitrate 0.08375 sumHitrate 0.08635  privacy: 3.3785\n",
      "\n",
      "--Time: Mon Sep 20 18:06:53 2021 Episode: 2   Index: 40000   Loss: 0.33343 --\n",
      "Reward: [-3.56975  0.1026   1.01   ] total reward: -2.45715\n",
      "UEHitrate: 0.004  edgeHitrate 0.11217 sumHitrate 0.11617  privacy: 2.80676\n",
      "\n",
      "--Time: Mon Sep 20 18:09:14 2021 Episode: 2   Index: 60000   Loss: 0.30715 --\n",
      "Reward: [-2.78275  0.11771  1.61787] total reward: -1.04716\n",
      "UEHitrate: 0.00543  edgeHitrate 0.17973 sumHitrate 0.18516  privacy: 2.49167\n",
      "\n",
      "--Time: Mon Sep 20 18:11:40 2021 Episode: 2   Index: 80000   Loss: 0.28437 --\n",
      "Reward: [-2.2877   0.11949  1.86669] total reward: -0.30153\n",
      "UEHitrate: 0.00569  edgeHitrate 0.20763 sumHitrate 0.21332  privacy: 2.29486\n",
      "\n",
      "--Time: Mon Sep 20 18:14:10 2021 Episode: 2   Index: 100000   Loss: 0.26759 --\n",
      "Reward: [-1.91759  0.12257  1.95703] total reward: 0.16201\n",
      "UEHitrate: 0.00605  edgeHitrate 0.2178 sumHitrate 0.22385  privacy: 2.11013\n",
      "\n",
      "--Time: Mon Sep 20 18:16:46 2021 Episode: 2   Index: 120000   Loss: 0.2549 --\n",
      "Reward: [-1.60072  0.12611  1.93783] total reward: 0.46322\n",
      "UEHitrate: 0.00613  edgeHitrate 0.21574 sumHitrate 0.22187  privacy: 1.97108\n",
      "\n",
      "--Time: Mon Sep 20 18:19:17 2021 Episode: 2   Index: 140000   Loss: 0.24387 --\n",
      "Reward: [-1.35347  0.13078  1.84833] total reward: 0.62563\n",
      "UEHitrate: 0.00618  edgeHitrate 0.20577 sumHitrate 0.21195  privacy: 1.85488\n",
      "\n",
      "--Time: Mon Sep 20 18:21:56 2021 Episode: 2   Index: 160000   Loss: 0.23353 --\n",
      "Reward: [-1.18921  0.13417  1.78761] total reward: 0.73258\n",
      "UEHitrate: 0.00614  edgeHitrate 0.19902 sumHitrate 0.20515  privacy: 1.75338\n",
      "\n",
      "--Time: Mon Sep 20 18:24:24 2021 Episode: 2   Index: 180000   Loss: 0.22318 --\n",
      "Reward: [-1.04549  0.13594  1.72389] total reward: 0.81433\n",
      "UEHitrate: 0.00602  edgeHitrate 0.19199 sumHitrate 0.198  privacy: 1.65955\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 18:26:45 2021 Episode: 2   Index: 198174   Loss: 0.21518 --\n",
      "Reward: [-0.95123  0.13587  1.67675] total reward: 0.86139\n",
      "UEHitrate: 0.006  edgeHitrate 0.18678 sumHitrate 0.19278  privacy: 1.58302\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward_ep2_0920-18-26-45\n",
      "\n",
      "--Time: Mon Sep 20 18:26:45 2021 Episode: 3   Index: 0   Loss: 7.16334 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 18:29:05 2021 Episode: 3   Index: 20000   Loss: 0.32147 --\n",
      "Reward: [-4.68853  0.07905  0.72086] total reward: -3.88862\n",
      "UEHitrate: 0.0022  edgeHitrate 0.08 sumHitrate 0.0822  privacy: 3.37242\n",
      "\n",
      "--Time: Mon Sep 20 18:31:11 2021 Episode: 3   Index: 40000   Loss: 0.33792 --\n",
      "Reward: [-3.59768  0.0989   1.0037 ] total reward: -2.49508\n",
      "UEHitrate: 0.00387  edgeHitrate 0.11155 sumHitrate 0.11542  privacy: 2.80083\n",
      "\n",
      "--Time: Mon Sep 20 18:33:04 2021 Episode: 3   Index: 60000   Loss: 0.30748 --\n",
      "Reward: [-2.76409  0.11711  1.68642] total reward: -0.96056\n",
      "UEHitrate: 0.00558  edgeHitrate 0.18746 sumHitrate 0.19305  privacy: 2.50073\n",
      "\n",
      "--Time: Mon Sep 20 18:34:54 2021 Episode: 3   Index: 80000   Loss: 0.28103 --\n",
      "Reward: [-2.24398  0.11705  1.98673] total reward: -0.1402\n",
      "UEHitrate: 0.00595  edgeHitrate 0.22075 sumHitrate 0.2267  privacy: 2.31752\n",
      "\n",
      "--Time: Mon Sep 20 18:36:49 2021 Episode: 3   Index: 100000   Loss: 0.26369 --\n",
      "Reward: [-1.86817  0.11857  2.1004 ] total reward: 0.3508\n",
      "UEHitrate: 0.00614  edgeHitrate 0.23347 sumHitrate 0.23961  privacy: 2.1481\n",
      "\n",
      "--Time: Mon Sep 20 18:38:44 2021 Episode: 3   Index: 120000   Loss: 0.25054 --\n",
      "Reward: [-1.55972  0.12239  2.08513] total reward: 0.6478\n",
      "UEHitrate: 0.00637  edgeHitrate 0.23184 sumHitrate 0.23821  privacy: 2.02005\n",
      "\n",
      "--Time: Mon Sep 20 18:40:36 2021 Episode: 3   Index: 140000   Loss: 0.23916 --\n",
      "Reward: [-1.3184   0.12808  1.99355] total reward: 0.80324\n",
      "UEHitrate: 0.0064  edgeHitrate 0.22184 sumHitrate 0.22824  privacy: 1.91279\n",
      "\n",
      "--Time: Mon Sep 20 18:42:31 2021 Episode: 3   Index: 160000   Loss: 0.22939 --\n",
      "Reward: [-1.15555  0.13286  1.92109] total reward: 0.89841\n",
      "UEHitrate: 0.00635  edgeHitrate 0.21393 sumHitrate 0.22028  privacy: 1.82109\n",
      "\n",
      "--Time: Mon Sep 20 18:44:23 2021 Episode: 3   Index: 180000   Loss: 0.2201 --\n",
      "Reward: [-1.0187   0.13451  1.84899] total reward: 0.9648\n",
      "UEHitrate: 0.00631  edgeHitrate 0.2059 sumHitrate 0.21221  privacy: 1.73291\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 18:46:05 2021 Episode: 3   Index: 198174   Loss: 0.21237 --\n",
      "Reward: [-0.92571  0.13439  1.80186] total reward: 1.01053\n",
      "UEHitrate: 0.00626  edgeHitrate 0.20069 sumHitrate 0.20695  privacy: 1.66225\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward_ep3_0920-18-46-05\n",
      "\n",
      "--Time: Mon Sep 20 18:46:06 2021 Episode: 4   Index: 0   Loss: 7.04201 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 18:47:53 2021 Episode: 4   Index: 20000   Loss: 0.32111 --\n",
      "Reward: [-4.70872  0.08335  0.69477] total reward: -3.93061\n",
      "UEHitrate: 0.0022  edgeHitrate 0.07765 sumHitrate 0.07985  privacy: 3.36661\n",
      "\n",
      "--Time: Mon Sep 20 18:49:38 2021 Episode: 4   Index: 40000   Loss: 0.33713 --\n",
      "Reward: [-3.62702  0.099    0.96388] total reward: -2.56415\n",
      "UEHitrate: 0.0036  edgeHitrate 0.10737 sumHitrate 0.11097  privacy: 2.79385\n",
      "\n",
      "--Time: Mon Sep 20 18:51:23 2021 Episode: 4   Index: 60000   Loss: 0.30442 --\n",
      "Reward: [-2.75041  0.11973  1.69212] total reward: -0.93856\n",
      "UEHitrate: 0.00508  edgeHitrate 0.18883 sumHitrate 0.19391  privacy: 2.50699\n",
      "\n",
      "--Time: Mon Sep 20 18:53:11 2021 Episode: 4   Index: 80000   Loss: 0.27716 --\n",
      "Reward: [-2.20809  0.12112  2.04016] total reward: -0.04681\n",
      "UEHitrate: 0.00572  edgeHitrate 0.22732 sumHitrate 0.23305  privacy: 2.33591\n",
      "\n",
      "--Time: Mon Sep 20 18:54:56 2021 Episode: 4   Index: 100000   Loss: 0.25849 --\n",
      "Reward: [-1.8306   0.12235  2.1733 ] total reward: 0.46505\n",
      "UEHitrate: 0.00641  edgeHitrate 0.24176 sumHitrate 0.24817  privacy: 2.17682\n",
      "\n",
      "--Time: Mon Sep 20 18:56:41 2021 Episode: 4   Index: 120000   Loss: 0.24537 --\n",
      "Reward: [-1.52528  0.12601  2.18016] total reward: 0.78089\n",
      "UEHitrate: 0.0065  edgeHitrate 0.24275 sumHitrate 0.24925  privacy: 2.05929\n",
      "\n",
      "--Time: Mon Sep 20 18:58:23 2021 Episode: 4   Index: 140000   Loss: 0.23466 --\n",
      "Reward: [-1.28955  0.13085  2.09229] total reward: 0.93359\n",
      "UEHitrate: 0.00648  edgeHitrate 0.23313 sumHitrate 0.23961  privacy: 1.96036\n",
      "\n",
      "--Time: Mon Sep 20 19:00:30 2021 Episode: 4   Index: 160000   Loss: 0.22514 --\n",
      "Reward: [-1.1306   0.13419  2.01942] total reward: 1.02302\n",
      "UEHitrate: 0.00648  edgeHitrate 0.22507 sumHitrate 0.23155  privacy: 1.87656\n",
      "\n",
      "--Time: Mon Sep 20 19:02:27 2021 Episode: 4   Index: 180000   Loss: 0.21655 --\n",
      "Reward: [-0.99732  0.13538  1.94189] total reward: 1.07994\n",
      "UEHitrate: 0.00642  edgeHitrate 0.21637 sumHitrate 0.22279  privacy: 1.79482\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 19:04:04 2021 Episode: 4   Index: 198174   Loss: 0.20949 --\n",
      "Reward: [-0.90715  0.13556  1.89033] total reward: 1.11874\n",
      "UEHitrate: 0.00639  edgeHitrate 0.21066 sumHitrate 0.21705  privacy: 1.7295\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h1_reward_ep4_0920-19-04-04\n",
      "\n",
      "--Time: Mon Sep 20 19:04:04 2021 Episode: 5   Index: 0   Loss: 7.28443 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 19:05:43 2021 Episode: 5   Index: 20000   Loss: 0.32558 --\n",
      "Reward: [-4.73168  0.0879   0.68127] total reward: -3.96251\n",
      "UEHitrate: 0.00215  edgeHitrate 0.0762 sumHitrate 0.07835  privacy: 3.36268\n",
      "\n",
      "--Time: Mon Sep 20 19:07:21 2021 Episode: 5   Index: 40000   Loss: 0.33767 --\n",
      "Reward: [-3.64414  0.09832  0.94948] total reward: -2.59634\n",
      "UEHitrate: 0.00352  edgeHitrate 0.10592 sumHitrate 0.10945  privacy: 2.79114\n",
      "\n",
      "--Time: Mon Sep 20 19:09:02 2021 Episode: 5   Index: 60000   Loss: 0.30155 --\n",
      "Reward: [-2.72951  0.1146   1.72977] total reward: -0.88515\n",
      "UEHitrate: 0.00498  edgeHitrate 0.19263 sumHitrate 0.19761  privacy: 2.51558\n",
      "\n",
      "--Time: Mon Sep 20 19:10:45 2021 Episode: 5   Index: 80000   Loss: 0.27284 --\n",
      "Reward: [-2.17784  0.11935  2.11149] total reward: 0.05299\n",
      "UEHitrate: 0.00536  edgeHitrate 0.2353 sumHitrate 0.24066  privacy: 2.35197\n",
      "\n",
      "--Time: Mon Sep 20 19:12:28 2021 Episode: 5   Index: 100000   Loss: 0.2531 --\n",
      "Reward: [-1.79829  0.12279  2.27167] total reward: 0.59617\n",
      "UEHitrate: 0.00584  edgeHitrate 0.25299 sumHitrate 0.25883  privacy: 2.20061\n",
      "\n",
      "--Time: Mon Sep 20 19:14:14 2021 Episode: 5   Index: 120000   Loss: 0.24054 --\n",
      "Reward: [-1.49787  0.12652  2.28073] total reward: 0.90938\n",
      "UEHitrate: 0.00606  edgeHitrate 0.25397 sumHitrate 0.26003  privacy: 2.09062\n",
      "\n",
      "--Time: Mon Sep 20 19:16:04 2021 Episode: 5   Index: 140000   Loss: 0.23037 --\n",
      "Reward: [-1.26583  0.13182  2.18666] total reward: 1.05266\n",
      "UEHitrate: 0.00614  edgeHitrate 0.24366 sumHitrate 0.2498  privacy: 1.99974\n",
      "\n",
      "--Time: Mon Sep 20 19:17:57 2021 Episode: 5   Index: 160000   Loss: 0.22142 --\n",
      "Reward: [-1.11062  0.13495  2.10908] total reward: 1.13341\n",
      "UEHitrate: 0.00616  edgeHitrate 0.23508 sumHitrate 0.24124  privacy: 1.92379\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d0bd8baf2454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0muit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0medgeHit\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0magentStep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0msumReward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ab6de8059248>\u001b[0m in \u001b[0;36mselectAction\u001b[0;34m(self, env, uit, QNetwork, train, memory)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatusFeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatusEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetReward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastAction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ab6de8059248>\u001b[0m in \u001b[0;36mgetReward\u001b[0;34m(self, lastru, lastp, ru, p, i, action, S, l, e, v)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#return  self.Rh.sum()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRo\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mselectAction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQNetwork\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "TARGET_UPDATE = 1\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = trainUIT.shape[0]*5\n",
    "agentStep = 0\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "        agentStep += 1\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        if index % 64 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 10000 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 20000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+'ep{}_'.format(bestEpisode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "        print(bestPath)\n",
    "    print()\n",
    "    env = ENV(userNum,contentNum,latency)\n",
    "    UEs = {}\n",
    "    sumReward = sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
