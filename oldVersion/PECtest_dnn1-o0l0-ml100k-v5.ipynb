{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "#v2 增加神经网络输出\n",
    "#v3 修改reward为直接变化量\n",
    "#v4 删除e,S特征 \n",
    "#v5 固定贪婪算法阈值0.2\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "data_path = '/home/ubuntu/data/dataset/ml-100k/'\n",
    "UIT = pd.read_csv(data_path + 'ml_header.trace')\n",
    "\n",
    "trainUIT = UIT[UIT['day']<18]\n",
    "validUIT = UIT[UIT['day']<24]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT.shape,validUIT.shape\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_o0l0_v5_'\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0,\"betal\":0}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 15\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#GAMMA = 0.99\n",
    "\n",
    "num_episodes = 20\n",
    "#EPS_START = 0.99\n",
    "#EPS_END = 0.01\n",
    "#EPS_DECAY = trainUIT.shape[0]*3\n",
    "#agentStep = 0\n",
    "eps_threshold = 0.2\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    " \n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        #statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        statusFeature = torch.zeros(size=(3,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        #statusFeature[3] = self.e\n",
    "        #statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + torch.exp( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = self.ALPHAh * ( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "        if self.W[-1] not in np.argwhere(self.lastAction.numpy()): #local cache miss\n",
    "            sample = random.random()\n",
    "            #eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            \n",
    "            if  not train or (train and sample > eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                with torch.no_grad():\n",
    "                    Q_value = QNetwork(self.statusFeature)\n",
    "                    actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "                    #actionIndex = list(Q_value.squeeze(1).argsort(descending=True)[0:self.Bu])\n",
    "                QNetwork.train()\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "\n",
    "            self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "            self.action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                self.action[index] = 1\n",
    "            env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "        else:\n",
    "            self.action = self.lastAction # keep the cache and no request the new video\n",
    "\n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "\n",
    "        if train:\n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                        lastAction.to(device), \n",
    "                        self.statusFeature,\n",
    "                        torch.tensor([self.reward.float()]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "                        \n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    '''\n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    '''\n",
    "    #next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    #expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "eps_threshold = 0.2\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        #agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 00:52:37 2021 Episode: 0   Index: 65335   Loss: 0.10838 --\n",
      "Reward: [-2.22255  0.       0.     ] total reward: -2.22255\n",
      "UEHitrate: 0.01387  edgeHitrate 0.41385 sumHitrate 0.42771  privacy: 1.51748\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 00:55:04 2021 Episode: 0   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.20668  0.       0.     ] total reward: -3.20668\n",
      "UEHitrate: 0.00693  edgeHitrate 0.30209 sumHitrate 0.30902  privacy: 0.78546\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep0_1011-00-55-04\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 00:57:18 2021 Episode: 1   Index: 65335   Loss: 0.13592 --\n",
      "Reward: [-3.23302  0.       0.     ] total reward: -3.23302\n",
      "UEHitrate: 0.00877  edgeHitrate 0.28147 sumHitrate 0.29024  privacy: 1.36654\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 00:59:48 2021 Episode: 1   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.22848  0.       0.     ] total reward: -3.22848\n",
      "UEHitrate: 0.01251  edgeHitrate 0.31651 sumHitrate 0.32902  privacy: 0.95189\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:02:01 2021 Episode: 2   Index: 65335   Loss: 0.13499 --\n",
      "Reward: [-3.27792  0.       0.     ] total reward: -3.27792\n",
      "UEHitrate: 0.00802  edgeHitrate 0.30864 sumHitrate 0.31666  privacy: 1.13847\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:04:29 2021 Episode: 2   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.28602  0.       0.     ] total reward: -3.28602\n",
      "UEHitrate: 0.00766  edgeHitrate 0.30435 sumHitrate 0.31201  privacy: 0.73786\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:06:42 2021 Episode: 3   Index: 65335   Loss: 0.13815 --\n",
      "Reward: [-3.24857  0.       0.     ] total reward: -3.24857\n",
      "UEHitrate: 0.00931  edgeHitrate 0.31814 sumHitrate 0.32745  privacy: 1.25695\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:09:10 2021 Episode: 3   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.28922  0.       0.     ] total reward: -3.28922\n",
      "UEHitrate: 0.00793  edgeHitrate 0.30638 sumHitrate 0.31431  privacy: 0.73768\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:11:27 2021 Episode: 4   Index: 65335   Loss: 0.14045 --\n",
      "Reward: [-3.2302  0.      0.    ] total reward: -3.2302\n",
      "UEHitrate: 0.01154  edgeHitrate 0.3235 sumHitrate 0.33504  privacy: 1.26723\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:13:56 2021 Episode: 4   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.2878  0.      0.    ] total reward: -3.2878\n",
      "UEHitrate: 0.00689  edgeHitrate 0.30142 sumHitrate 0.30831  privacy: 0.73652\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:16:10 2021 Episode: 5   Index: 65335   Loss: 0.14038 --\n",
      "Reward: [-3.28261  0.       0.     ] total reward: -3.28261\n",
      "UEHitrate: 0.00796  edgeHitrate 0.31404 sumHitrate 0.322  privacy: 1.1444\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:18:44 2021 Episode: 5   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.28665  0.       0.     ] total reward: -3.28665\n",
      "UEHitrate: 0.00789  edgeHitrate 0.30234 sumHitrate 0.31023  privacy: 0.73739\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:20:57 2021 Episode: 6   Index: 65335   Loss: 0.14 --\n",
      "Reward: [-3.26839  0.       0.     ] total reward: -3.26839\n",
      "UEHitrate: 0.00845  edgeHitrate 0.29537 sumHitrate 0.30381  privacy: 1.37071\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:23:28 2021 Episode: 6   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.21921  0.       0.     ] total reward: -3.21921\n",
      "UEHitrate: 0.0096  edgeHitrate 0.32661 sumHitrate 0.33622  privacy: 0.98487\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:25:39 2021 Episode: 7   Index: 65335   Loss: 0.13442 --\n",
      "Reward: [-3.24318  0.       0.     ] total reward: -3.24318\n",
      "UEHitrate: 0.00762  edgeHitrate 0.30542 sumHitrate 0.31304  privacy: 1.43818\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:28:04 2021 Episode: 7   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.10075  0.       0.     ] total reward: -3.10075\n",
      "UEHitrate: 0.00711  edgeHitrate 0.35591 sumHitrate 0.36302  privacy: 1.23467\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep7_1011-01-28-04\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:30:20 2021 Episode: 8   Index: 65335   Loss: 0.13426 --\n",
      "Reward: [-3.29764  0.       0.     ] total reward: -3.29764\n",
      "UEHitrate: 0.00718  edgeHitrate 0.29393 sumHitrate 0.30111  privacy: 1.3696\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:33:16 2021 Episode: 8   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.24569  0.       0.     ] total reward: -3.24569\n",
      "UEHitrate: 0.00604  edgeHitrate 0.31172 sumHitrate 0.31776  privacy: 0.99102\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:35:56 2021 Episode: 9   Index: 65335   Loss: 0.13536 --\n",
      "Reward: [-3.25949  0.       0.     ] total reward: -3.25949\n",
      "UEHitrate: 0.00612  edgeHitrate 0.29673 sumHitrate 0.30285  privacy: 1.37016\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:38:55 2021 Episode: 9   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.31461  0.       0.     ] total reward: -3.31461\n",
      "UEHitrate: 0.00721  edgeHitrate 0.29984 sumHitrate 0.30705  privacy: 0.99532\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:41:18 2021 Episode: 10   Index: 65335   Loss: 0.14691 --\n",
      "Reward: [-3.25821  0.       0.     ] total reward: -3.25821\n",
      "UEHitrate: 0.00673  edgeHitrate 0.29847 sumHitrate 0.30521  privacy: 1.36966\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:43:46 2021 Episode: 10   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.25374  0.       0.     ] total reward: -3.25374\n",
      "UEHitrate: 0.0071  edgeHitrate 0.31468 sumHitrate 0.32178  privacy: 0.98321\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:46:00 2021 Episode: 11   Index: 65335   Loss: 0.13829 --\n",
      "Reward: [-3.22604  0.       0.     ] total reward: -3.22604\n",
      "UEHitrate: 0.00621  edgeHitrate 0.2862 sumHitrate 0.29241  privacy: 1.36802\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:48:32 2021 Episode: 11   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.21434  0.       0.     ] total reward: -3.21434\n",
      "UEHitrate: 0.00975  edgeHitrate 0.29378 sumHitrate 0.30354  privacy: 0.98986\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:50:44 2021 Episode: 12   Index: 65335   Loss: 0.13643 --\n",
      "Reward: [-3.2211  0.      0.    ] total reward: -3.2211\n",
      "UEHitrate: 0.00602  edgeHitrate 0.28825 sumHitrate 0.29426  privacy: 1.36614\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:53:14 2021 Episode: 12   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.21107  0.       0.     ] total reward: -3.21107\n",
      "UEHitrate: 0.01102  edgeHitrate 0.28996 sumHitrate 0.30098  privacy: 0.98831\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 01:55:29 2021 Episode: 13   Index: 65335   Loss: 0.14235 --\n",
      "Reward: [-3.22964  0.       0.     ] total reward: -3.22964\n",
      "UEHitrate: 0.00652  edgeHitrate 0.28676 sumHitrate 0.29328  privacy: 1.36559\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 01:58:01 2021 Episode: 13   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.20748  0.       0.     ] total reward: -3.20748\n",
      "UEHitrate: 0.00987  edgeHitrate 0.2915 sumHitrate 0.30138  privacy: 0.98644\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:00:17 2021 Episode: 14   Index: 65335   Loss: 0.14401 --\n",
      "Reward: [-3.22532  0.       0.     ] total reward: -3.22532\n",
      "UEHitrate: 0.00661  edgeHitrate 0.28409 sumHitrate 0.2907  privacy: 1.36689\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:02:47 2021 Episode: 14   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.20548  0.       0.     ] total reward: -3.20548\n",
      "UEHitrate: 0.01068  edgeHitrate 0.2828 sumHitrate 0.29348  privacy: 0.98425\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:04:59 2021 Episode: 15   Index: 65335   Loss: 0.14515 --\n",
      "Reward: [-3.22569  0.       0.     ] total reward: -3.22569\n",
      "UEHitrate: 0.0079  edgeHitrate 0.28236 sumHitrate 0.29025  privacy: 1.36615\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:07:27 2021 Episode: 15   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.20574  0.       0.     ] total reward: -3.20574\n",
      "UEHitrate: 0.01148  edgeHitrate 0.29048 sumHitrate 0.30196  privacy: 0.98335\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:09:52 2021 Episode: 16   Index: 65335   Loss: 0.13875 --\n",
      "Reward: [-3.22259  0.       0.     ] total reward: -3.22259\n",
      "UEHitrate: 0.00857  edgeHitrate 0.28124 sumHitrate 0.28981  privacy: 1.36755\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:12:19 2021 Episode: 16   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.20682  0.       0.     ] total reward: -3.20682\n",
      "UEHitrate: 0.01  edgeHitrate 0.28944 sumHitrate 0.29944  privacy: 0.98373\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:14:34 2021 Episode: 17   Index: 65335   Loss: 0.13599 --\n",
      "Reward: [-3.22298  0.       0.     ] total reward: -3.22298\n",
      "UEHitrate: 0.00735  edgeHitrate 0.27025 sumHitrate 0.2776  privacy: 1.36592\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:17:04 2021 Episode: 17   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.20495  0.       0.     ] total reward: -3.20495\n",
      "UEHitrate: 0.00992  edgeHitrate 0.28668 sumHitrate 0.29661  privacy: 0.97689\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:19:18 2021 Episode: 18   Index: 65335   Loss: 0.13432 --\n",
      "Reward: [-3.22827  0.       0.     ] total reward: -3.22827\n",
      "UEHitrate: 0.00788  edgeHitrate 0.27323 sumHitrate 0.28112  privacy: 1.36648\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:21:49 2021 Episode: 18   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.20411  0.       0.     ] total reward: -3.20411\n",
      "UEHitrate: 0.00975  edgeHitrate 0.28806 sumHitrate 0.29782  privacy: 0.97783\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:24:07 2021 Episode: 19   Index: 65335   Loss: 0.13358 --\n",
      "Reward: [-3.22205  0.       0.     ] total reward: -3.22205\n",
      "UEHitrate: 0.0083  edgeHitrate 0.26993 sumHitrate 0.27822  privacy: 1.36771\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:26:45 2021 Episode: 19   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.20483  0.       0.     ] total reward: -3.20483\n",
      "UEHitrate: 0.00947  edgeHitrate 0.28703 sumHitrate 0.2965  privacy: 0.97455\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#test\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Mon Oct 11 02:27:03 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-5.44128  0.       0.     ] total reward: -5.44128\n",
      "UEHitrate: 0.0077  edgeHitrate 0.2742 sumHitrate 0.2819  privacy: 6.62953\n",
      "\n",
      "--Time: Mon Oct 11 02:27:21 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-4.95775  0.       0.     ] total reward: -4.95775\n",
      "UEHitrate: 0.0082  edgeHitrate 0.2885 sumHitrate 0.2967  privacy: 4.38349\n",
      "\n",
      "--Time: Mon Oct 11 02:27:39 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-3.73797  0.       0.     ] total reward: -3.73797\n",
      "UEHitrate: 0.00737  edgeHitrate 0.30837 sumHitrate 0.31573  privacy: 3.06879\n",
      "\n",
      "--Time: Mon Oct 11 02:27:58 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-3.55144  0.       0.     ] total reward: -3.55144\n",
      "UEHitrate: 0.00692  edgeHitrate 0.30345 sumHitrate 0.31038  privacy: 2.38948\n",
      "\n",
      "--Time: Mon Oct 11 02:28:17 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-3.27824  0.       0.     ] total reward: -3.27824\n",
      "UEHitrate: 0.00628  edgeHitrate 0.3006 sumHitrate 0.30688  privacy: 1.91545\n",
      "\n",
      "--Time: Mon Oct 11 02:28:35 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-3.30956  0.       0.     ] total reward: -3.30956\n",
      "UEHitrate: 0.00598  edgeHitrate 0.30217 sumHitrate 0.30815  privacy: 1.4927\n",
      "\n",
      "--Time: Mon Oct 11 02:28:54 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-3.37623  0.       0.     ] total reward: -3.37623\n",
      "UEHitrate: 0.00631  edgeHitrate 0.31749 sumHitrate 0.3238  privacy: 1.3129\n",
      "\n",
      "--Time: Mon Oct 11 02:29:12 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-3.11168  0.       0.     ] total reward: -3.11168\n",
      "UEHitrate: 0.00651  edgeHitrate 0.35214 sumHitrate 0.35865  privacy: 1.24267\n",
      "\n",
      "--Time: Mon Oct 11 02:29:29 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-2.85622  0.       0.     ] total reward: -2.85622\n",
      "UEHitrate: 0.00681  edgeHitrate 0.38958 sumHitrate 0.39639  privacy: 1.15457\n",
      "\n",
      "--Time: Mon Oct 11 02:29:47 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-2.65403  0.       0.     ] total reward: -2.65403\n",
      "UEHitrate: 0.00699  edgeHitrate 0.41125 sumHitrate 0.41824  privacy: 1.09798\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Oct 11 02:29:47 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-2.65403  0.       0.     ] total reward: -2.65403\n",
      "UEHitrate: 0.00699  edgeHitrate 0.41125 sumHitrate 0.41824  privacy: 1.09798\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.2819 , 0.2967 , 0.31573, 0.31038, 0.30688, 0.30815, 0.3238 ,\n",
       "        0.35865, 0.39639, 0.41824, 0.41824]),\n",
       " array([0.0077 , 0.0082 , 0.00737, 0.00692, 0.00628, 0.00598, 0.00631,\n",
       "        0.00651, 0.00681, 0.00699, 0.00699]),\n",
       " array([0.2742 , 0.2885 , 0.30837, 0.30345, 0.3006 , 0.30217, 0.31749,\n",
       "        0.35214, 0.38958, 0.41125, 0.41125]))"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([6.62953, 4.38349, 3.06879, 2.38948, 1.91545, 1.4927 , 1.3129 ,\n",
       "       1.24267, 1.15457, 1.09798, 1.09798])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep7_1011-01-28-04'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "eps_threshold = 0.5\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        #agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:31:55 2021 Episode: 0   Index: 65335   Loss: 0.20758 --\n",
      "Reward: [-3.02872  0.       0.     ] total reward: -3.02872\n",
      "UEHitrate: 0.01081  edgeHitrate 0.34156 sumHitrate 0.35236  privacy: 1.32102\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:34:58 2021 Episode: 0   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.2824  0.      0.    ] total reward: -3.2824\n",
      "UEHitrate: 0.00795  edgeHitrate 0.30494 sumHitrate 0.31289  privacy: 0.74002\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep0_1011-02-34-58\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:36:58 2021 Episode: 1   Index: 65335   Loss: 0.16849 --\n",
      "Reward: [-3.08873  0.       0.     ] total reward: -3.08873\n",
      "UEHitrate: 0.00833  edgeHitrate 0.32189 sumHitrate 0.33022  privacy: 1.34491\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:39:39 2021 Episode: 1   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29752  0.       0.     ] total reward: -3.29752\n",
      "UEHitrate: 0.00671  edgeHitrate 0.31572 sumHitrate 0.32242  privacy: 0.88955\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:41:44 2021 Episode: 2   Index: 65335   Loss: 0.17306 --\n",
      "Reward: [-3.10355  0.       0.     ] total reward: -3.10355\n",
      "UEHitrate: 0.00949  edgeHitrate 0.31742 sumHitrate 0.32691  privacy: 1.36031\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:44:25 2021 Episode: 2   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.2961  0.      0.    ] total reward: -3.2961\n",
      "UEHitrate: 0.00661  edgeHitrate 0.31473 sumHitrate 0.32134  privacy: 0.90096\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:46:26 2021 Episode: 3   Index: 65335   Loss: 0.16833 --\n",
      "Reward: [-3.09717  0.       0.     ] total reward: -3.09717\n",
      "UEHitrate: 0.00917  edgeHitrate 0.31398 sumHitrate 0.32314  privacy: 1.3467\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:49:11 2021 Episode: 3   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29901  0.       0.     ] total reward: -3.29901\n",
      "UEHitrate: 0.00664  edgeHitrate 0.30606 sumHitrate 0.31271  privacy: 0.89226\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:51:13 2021 Episode: 4   Index: 65335   Loss: 0.18285 --\n",
      "Reward: [-3.09877  0.       0.     ] total reward: -3.09877\n",
      "UEHitrate: 0.00875  edgeHitrate 0.31636 sumHitrate 0.32512  privacy: 1.3374\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:53:58 2021 Episode: 4   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29332  0.       0.     ] total reward: -3.29332\n",
      "UEHitrate: 0.007  edgeHitrate 0.30037 sumHitrate 0.30737  privacy: 0.87166\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 02:56:08 2021 Episode: 5   Index: 65335   Loss: 0.17121 --\n",
      "Reward: [-3.10298  0.       0.     ] total reward: -3.10298\n",
      "UEHitrate: 0.00816  edgeHitrate 0.3186 sumHitrate 0.32676  privacy: 1.33916\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 02:58:47 2021 Episode: 5   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29081  0.       0.     ] total reward: -3.29081\n",
      "UEHitrate: 0.00714  edgeHitrate 0.29722 sumHitrate 0.30436  privacy: 0.87523\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:00:55 2021 Episode: 6   Index: 65335   Loss: 0.17136 --\n",
      "Reward: [-3.09232  0.       0.     ] total reward: -3.09232\n",
      "UEHitrate: 0.00843  edgeHitrate 0.31274 sumHitrate 0.32117  privacy: 1.34024\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:03:35 2021 Episode: 6   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29903  0.       0.     ] total reward: -3.29903\n",
      "UEHitrate: 0.00636  edgeHitrate 0.29906 sumHitrate 0.30542  privacy: 0.89707\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:05:51 2021 Episode: 7   Index: 65335   Loss: 0.16554 --\n",
      "Reward: [-3.0832  0.      0.    ] total reward: -3.0832\n",
      "UEHitrate: 0.00932  edgeHitrate 0.31578 sumHitrate 0.3251  privacy: 1.34909\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:09:03 2021 Episode: 7   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.30034  0.       0.     ] total reward: -3.30034\n",
      "UEHitrate: 0.00629  edgeHitrate 0.30564 sumHitrate 0.31193  privacy: 0.89143\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:11:08 2021 Episode: 8   Index: 65335   Loss: 0.17305 --\n",
      "Reward: [-3.0909  0.      0.    ] total reward: -3.0909\n",
      "UEHitrate: 0.00952  edgeHitrate 0.32088 sumHitrate 0.3304  privacy: 1.33385\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:13:47 2021 Episode: 8   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29793  0.       0.     ] total reward: -3.29793\n",
      "UEHitrate: 0.00711  edgeHitrate 0.30453 sumHitrate 0.31165  privacy: 0.8809\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:15:48 2021 Episode: 9   Index: 65335   Loss: 0.17287 --\n",
      "Reward: [-3.09592  0.       0.     ] total reward: -3.09592\n",
      "UEHitrate: 0.0088  edgeHitrate 0.32213 sumHitrate 0.33094  privacy: 1.31773\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:18:28 2021 Episode: 9   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29289  0.       0.     ] total reward: -3.29289\n",
      "UEHitrate: 0.00714  edgeHitrate 0.3091 sumHitrate 0.31623  privacy: 0.84791\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:20:30 2021 Episode: 10   Index: 65335   Loss: 0.16924 --\n",
      "Reward: [-3.08574  0.       0.     ] total reward: -3.08574\n",
      "UEHitrate: 0.0088  edgeHitrate 0.32607 sumHitrate 0.33487  privacy: 1.323\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:23:12 2021 Episode: 10   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29297  0.       0.     ] total reward: -3.29297\n",
      "UEHitrate: 0.00741  edgeHitrate 0.30731 sumHitrate 0.31472  privacy: 0.86168\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:25:12 2021 Episode: 11   Index: 65335   Loss: 0.16687 --\n",
      "Reward: [-3.08831  0.       0.     ] total reward: -3.08831\n",
      "UEHitrate: 0.00915  edgeHitrate 0.31724 sumHitrate 0.32639  privacy: 1.32389\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:27:54 2021 Episode: 11   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29038  0.       0.     ] total reward: -3.29038\n",
      "UEHitrate: 0.00904  edgeHitrate 0.31241 sumHitrate 0.32145  privacy: 0.85574\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:29:57 2021 Episode: 12   Index: 65335   Loss: 0.16014 --\n",
      "Reward: [-3.09321  0.       0.     ] total reward: -3.09321\n",
      "UEHitrate: 0.00931  edgeHitrate 0.31512 sumHitrate 0.32443  privacy: 1.32091\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:33:11 2021 Episode: 12   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29241  0.       0.     ] total reward: -3.29241\n",
      "UEHitrate: 0.00722  edgeHitrate 0.30736 sumHitrate 0.31458  privacy: 0.85957\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:35:17 2021 Episode: 13   Index: 65335   Loss: 0.16444 --\n",
      "Reward: [-3.09369  0.       0.     ] total reward: -3.09369\n",
      "UEHitrate: 0.00905  edgeHitrate 0.32054 sumHitrate 0.32959  privacy: 1.30913\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:37:55 2021 Episode: 13   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29419  0.       0.     ] total reward: -3.29419\n",
      "UEHitrate: 0.00841  edgeHitrate 0.30892 sumHitrate 0.31733  privacy: 0.85157\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:39:57 2021 Episode: 14   Index: 65335   Loss: 0.16506 --\n",
      "Reward: [-3.10083  0.       0.     ] total reward: -3.10083\n",
      "UEHitrate: 0.00865  edgeHitrate 0.32448 sumHitrate 0.33312  privacy: 1.31659\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:42:35 2021 Episode: 14   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.2967  0.      0.    ] total reward: -3.2967\n",
      "UEHitrate: 0.00787  edgeHitrate 0.30338 sumHitrate 0.31124  privacy: 0.84792\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:44:37 2021 Episode: 15   Index: 65335   Loss: 0.17007 --\n",
      "Reward: [-3.1109  0.      0.    ] total reward: -3.1109\n",
      "UEHitrate: 0.00848  edgeHitrate 0.3193 sumHitrate 0.32778  privacy: 1.30375\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:47:18 2021 Episode: 15   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29361  0.       0.     ] total reward: -3.29361\n",
      "UEHitrate: 0.00734  edgeHitrate 0.30474 sumHitrate 0.31208  privacy: 0.84241\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:49:22 2021 Episode: 16   Index: 65335   Loss: 0.17012 --\n",
      "Reward: [-3.09834  0.       0.     ] total reward: -3.09834\n",
      "UEHitrate: 0.00926  edgeHitrate 0.31652 sumHitrate 0.32578  privacy: 1.29683\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:52:05 2021 Episode: 16   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29648  0.       0.     ] total reward: -3.29648\n",
      "UEHitrate: 0.00704  edgeHitrate 0.30213 sumHitrate 0.30917  privacy: 0.84996\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:54:09 2021 Episode: 17   Index: 65335   Loss: 0.16245 --\n",
      "Reward: [-3.09362  0.       0.     ] total reward: -3.09362\n",
      "UEHitrate: 0.00888  edgeHitrate 0.31372 sumHitrate 0.32259  privacy: 1.30274\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 03:56:49 2021 Episode: 17   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29827  0.       0.     ] total reward: -3.29827\n",
      "UEHitrate: 0.00848  edgeHitrate 0.30535 sumHitrate 0.31383  privacy: 0.86335\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 03:58:51 2021 Episode: 18   Index: 65335   Loss: 0.16124 --\n",
      "Reward: [-3.10834  0.       0.     ] total reward: -3.10834\n",
      "UEHitrate: 0.00886  edgeHitrate 0.3161 sumHitrate 0.32497  privacy: 1.31961\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:01:29 2021 Episode: 18   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29318  0.       0.     ] total reward: -3.29318\n",
      "UEHitrate: 0.00925  edgeHitrate 0.30339 sumHitrate 0.31263  privacy: 0.84154\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:03:36 2021 Episode: 19   Index: 65335   Loss: 0.16281 --\n",
      "Reward: [-3.09414  0.       0.     ] total reward: -3.09414\n",
      "UEHitrate: 0.00903  edgeHitrate 0.31539 sumHitrate 0.32442  privacy: 1.30214\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:06:20 2021 Episode: 19   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.29787  0.       0.     ] total reward: -3.29787\n",
      "UEHitrate: 0.00825  edgeHitrate 0.30421 sumHitrate 0.31246  privacy: 0.83075\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#test\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Mon Oct 11 04:06:41 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-5.44603  0.       0.     ] total reward: -5.44603\n",
      "UEHitrate: 0.0081  edgeHitrate 0.295 sumHitrate 0.3031  privacy: 6.65198\n",
      "\n",
      "--Time: Mon Oct 11 04:07:01 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-4.95855  0.       0.     ] total reward: -4.95855\n",
      "UEHitrate: 0.00785  edgeHitrate 0.29845 sumHitrate 0.3063  privacy: 4.3751\n",
      "\n",
      "--Time: Mon Oct 11 04:07:21 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-3.73224  0.       0.     ] total reward: -3.73224\n",
      "UEHitrate: 0.00777  edgeHitrate 0.29713 sumHitrate 0.3049  privacy: 2.84551\n",
      "\n",
      "--Time: Mon Oct 11 04:07:41 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-3.52528  0.       0.     ] total reward: -3.52528\n",
      "UEHitrate: 0.0081  edgeHitrate 0.29885 sumHitrate 0.30695  privacy: 2.11572\n",
      "\n",
      "--Time: Mon Oct 11 04:08:01 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-3.2438  0.      0.    ] total reward: -3.2438\n",
      "UEHitrate: 0.00786  edgeHitrate 0.30042 sumHitrate 0.30828  privacy: 1.65136\n",
      "\n",
      "--Time: Mon Oct 11 04:08:20 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-3.28281  0.       0.     ] total reward: -3.28281\n",
      "UEHitrate: 0.00787  edgeHitrate 0.30505 sumHitrate 0.31292  privacy: 1.22643\n",
      "\n",
      "--Time: Mon Oct 11 04:08:39 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-3.39924  0.       0.     ] total reward: -3.39924\n",
      "UEHitrate: 0.00773  edgeHitrate 0.30631 sumHitrate 0.31404  privacy: 0.95316\n",
      "\n",
      "--Time: Mon Oct 11 04:08:58 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-3.26639  0.       0.     ] total reward: -3.26639\n",
      "UEHitrate: 0.00748  edgeHitrate 0.30506 sumHitrate 0.31254  privacy: 0.76447\n",
      "\n",
      "--Time: Mon Oct 11 04:09:17 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-3.10602  0.       0.     ] total reward: -3.10602\n",
      "UEHitrate: 0.00762  edgeHitrate 0.30918 sumHitrate 0.3168  privacy: 0.539\n",
      "\n",
      "--Time: Mon Oct 11 04:09:36 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-3.02958  0.       0.     ] total reward: -3.02958\n",
      "UEHitrate: 0.00755  edgeHitrate 0.31139 sumHitrate 0.31894  privacy: 0.29245\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Oct 11 04:09:36 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-3.02958  0.       0.     ] total reward: -3.02958\n",
      "UEHitrate: 0.00755  edgeHitrate 0.31139 sumHitrate 0.31894  privacy: 0.29245\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.3031 , 0.3063 , 0.3049 , 0.30695, 0.30828, 0.31292, 0.31404,\n",
       "        0.31254, 0.3168 , 0.31894, 0.31894]),\n",
       " array([0.0081 , 0.00785, 0.00777, 0.0081 , 0.00786, 0.00787, 0.00773,\n",
       "        0.00748, 0.00762, 0.00755, 0.00755]),\n",
       " array([0.295  , 0.29845, 0.29713, 0.29885, 0.30042, 0.30505, 0.30631,\n",
       "        0.30506, 0.30918, 0.31139, 0.31139]))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([6.65198, 4.3751 , 2.84551, 2.11572, 1.65136, 1.22643, 0.95316,\n",
       "       0.76447, 0.539  , 0.29245, 0.29245])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep0_1011-02-34-58'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "eps_threshold = 0.8\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        #agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:11:20 2021 Episode: 0   Index: 65335   Loss: 0.30786 --\n",
      "Reward: [-2.25285  0.       0.     ] total reward: -2.25285\n",
      "UEHitrate: 0.01163  edgeHitrate 0.30631 sumHitrate 0.31794  privacy: 1.80176\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:14:03 2021 Episode: 0   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.35285  0.       0.     ] total reward: -0.35285\n",
      "UEHitrate: 0.00153  edgeHitrate 0.49615 sumHitrate 0.49768  privacy: 1.22871\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep0_1011-04-14-03\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:15:47 2021 Episode: 1   Index: 65335   Loss: 0.28103 --\n",
      "Reward: [-2.26133  0.       0.     ] total reward: -2.26133\n",
      "UEHitrate: 0.01157  edgeHitrate 0.30737 sumHitrate 0.31894  privacy: 1.8018\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:18:28 2021 Episode: 1   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-1.42976  0.       0.     ] total reward: -1.42976\n",
      "UEHitrate: 0.00415  edgeHitrate 0.46613 sumHitrate 0.47029  privacy: 1.83074\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:20:26 2021 Episode: 2   Index: 65335   Loss: 0.26815 --\n",
      "Reward: [-2.36195  0.       0.     ] total reward: -2.36195\n",
      "UEHitrate: 0.01012  edgeHitrate 0.29945 sumHitrate 0.30957  privacy: 1.73731\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:23:38 2021 Episode: 2   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.09744  0.       0.     ] total reward: -3.09744\n",
      "UEHitrate: 0.00877  edgeHitrate 0.31548 sumHitrate 0.32425  privacy: 1.19271\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:25:25 2021 Episode: 3   Index: 65335   Loss: 0.26281 --\n",
      "Reward: [-2.80823  0.       0.     ] total reward: -2.80823\n",
      "UEHitrate: 0.00885  edgeHitrate 0.29039 sumHitrate 0.29924  privacy: 1.56348\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:28:07 2021 Episode: 3   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.28537  0.       0.     ] total reward: -3.28537\n",
      "UEHitrate: 0.0051  edgeHitrate 0.2809 sumHitrate 0.286  privacy: 0.98876\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:29:48 2021 Episode: 4   Index: 65335   Loss: 0.26042 --\n",
      "Reward: [-2.81003  0.       0.     ] total reward: -2.81003\n",
      "UEHitrate: 0.00802  edgeHitrate 0.28318 sumHitrate 0.2912  privacy: 1.55942\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:32:28 2021 Episode: 4   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.27679  0.       0.     ] total reward: -3.27679\n",
      "UEHitrate: 0.00835  edgeHitrate 0.2884 sumHitrate 0.29674  privacy: 0.98857\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:34:13 2021 Episode: 5   Index: 65335   Loss: 0.24624 --\n",
      "Reward: [-2.8163  0.      0.    ] total reward: -2.8163\n",
      "UEHitrate: 0.00826  edgeHitrate 0.29295 sumHitrate 0.30121  privacy: 1.55989\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:36:57 2021 Episode: 5   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.32214  0.       0.     ] total reward: -3.32214\n",
      "UEHitrate: 0.01186  edgeHitrate 0.2829 sumHitrate 0.29476  privacy: 0.98733\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:38:42 2021 Episode: 6   Index: 65335   Loss: 0.23605 --\n",
      "Reward: [-2.81116  0.       0.     ] total reward: -2.81116\n",
      "UEHitrate: 0.00808  edgeHitrate 0.29209 sumHitrate 0.30017  privacy: 1.56072\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:41:24 2021 Episode: 6   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.27924  0.       0.     ] total reward: -3.27924\n",
      "UEHitrate: 0.01259  edgeHitrate 0.29279 sumHitrate 0.30537  privacy: 0.98955\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:43:13 2021 Episode: 7   Index: 65335   Loss: 0.13962 --\n",
      "Reward: [-2.80787  0.       0.     ] total reward: -2.80787\n",
      "UEHitrate: 0.00762  edgeHitrate 0.28523 sumHitrate 0.29286  privacy: 1.5587\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:45:56 2021 Episode: 7   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.25926  0.       0.     ] total reward: -3.25926\n",
      "UEHitrate: 0.00502  edgeHitrate 0.29589 sumHitrate 0.30091  privacy: 0.98703\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:47:38 2021 Episode: 8   Index: 65335   Loss: 0.13219 --\n",
      "Reward: [-2.80185  0.       0.     ] total reward: -2.80185\n",
      "UEHitrate: 0.00849  edgeHitrate 0.29175 sumHitrate 0.30025  privacy: 1.55923\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:50:16 2021 Episode: 8   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.28603  0.       0.     ] total reward: -3.28603\n",
      "UEHitrate: 0.00734  edgeHitrate 0.28035 sumHitrate 0.28768  privacy: 0.98881\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:51:58 2021 Episode: 9   Index: 65335   Loss: 0.13053 --\n",
      "Reward: [-2.81363  0.       0.     ] total reward: -2.81363\n",
      "UEHitrate: 0.0075  edgeHitrate 0.29436 sumHitrate 0.30186  privacy: 1.5609\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:54:38 2021 Episode: 9   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.3059  0.      0.    ] total reward: -3.3059\n",
      "UEHitrate: 0.00693  edgeHitrate 0.2837 sumHitrate 0.29063  privacy: 0.98937\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 04:56:19 2021 Episode: 10   Index: 65335   Loss: 0.13801 --\n",
      "Reward: [-2.81224  0.       0.     ] total reward: -2.81224\n",
      "UEHitrate: 0.00799  edgeHitrate 0.28999 sumHitrate 0.29798  privacy: 1.56133\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 04:59:03 2021 Episode: 10   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.27315  0.       0.     ] total reward: -3.27315\n",
      "UEHitrate: 0.00705  edgeHitrate 0.29075 sumHitrate 0.2978  privacy: 0.98845\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 05:00:49 2021 Episode: 11   Index: 65335   Loss: 0.13811 --\n",
      "Reward: [-2.81579  0.       0.     ] total reward: -2.81579\n",
      "UEHitrate: 0.00787  edgeHitrate 0.28903 sumHitrate 0.2969  privacy: 1.5578\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 05:03:59 2021 Episode: 11   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.32637  0.       0.     ] total reward: -3.32637\n",
      "UEHitrate: 0.00671  edgeHitrate 0.27855 sumHitrate 0.28525  privacy: 0.98961\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 05:05:47 2021 Episode: 12   Index: 65335   Loss: 0.13725 --\n",
      "Reward: [-2.80288  0.       0.     ] total reward: -2.80288\n",
      "UEHitrate: 0.00911  edgeHitrate 0.29913 sumHitrate 0.30824  privacy: 1.56572\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 05:08:26 2021 Episode: 12   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.31815  0.       0.     ] total reward: -3.31815\n",
      "UEHitrate: 0.00635  edgeHitrate 0.28413 sumHitrate 0.29048  privacy: 0.99024\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 05:10:07 2021 Episode: 13   Index: 65335   Loss: 0.13001 --\n",
      "Reward: [-2.81121  0.       0.     ] total reward: -2.81121\n",
      "UEHitrate: 0.00918  edgeHitrate 0.3092 sumHitrate 0.31838  privacy: 1.55993\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 05:12:47 2021 Episode: 13   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.32623  0.       0.     ] total reward: -3.32623\n",
      "UEHitrate: 0.00566  edgeHitrate 0.28612 sumHitrate 0.29177  privacy: 0.99017\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 05:14:33 2021 Episode: 14   Index: 65335   Loss: 0.13522 --\n",
      "Reward: [-2.81484  0.       0.     ] total reward: -2.81484\n",
      "UEHitrate: 0.00888  edgeHitrate 0.30253 sumHitrate 0.31141  privacy: 1.54912\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 05:17:14 2021 Episode: 14   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-2.96632  0.       0.     ] total reward: -2.96632\n",
      "UEHitrate: 0.00526  edgeHitrate 0.344 sumHitrate 0.34926  privacy: 0.99601\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 05:18:55 2021 Episode: 15   Index: 65335   Loss: 0.12654 --\n",
      "Reward: [-2.61171  0.       0.     ] total reward: -2.61171\n",
      "UEHitrate: 0.00837  edgeHitrate 0.29563 sumHitrate 0.304  privacy: 1.60023\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 05:21:31 2021 Episode: 15   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.24002  0.       0.     ] total reward: -3.24002\n",
      "UEHitrate: 0.00725  edgeHitrate 0.28202 sumHitrate 0.28927  privacy: 0.99194\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 05:23:14 2021 Episode: 16   Index: 65335   Loss: 0.12792 --\n",
      "Reward: [-2.71  0.    0.  ] total reward: -2.71\n",
      "UEHitrate: 0.00826  edgeHitrate 0.29533 sumHitrate 0.3036  privacy: 1.57857\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 05:26:27 2021 Episode: 16   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.23392  0.       0.     ] total reward: -3.23392\n",
      "UEHitrate: 0.00641  edgeHitrate 0.2866 sumHitrate 0.29301  privacy: 0.98972\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 05:28:30 2021 Episode: 17   Index: 65335   Loss: 0.1267 --\n",
      "Reward: [-2.65285  0.       0.     ] total reward: -2.65285\n",
      "UEHitrate: 0.00892  edgeHitrate 0.29624 sumHitrate 0.30516  privacy: 1.59388\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 05:31:07 2021 Episode: 17   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.22185  0.       0.     ] total reward: -3.22185\n",
      "UEHitrate: 0.00785  edgeHitrate 0.27946 sumHitrate 0.28731  privacy: 0.99228\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 05:32:48 2021 Episode: 18   Index: 65335   Loss: 0.12796 --\n",
      "Reward: [-2.788  0.     0.   ] total reward: -2.788\n",
      "UEHitrate: 0.0084  edgeHitrate 0.28399 sumHitrate 0.2924  privacy: 1.56768\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 05:35:27 2021 Episode: 18   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.11674  0.       0.     ] total reward: -3.11674\n",
      "UEHitrate: 0.00737  edgeHitrate 0.3199 sumHitrate 0.32727  privacy: 1.14425\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 05:37:18 2021 Episode: 19   Index: 65335   Loss: 0.12961 --\n",
      "Reward: [-2.79424  0.       0.     ] total reward: -2.79424\n",
      "UEHitrate: 0.00822  edgeHitrate 0.28514 sumHitrate 0.29336  privacy: 1.56507\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 05:40:01 2021 Episode: 19   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-2.95607  0.       0.     ] total reward: -2.95607\n",
      "UEHitrate: 0.00538  edgeHitrate 0.32394 sumHitrate 0.32931  privacy: 1.32428\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#test\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Mon Oct 11 05:40:22 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-0.57807  0.       0.     ] total reward: -0.57807\n",
      "UEHitrate: 0.0022  edgeHitrate 0.5069 sumHitrate 0.5091  privacy: 1.58945\n",
      "\n",
      "--Time: Mon Oct 11 05:40:42 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-0.53477  0.       0.     ] total reward: -0.53477\n",
      "UEHitrate: 0.0018  edgeHitrate 0.52675 sumHitrate 0.52855  privacy: 1.43453\n",
      "\n",
      "--Time: Mon Oct 11 05:41:02 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-0.42061  0.       0.     ] total reward: -0.42061\n",
      "UEHitrate: 0.00173  edgeHitrate 0.53437 sumHitrate 0.5361  privacy: 1.37822\n",
      "\n",
      "--Time: Mon Oct 11 05:41:23 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-0.39449  0.       0.     ] total reward: -0.39449\n",
      "UEHitrate: 0.00172  edgeHitrate 0.53373 sumHitrate 0.53545  privacy: 1.32796\n",
      "\n",
      "--Time: Mon Oct 11 05:41:43 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-0.368  0.     0.   ] total reward: -0.368\n",
      "UEHitrate: 0.00158  edgeHitrate 0.52516 sumHitrate 0.52674  privacy: 1.29221\n",
      "\n",
      "--Time: Mon Oct 11 05:42:04 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-0.36333  0.       0.     ] total reward: -0.36333\n",
      "UEHitrate: 0.00148  edgeHitrate 0.51988 sumHitrate 0.52137  privacy: 1.27062\n",
      "\n",
      "--Time: Mon Oct 11 05:42:24 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-0.36822  0.       0.     ] total reward: -0.36822\n",
      "UEHitrate: 0.0015  edgeHitrate 0.50714 sumHitrate 0.50864  privacy: 1.2488\n",
      "\n",
      "--Time: Mon Oct 11 05:42:44 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-0.35273  0.       0.     ] total reward: -0.35273\n",
      "UEHitrate: 0.00148  edgeHitrate 0.49691 sumHitrate 0.49839  privacy: 1.23101\n",
      "\n",
      "--Time: Mon Oct 11 05:43:04 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-0.33127  0.       0.     ] total reward: -0.33127\n",
      "UEHitrate: 0.00143  edgeHitrate 0.50187 sumHitrate 0.5033  privacy: 1.2165\n",
      "\n",
      "--Time: Mon Oct 11 05:43:25 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.31867  0.       0.     ] total reward: -0.31867\n",
      "UEHitrate: 0.00144  edgeHitrate 0.49778 sumHitrate 0.49922  privacy: 1.20142\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Oct 11 05:43:25 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.31867  0.       0.     ] total reward: -0.31867\n",
      "UEHitrate: 0.00144  edgeHitrate 0.49778 sumHitrate 0.49922  privacy: 1.20142\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5091 , 0.52855, 0.5361 , 0.53545, 0.52674, 0.52137, 0.50864,\n",
       "        0.49839, 0.5033 , 0.49922, 0.49922]),\n",
       " array([0.0022 , 0.0018 , 0.00173, 0.00172, 0.00158, 0.00148, 0.0015 ,\n",
       "        0.00148, 0.00143, 0.00144, 0.00144]),\n",
       " array([0.5069 , 0.52675, 0.53437, 0.53373, 0.52516, 0.51988, 0.50714,\n",
       "        0.49691, 0.50187, 0.49778, 0.49778]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.58945, 1.43453, 1.37822, 1.32796, 1.29221, 1.27062, 1.2488 ,\n",
       "       1.23101, 1.2165 , 1.20142, 1.20142])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep0_1011-04-14-03'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "eps_threshold = 0\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        #agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 10:23:22 2021 Episode: 0   Index: 65335   Loss: 0.15375 --\n",
      "Reward: [-0.36646  0.       0.     ] total reward: -0.36646\n",
      "UEHitrate: 0.00099  edgeHitrate 0.49761 sumHitrate 0.49861  privacy: 1.25826\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 10:26:02 2021 Episode: 0   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.35069  0.       0.     ] total reward: -0.35069\n",
      "UEHitrate: 0.00104  edgeHitrate 0.48101 sumHitrate 0.48205  privacy: 1.22761\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep0_1011-10-26-02\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 10:28:39 2021 Episode: 1   Index: 65335   Loss: 0.11375 --\n",
      "Reward: [-0.36635  0.       0.     ] total reward: -0.36635\n",
      "UEHitrate: 0.00098  edgeHitrate 0.49747 sumHitrate 0.49845  privacy: 1.25828\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 10:31:20 2021 Episode: 1   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.35077  0.       0.     ] total reward: -0.35077\n",
      "UEHitrate: 0.001  edgeHitrate 0.48267 sumHitrate 0.48366  privacy: 1.22745\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 10:34:00 2021 Episode: 2   Index: 65335   Loss: 0.11256 --\n",
      "Reward: [-0.36638  0.       0.     ] total reward: -0.36638\n",
      "UEHitrate: 0.00099  edgeHitrate 0.49738 sumHitrate 0.49838  privacy: 1.25819\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 10:36:45 2021 Episode: 2   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.35126  0.       0.     ] total reward: -0.35126\n",
      "UEHitrate: 0.0012  edgeHitrate 0.48508 sumHitrate 0.48628  privacy: 1.22788\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 10:39:23 2021 Episode: 3   Index: 65335   Loss: 0.11101 --\n",
      "Reward: [-0.36558  0.       0.     ] total reward: -0.36558\n",
      "UEHitrate: 0.00522  edgeHitrate 0.49593 sumHitrate 0.50115  privacy: 1.24415\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 10:42:05 2021 Episode: 3   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.34913  0.       0.     ] total reward: -0.34913\n",
      "UEHitrate: 0.01645  edgeHitrate 0.4824 sumHitrate 0.49884  privacy: 1.17691\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep3_1011-10-42-05\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 10:44:36 2021 Episode: 4   Index: 65335   Loss: 0.11049 --\n",
      "Reward: [-0.36268  0.       0.     ] total reward: -0.36268\n",
      "UEHitrate: 0.0244  edgeHitrate 0.49963 sumHitrate 0.52403  privacy: 1.17797\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 10:47:16 2021 Episode: 4   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.34608  0.       0.     ] total reward: -0.34608\n",
      "UEHitrate: 0.03705  edgeHitrate 0.49587 sumHitrate 0.53292  privacy: 1.09632\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep4_1011-10-47-16\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 10:49:49 2021 Episode: 5   Index: 65335   Loss: 0.11079 --\n",
      "Reward: [-0.36051  0.       0.     ] total reward: -0.36051\n",
      "UEHitrate: 0.03473  edgeHitrate 0.50767 sumHitrate 0.5424  privacy: 1.13726\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 10:52:24 2021 Episode: 5   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.34377  0.       0.     ] total reward: -0.34377\n",
      "UEHitrate: 0.044  edgeHitrate 0.50043 sumHitrate 0.54443  privacy: 1.05104\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep5_1011-10-52-24\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 10:54:57 2021 Episode: 6   Index: 65335   Loss: 0.10934 --\n",
      "Reward: [-0.36157  0.       0.     ] total reward: -0.36157\n",
      "UEHitrate: 0.03034  edgeHitrate 0.50421 sumHitrate 0.53454  privacy: 1.15681\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 10:57:41 2021 Episode: 6   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.34328  0.       0.     ] total reward: -0.34328\n",
      "UEHitrate: 0.04568  edgeHitrate 0.50228 sumHitrate 0.54796  privacy: 1.04362\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep6_1011-10-57-41\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:00:23 2021 Episode: 7   Index: 65335   Loss: 0.10887 --\n",
      "Reward: [-0.35854  0.       0.     ] total reward: -0.35854\n",
      "UEHitrate: 0.04094  edgeHitrate 0.51475 sumHitrate 0.5557  privacy: 1.10117\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:02:59 2021 Episode: 7   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.34449  0.       0.     ] total reward: -0.34449\n",
      "UEHitrate: 0.05148  edgeHitrate 0.52513 sumHitrate 0.57661  privacy: 1.00718\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:05:29 2021 Episode: 8   Index: 65335   Loss: 0.10854 --\n",
      "Reward: [-0.35764  0.       0.     ] total reward: -0.35764\n",
      "UEHitrate: 0.05349  edgeHitrate 0.5249 sumHitrate 0.57839  privacy: 1.03193\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:08:04 2021 Episode: 8   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.34733  0.       0.     ] total reward: -0.34733\n",
      "UEHitrate: 0.05745  edgeHitrate 0.5102 sumHitrate 0.56765  privacy: 0.9962\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:10:34 2021 Episode: 9   Index: 65335   Loss: 0.10821 --\n",
      "Reward: [-0.36609  0.       0.     ] total reward: -0.36609\n",
      "UEHitrate: 0.06891  edgeHitrate 0.511 sumHitrate 0.57991  privacy: 1.07728\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:13:11 2021 Episode: 9   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.35154  0.       0.     ] total reward: -0.35154\n",
      "UEHitrate: 0.06449  edgeHitrate 0.50583 sumHitrate 0.57032  privacy: 1.05155\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:15:46 2021 Episode: 10   Index: 65335   Loss: 0.10687 --\n",
      "Reward: [-0.36675  0.       0.     ] total reward: -0.36675\n",
      "UEHitrate: 0.00116  edgeHitrate 0.49645 sumHitrate 0.49761  privacy: 1.25838\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:18:27 2021 Episode: 10   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.35173  0.       0.     ] total reward: -0.35173\n",
      "UEHitrate: 0.00134  edgeHitrate 0.48579 sumHitrate 0.48713  privacy: 1.22803\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:21:01 2021 Episode: 11   Index: 65335   Loss: 0.10633 --\n",
      "Reward: [-0.36684  0.       0.     ] total reward: -0.36684\n",
      "UEHitrate: 0.00129  edgeHitrate 0.49605 sumHitrate 0.49734  privacy: 1.25865\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:23:41 2021 Episode: 11   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-0.35221  0.       0.     ] total reward: -0.35221\n",
      "UEHitrate: 0.0021  edgeHitrate 0.50178 sumHitrate 0.50387  privacy: 1.22737\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:26:19 2021 Episode: 12   Index: 65335   Loss: 0.16543 --\n",
      "Reward: [-2.03514  0.       0.     ] total reward: -2.03514\n",
      "UEHitrate: 0.00459  edgeHitrate 0.32226 sumHitrate 0.32685  privacy: 1.60563\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:29:06 2021 Episode: 12   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.21387  0.       0.     ] total reward: -3.21387\n",
      "UEHitrate: 0.0047  edgeHitrate 0.28322 sumHitrate 0.28792  privacy: 0.9868\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:31:42 2021 Episode: 13   Index: 65335   Loss: 0.15918 --\n",
      "Reward: [-3.3105  0.      0.    ] total reward: -3.3105\n",
      "UEHitrate: 0.00542  edgeHitrate 0.27962 sumHitrate 0.28503  privacy: 1.32305\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:34:27 2021 Episode: 13   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.21697  0.       0.     ] total reward: -3.21697\n",
      "UEHitrate: 0.00514  edgeHitrate 0.2826 sumHitrate 0.28774  privacy: 0.98745\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:37:06 2021 Episode: 14   Index: 65335   Loss: 0.16111 --\n",
      "Reward: [-3.30488  0.       0.     ] total reward: -3.30488\n",
      "UEHitrate: 0.00559  edgeHitrate 0.28301 sumHitrate 0.2886  privacy: 1.32428\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:39:51 2021 Episode: 14   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.21685  0.       0.     ] total reward: -3.21685\n",
      "UEHitrate: 0.00528  edgeHitrate 0.28952 sumHitrate 0.29479  privacy: 0.98875\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:42:42 2021 Episode: 15   Index: 65335   Loss: 0.16546 --\n",
      "Reward: [-3.31213  0.       0.     ] total reward: -3.31213\n",
      "UEHitrate: 0.00571  edgeHitrate 0.28975 sumHitrate 0.29546  privacy: 1.32337\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:46:18 2021 Episode: 15   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.21523  0.       0.     ] total reward: -3.21523\n",
      "UEHitrate: 0.00547  edgeHitrate 0.29875 sumHitrate 0.30423  privacy: 0.98757\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:49:14 2021 Episode: 16   Index: 65335   Loss: 0.16725 --\n",
      "Reward: [-3.31948  0.       0.     ] total reward: -3.31948\n",
      "UEHitrate: 0.00568  edgeHitrate 0.29027 sumHitrate 0.29595  privacy: 1.3241\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:52:00 2021 Episode: 16   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.21947  0.       0.     ] total reward: -3.21947\n",
      "UEHitrate: 0.00524  edgeHitrate 0.28379 sumHitrate 0.28903  privacy: 0.98764\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 11:54:59 2021 Episode: 17   Index: 65335   Loss: 0.16597 --\n",
      "Reward: [-3.3244  0.      0.    ] total reward: -3.3244\n",
      "UEHitrate: 0.00497  edgeHitrate 0.28415 sumHitrate 0.28912  privacy: 1.32309\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 11:58:04 2021 Episode: 17   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.2321  0.      0.    ] total reward: -3.2321\n",
      "UEHitrate: 0.00515  edgeHitrate 0.29631 sumHitrate 0.30146  privacy: 0.98828\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 12:00:49 2021 Episode: 18   Index: 65335   Loss: 0.1697 --\n",
      "Reward: [-3.33642  0.       0.     ] total reward: -3.33642\n",
      "UEHitrate: 0.0054  edgeHitrate 0.28944 sumHitrate 0.29485  privacy: 1.32335\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 12:03:43 2021 Episode: 18   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.30551  0.       0.     ] total reward: -3.30551\n",
      "UEHitrate: 0.00579  edgeHitrate 0.28456 sumHitrate 0.29036  privacy: 0.99147\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 12:06:36 2021 Episode: 19   Index: 65335   Loss: 0.1645 --\n",
      "Reward: [-3.35606  0.       0.     ] total reward: -3.35606\n",
      "UEHitrate: 0.00597  edgeHitrate 0.29838 sumHitrate 0.30435  privacy: 1.32377\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 12:09:35 2021 Episode: 19   Index: 81113   Loss: 0.0 --\n",
      "Reward: [-3.30718  0.       0.     ] total reward: -3.30718\n",
      "UEHitrate: 0.00566  edgeHitrate 0.28751 sumHitrate 0.29317  privacy: 0.99042\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "#test\n",
    "policy_net = DQN(3, 2).to(device)\n",
    "target_net = DQN(3, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Mon Oct 11 14:26:44 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-0.57783  0.       0.     ] total reward: -0.57783\n",
      "UEHitrate: 0.0019  edgeHitrate 0.5071 sumHitrate 0.509  privacy: 1.59112\n",
      "\n",
      "--Time: Mon Oct 11 14:27:05 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-0.53051  0.       0.     ] total reward: -0.53051\n",
      "UEHitrate: 0.0068  edgeHitrate 0.51075 sumHitrate 0.51755  privacy: 1.41753\n",
      "\n",
      "--Time: Mon Oct 11 14:27:25 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-0.4093  0.      0.    ] total reward: -0.4093\n",
      "UEHitrate: 0.02463  edgeHitrate 0.5312 sumHitrate 0.55583  privacy: 1.26861\n",
      "\n",
      "--Time: Mon Oct 11 14:27:45 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-0.38171  0.       0.     ] total reward: -0.38171\n",
      "UEHitrate: 0.03258  edgeHitrate 0.54053 sumHitrate 0.5731  privacy: 1.19273\n",
      "\n",
      "--Time: Mon Oct 11 14:28:05 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-0.35605  0.       0.     ] total reward: -0.35605\n",
      "UEHitrate: 0.03662  edgeHitrate 0.53162 sumHitrate 0.56824  privacy: 1.14609\n",
      "\n",
      "--Time: Mon Oct 11 14:28:24 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-0.35153  0.       0.     ] total reward: -0.35153\n",
      "UEHitrate: 0.04087  edgeHitrate 0.52798 sumHitrate 0.56885  privacy: 1.10224\n",
      "\n",
      "--Time: Mon Oct 11 14:28:45 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-0.35733  0.       0.     ] total reward: -0.35733\n",
      "UEHitrate: 0.04266  edgeHitrate 0.51616 sumHitrate 0.55881  privacy: 1.0713\n",
      "\n",
      "--Time: Mon Oct 11 14:29:04 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-0.34293  0.       0.     ] total reward: -0.34293\n",
      "UEHitrate: 0.04355  edgeHitrate 0.50502 sumHitrate 0.54858  privacy: 1.04645\n",
      "\n",
      "--Time: Mon Oct 11 14:29:24 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-0.32199  0.       0.     ] total reward: -0.32199\n",
      "UEHitrate: 0.04559  edgeHitrate 0.51041 sumHitrate 0.556  privacy: 1.013\n",
      "\n",
      "--Time: Mon Oct 11 14:29:44 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.31007  0.       0.     ] total reward: -0.31007\n",
      "UEHitrate: 0.04546  edgeHitrate 0.50673 sumHitrate 0.55219  privacy: 0.98708\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Oct 11 14:29:44 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.31007  0.       0.     ] total reward: -0.31007\n",
      "UEHitrate: 0.04546  edgeHitrate 0.50673 sumHitrate 0.55219  privacy: 0.98708\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.509  , 0.51755, 0.55583, 0.5731 , 0.56824, 0.56885, 0.55881,\n",
       "        0.54858, 0.556  , 0.55219, 0.55219]),\n",
       " array([0.0019 , 0.0068 , 0.02463, 0.03258, 0.03662, 0.04087, 0.04266,\n",
       "        0.04355, 0.04559, 0.04546, 0.04546]),\n",
       " array([0.5071 , 0.51075, 0.5312 , 0.54053, 0.53162, 0.52798, 0.51616,\n",
       "        0.50502, 0.51041, 0.50673, 0.50673]))"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.59112, 1.41753, 1.26861, 1.19273, 1.14609, 1.10224, 1.0713 ,\n",
       "       1.04645, 1.013  , 0.98708, 0.98708])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v5_ep6_1011-10-57-41'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}