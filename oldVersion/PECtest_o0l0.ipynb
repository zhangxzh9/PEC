{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>1030</td>\n",
       "      <td>101001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11807</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15068</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>1035</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5375</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1030</td>\n",
       "      <td>10202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5992</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3468</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148        1030        101001     0   \n",
       "1       203  5779    0        0         7        1030         10203     0   \n",
       "2       208  4675    0        0        92        1035         10203     0   \n",
       "3       159   332    0        0        56        1030         10202     0   \n",
       "4        50   674    0        0       439        1030         10203     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831   29  2591880        34        1030         10203     0   \n",
       "300979  158  8448   29  2591880        34        1030         10203     0   \n",
       "300980  483  6463   29  2591940        35        1030         10203     0   \n",
       "300981  158  4715   29  2591940        34        1030         10203     0   \n",
       "300982  483  2021   29  2591940        34        1030         10203     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0      11807          1            2  \n",
       "1              0      15068          1            2  \n",
       "2              0       5375          1            2  \n",
       "3              0       5992          1            2  \n",
       "4              0       3468          1            2  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0      10010          1            2  \n",
       "300979         0      23340          1            2  \n",
       "300980         0      10010          1            2  \n",
       "300981         0      23340          1            2  \n",
       "300982         0      10010          1            2  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " 500,\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148        1030        101001     0   \n",
       " 1       203  5779    0        0         7        1030         10203     0   \n",
       " 2       208  4675    0        0        92        1035         10203     0   \n",
       " 3       159   332    0        0        56        1030         10202     0   \n",
       " 4        50   674    0        0       439        1030         10203     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 198170  264  7442   17  1555140        90        1035         10203     0   \n",
       " 198171   19  9362   17  1555140       424        1035         10203     0   \n",
       " 198172   82  9223   17  1555140        94        1037         10203     0   \n",
       " 198173   35  4164   17  1555140        22        1030         10203     0   \n",
       " 198174  239  5062   17  1555140        89        1035         10203     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0      11807          1            2  \n",
       " 1              0      15068          1            2  \n",
       " 2              0       5375          1            2  \n",
       " 3              0       5992          1            2  \n",
       " 4              0       3468          1            2  \n",
       " ...          ...        ...        ...          ...  \n",
       " 198170         0       7592          1            2  \n",
       " 198171         0       5938          1            2  \n",
       " 198172         0      11393          1            2  \n",
       " 198173         0       5866          1            2  \n",
       " 198174         0      23746          1            2  \n",
       " \n",
       " [198175 rows x 12 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=15,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p) , \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)).sum() - torch.log(v * p + (1-v) * (1-p)).sum())\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl *  ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "\n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "        \n",
    "        if train: \n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                    lastAction.to(device), \n",
    "                    self.statusFeature,\n",
    "                    torch.tensor([self.reward.float()]).to(device),\n",
    "                    torch.tensor([self.W[-1]]).to(device))\n",
    "        \n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "        \n",
    "\n",
    "        if  not train or (train and sample > eps_threshold):\n",
    "            QNetwork.eval()\n",
    "            with torch.no_grad():\n",
    "                Q_value = QNetwork(self.statusFeature)\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "            QNetwork.train()\n",
    "        else:\n",
    "            actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort()[0:10])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELPATH =  './model_dict/dnn_o0l0_'\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.0,\"betal\":0}\n",
    "latency = [0.1,1,0.9]\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Mon Sep 20 12:31:55 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 12:39:19 2021 Episode: 0   Index: 50000   Loss: 0.3114 --\n",
      "Reward: [0.30666 0.      0.     ] total reward: 0.30666\n",
      "UEHitrate: 0.0052  edgeHitrate 0.10284 sumHitrate 0.10804  privacy: 2.52815\n",
      "\n",
      "--Time: Mon Sep 20 12:46:20 2021 Episode: 0   Index: 100000   Loss: 0.29459 --\n",
      "Reward: [0.36806 0.      0.     ] total reward: 0.36806\n",
      "UEHitrate: 0.00519  edgeHitrate 0.09411 sumHitrate 0.0993  privacy: 1.9491\n",
      "\n",
      "--Time: Mon Sep 20 12:53:27 2021 Episode: 0   Index: 150000   Loss: 0.28303 --\n",
      "Reward: [0.4362 0.     0.    ] total reward: 0.4362\n",
      "UEHitrate: 0.00512  edgeHitrate 0.09331 sumHitrate 0.09843  privacy: 1.58978\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 13:00:42 2021 Episode: 0   Index: 198174   Loss: 0.27439 --\n",
      "Reward: [0.49813 0.      0.     ] total reward: 0.49813\n",
      "UEHitrate: 0.00531  edgeHitrate 0.10389 sumHitrate 0.1092  privacy: 1.33697\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_o0l0_ep0_0920-13-00-42\n",
      "\n",
      "--Time: Mon Sep 20 13:00:42 2021 Episode: 1   Index: 0   Loss: 6.59654 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 13:08:23 2021 Episode: 1   Index: 50000   Loss: 0.21586 --\n",
      "Reward: [0.30762 0.      0.     ] total reward: 0.30762\n",
      "UEHitrate: 0.0118  edgeHitrate 0.12056 sumHitrate 0.13236  privacy: 2.58627\n",
      "\n",
      "--Time: Mon Sep 20 13:15:50 2021 Episode: 1   Index: 100000   Loss: 0.20048 --\n",
      "Reward: [0.36106 0.      0.     ] total reward: 0.36106\n",
      "UEHitrate: 0.01442  edgeHitrate 0.13098 sumHitrate 0.1454  privacy: 2.0581\n",
      "\n",
      "--Time: Mon Sep 20 13:23:24 2021 Episode: 1   Index: 150000   Loss: 0.18973 --\n",
      "Reward: [0.42149 0.      0.     ] total reward: 0.42149\n",
      "UEHitrate: 0.01534  edgeHitrate 0.15497 sumHitrate 0.17031  privacy: 1.69732\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 13:30:26 2021 Episode: 1   Index: 198174   Loss: 0.18235 --\n",
      "Reward: [0.47831 0.      0.     ] total reward: 0.47831\n",
      "UEHitrate: 0.01455  edgeHitrate 0.17928 sumHitrate 0.19383  privacy: 1.39472\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 13:30:26 2021 Episode: 2   Index: 0   Loss: 5.57119 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 13:37:50 2021 Episode: 2   Index: 50000   Loss: 0.19264 --\n",
      "Reward: [0.29839 0.      0.     ] total reward: 0.29839\n",
      "UEHitrate: 0.00656  edgeHitrate 0.114 sumHitrate 0.12056  privacy: 2.58643\n",
      "\n",
      "--Time: Mon Sep 20 13:44:56 2021 Episode: 2   Index: 100000   Loss: 0.18093 --\n",
      "Reward: [0.35918 0.      0.     ] total reward: 0.35918\n",
      "UEHitrate: 0.01645  edgeHitrate 0.19228 sumHitrate 0.20873  privacy: 2.08614\n",
      "\n",
      "--Time: Mon Sep 20 13:52:13 2021 Episode: 2   Index: 150000   Loss: 0.16871 --\n",
      "Reward: [0.41676 0.      0.     ] total reward: 0.41676\n",
      "UEHitrate: 0.02077  edgeHitrate 0.23445 sumHitrate 0.25522  privacy: 1.78583\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 13:59:19 2021 Episode: 2   Index: 198174   Loss: 0.15999 --\n",
      "Reward: [0.46384 0.      0.     ] total reward: 0.46384\n",
      "UEHitrate: 0.02119  edgeHitrate 0.23866 sumHitrate 0.25986  privacy: 1.56654\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 13:59:20 2021 Episode: 3   Index: 0   Loss: 4.98792 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 14:09:22 2021 Episode: 3   Index: 50000   Loss: 0.15881 --\n",
      "Reward: [0.30246 0.      0.     ] total reward: 0.30246\n",
      "UEHitrate: 0.01452  edgeHitrate 0.19308 sumHitrate 0.2076  privacy: 2.6751\n",
      "\n",
      "--Time: Mon Sep 20 14:18:31 2021 Episode: 3   Index: 100000   Loss: 0.15045 --\n",
      "Reward: [0.35368 0.      0.     ] total reward: 0.35368\n",
      "UEHitrate: 0.02339  edgeHitrate 0.24966 sumHitrate 0.27305  privacy: 2.20651\n",
      "\n",
      "--Time: Mon Sep 20 14:25:49 2021 Episode: 3   Index: 150000   Loss: 0.13855 --\n",
      "Reward: [0.40287 0.      0.     ] total reward: 0.40287\n",
      "UEHitrate: 0.02049  edgeHitrate 0.21789 sumHitrate 0.23838  privacy: 1.7632\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 14:33:03 2021 Episode: 3   Index: 198174   Loss: 0.13007 --\n",
      "Reward: [0.46168 0.      0.     ] total reward: 0.46168\n",
      "UEHitrate: 0.01759  edgeHitrate 0.24885 sumHitrate 0.26644  privacy: 1.38179\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 14:33:04 2021 Episode: 4   Index: 0   Loss: 4.46701 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 14:40:16 2021 Episode: 4   Index: 50000   Loss: 0.11998 --\n",
      "Reward: [0.30109 0.      0.     ] total reward: 0.30109\n",
      "UEHitrate: 0.00346  edgeHitrate 0.104 sumHitrate 0.10746  privacy: 2.53516\n",
      "\n",
      "--Time: Mon Sep 20 14:47:36 2021 Episode: 4   Index: 100000   Loss: 0.11004 --\n",
      "Reward: [0.37325 0.      0.     ] total reward: 0.37325\n",
      "UEHitrate: 0.00344  edgeHitrate 0.09759 sumHitrate 0.10103  privacy: 1.81358\n",
      "\n",
      "--Time: Mon Sep 20 14:55:19 2021 Episode: 4   Index: 150000   Loss: 0.1056 --\n",
      "Reward: [0.46398 0.      0.     ] total reward: 0.46398\n",
      "UEHitrate: 0.00406  edgeHitrate 0.12119 sumHitrate 0.12525  privacy: 1.31214\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 15:03:58 2021 Episode: 4   Index: 198174   Loss: 0.10253 --\n",
      "Reward: [0.58364 0.      0.     ] total reward: 0.58364\n",
      "UEHitrate: 0.00421  edgeHitrate 0.12085 sumHitrate 0.12506  privacy: 0.86307\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_o0l0_ep4_0920-15-03-58\n",
      "\n",
      "--Time: Mon Sep 20 15:03:58 2021 Episode: 5   Index: 0   Loss: 3.31713 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 15:12:07 2021 Episode: 5   Index: 50000   Loss: 0.11438 --\n",
      "Reward: [0.3002 0.     0.    ] total reward: 0.3002\n",
      "UEHitrate: 0.00324  edgeHitrate 0.07118 sumHitrate 0.07442  privacy: 2.5176\n",
      "\n",
      "--Time: Mon Sep 20 15:21:18 2021 Episode: 5   Index: 100000   Loss: 0.10767 --\n",
      "Reward: [0.3748 0.     0.    ] total reward: 0.3748\n",
      "UEHitrate: 0.0034  edgeHitrate 0.0736 sumHitrate 0.077  privacy: 1.79293\n",
      "\n",
      "--Time: Mon Sep 20 15:30:19 2021 Episode: 5   Index: 150000   Loss: 0.10304 --\n",
      "Reward: [0.49281 0.      0.     ] total reward: 0.49281\n",
      "UEHitrate: 0.00433  edgeHitrate 0.09971 sumHitrate 0.10404  privacy: 1.1225\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 15:39:39 2021 Episode: 5   Index: 198174   Loss: 0.09962 --\n",
      "Reward: [0.65163 0.      0.     ] total reward: 0.65163\n",
      "UEHitrate: 0.00451  edgeHitrate 0.10093 sumHitrate 0.10544  privacy: 0.74629\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_o0l0_ep5_0920-15-39-39\n",
      "\n",
      "--Time: Mon Sep 20 15:39:40 2021 Episode: 6   Index: 0   Loss: 2.54908 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 15:48:18 2021 Episode: 6   Index: 50000   Loss: 0.11869 --\n",
      "Reward: [0.3592 0.     0.    ] total reward: 0.3592\n",
      "UEHitrate: 0.00874  edgeHitrate 0.15538 sumHitrate 0.16412  privacy: 2.35253\n",
      "\n",
      "--Time: Mon Sep 20 15:57:21 2021 Episode: 6   Index: 100000   Loss: 0.11436 --\n",
      "Reward: [0.41777 0.      0.     ] total reward: 0.41777\n",
      "UEHitrate: 0.00704  edgeHitrate 0.13685 sumHitrate 0.14389  privacy: 1.82518\n",
      "\n",
      "--Time: Mon Sep 20 16:06:18 2021 Episode: 6   Index: 150000   Loss: 0.11 --\n",
      "Reward: [0.49132 0.      0.     ] total reward: 0.49132\n",
      "UEHitrate: 0.00653  edgeHitrate 0.13004 sumHitrate 0.13657  privacy: 1.38999\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 16:15:50 2021 Episode: 6   Index: 198174   Loss: 0.10563 --\n",
      "Reward: [0.55057 0.      0.     ] total reward: 0.55057\n",
      "UEHitrate: 0.00592  edgeHitrate 0.12356 sumHitrate 0.12948  privacy: 1.25878\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 16:15:51 2021 Episode: 7   Index: 0   Loss: 2.80002 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 16:24:39 2021 Episode: 7   Index: 50000   Loss: 0.1195 --\n",
      "Reward: [0.32475 0.      0.     ] total reward: 0.32475\n",
      "UEHitrate: 0.00382  edgeHitrate 0.1272 sumHitrate 0.13102  privacy: 2.68859\n",
      "\n",
      "--Time: Mon Sep 20 16:33:46 2021 Episode: 7   Index: 100000   Loss: 0.11556 --\n",
      "Reward: [0.35271 0.      0.     ] total reward: 0.35271\n",
      "UEHitrate: 0.00398  edgeHitrate 0.11942 sumHitrate 0.1234  privacy: 2.11169\n",
      "\n",
      "--Time: Mon Sep 20 16:43:12 2021 Episode: 7   Index: 150000   Loss: 0.1087 --\n",
      "Reward: [0.40723 0.      0.     ] total reward: 0.40723\n",
      "UEHitrate: 0.00943  edgeHitrate 0.12313 sumHitrate 0.13257  privacy: 1.77349\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 16:51:05 2021 Episode: 7   Index: 198174   Loss: 0.10197 --\n",
      "Reward: [0.45252 0.      0.     ] total reward: 0.45252\n",
      "UEHitrate: 0.01342  edgeHitrate 0.12706 sumHitrate 0.14048  privacy: 1.5717\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 16:51:06 2021 Episode: 8   Index: 0   Loss: 2.55292 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 16:59:44 2021 Episode: 8   Index: 50000   Loss: 0.09731 --\n",
      "Reward: [0.37133 0.      0.     ] total reward: 0.37133\n",
      "UEHitrate: 0.02416  edgeHitrate 0.19412 sumHitrate 0.21828  privacy: 2.20271\n",
      "\n",
      "--Time: Mon Sep 20 17:08:58 2021 Episode: 8   Index: 100000   Loss: 0.09156 --\n",
      "Reward: [0.42845 0.      0.     ] total reward: 0.42845\n",
      "UEHitrate: 0.0141  edgeHitrate 0.14951 sumHitrate 0.16361  privacy: 1.56814\n",
      "\n",
      "--Time: Mon Sep 20 17:18:38 2021 Episode: 8   Index: 150000   Loss: 0.08633 --\n",
      "Reward: [0.52813 0.      0.     ] total reward: 0.52813\n",
      "UEHitrate: 0.01115  edgeHitrate 0.13604 sumHitrate 0.14719  privacy: 1.21193\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 17:27:37 2021 Episode: 8   Index: 198174   Loss: 0.08158 --\n",
      "Reward: [0.60733 0.      0.     ] total reward: 0.60733\n",
      "UEHitrate: 0.00975  edgeHitrate 0.12832 sumHitrate 0.13807  privacy: 1.04413\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 17:27:37 2021 Episode: 9   Index: 0   Loss: 1.97056 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 17:36:06 2021 Episode: 9   Index: 50000   Loss: 0.09554 --\n",
      "Reward: [0.32322 0.      0.     ] total reward: 0.32322\n",
      "UEHitrate: 0.00914  edgeHitrate 0.16946 sumHitrate 0.1786  privacy: 2.42575\n",
      "\n",
      "--Time: Mon Sep 20 17:46:02 2021 Episode: 9   Index: 100000   Loss: 0.09333 --\n",
      "Reward: [0.39516 0.      0.     ] total reward: 0.39516\n",
      "UEHitrate: 0.01921  edgeHitrate 0.16861 sumHitrate 0.18782  privacy: 1.90681\n",
      "\n",
      "--Time: Mon Sep 20 17:56:22 2021 Episode: 9   Index: 150000   Loss: 0.08969 --\n",
      "Reward: [0.46078 0.      0.     ] total reward: 0.46078\n",
      "UEHitrate: 0.02717  edgeHitrate 0.17534 sumHitrate 0.20251  privacy: 1.72561\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 18:06:58 2021 Episode: 9   Index: 198174   Loss: 0.08728 --\n",
      "Reward: [0.50423 0.      0.     ] total reward: 0.50423\n",
      "UEHitrate: 0.03164  edgeHitrate 0.18022 sumHitrate 0.21186  privacy: 1.62344\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "TARGET_UPDATE = 1\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = trainUIT.shape[0]*5\n",
    "agentStep = 0\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "        agentStep += 1\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 50000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+'ep{}_'.format(bestEpisode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "        print(bestPath)\n",
    "    print()\n",
    "    env = ENV(userNum,contentNum,latency)\n",
    "    UEs = {}\n",
    "    sumReward = sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Mon Sep 20 18:07:04 2021   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 18:07:56 2021   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.52713 0.      0.     ] total reward: 0.52713\n",
      "UEHitrate: 0.0112  edgeHitrate 0.34367 sumHitrate 0.35486  privacy: 1.48083\n",
      "\n",
      "--Time: Mon Sep 20 18:08:46 2021   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.67105 0.      0.     ] total reward: 0.67105\n",
      "UEHitrate: 0.0134  edgeHitrate 0.42693 sumHitrate 0.44033  privacy: 1.08159\n",
      "\n",
      "--Time: Mon Sep 20 18:09:36 2021   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.78139 0.      0.     ] total reward: 0.78139\n",
      "UEHitrate: 0.01207  edgeHitrate 0.41125 sumHitrate 0.42332  privacy: 0.87571\n",
      "\n",
      "--Time: Mon Sep 20 18:10:27 2021   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.87756 0.      0.     ] total reward: 0.87756\n",
      "UEHitrate: 0.01087  edgeHitrate 0.38912 sumHitrate 0.39999  privacy: 0.79483\n",
      "\n",
      "--Time: Mon Sep 20 18:11:22 2021   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.96835 0.      0.     ] total reward: 0.96835\n",
      "UEHitrate: 0.01036  edgeHitrate 0.37677 sumHitrate 0.38713  privacy: 0.72363\n",
      "\n",
      "--Time: Mon Sep 20 18:12:06 2021   Index: 60000   Loss: 0.0 --\n",
      "Reward: [1.05018 0.      0.     ] total reward: 1.05018\n",
      "UEHitrate: 0.00962  edgeHitrate 0.36598 sumHitrate 0.37559  privacy: 0.66947\n",
      "\n",
      "--Time: Mon Sep 20 18:12:55 2021   Index: 70000   Loss: 0.0 --\n",
      "Reward: [1.12437 0.      0.     ] total reward: 1.12437\n",
      "UEHitrate: 0.00897  edgeHitrate 0.35237 sumHitrate 0.36134  privacy: 0.62904\n",
      "\n",
      "--Time: Mon Sep 20 18:13:41 2021   Index: 80000   Loss: 0.0 --\n",
      "Reward: [1.19508 0.      0.     ] total reward: 1.19508\n",
      "UEHitrate: 0.00855  edgeHitrate 0.34391 sumHitrate 0.35246  privacy: 0.60364\n",
      "\n",
      "--Time: Mon Sep 20 18:14:26 2021   Index: 90000   Loss: 0.0 --\n",
      "Reward: [1.26761 0.      0.     ] total reward: 1.26761\n",
      "UEHitrate: 0.00811  edgeHitrate 0.33566 sumHitrate 0.34377  privacy: 0.55926\n",
      "\n",
      "--Time: Mon Sep 20 18:15:12 2021   Index: 100000   Loss: 0.0 --\n",
      "Reward: [1.34115 0.      0.     ] total reward: 1.34115\n",
      "UEHitrate: 0.00789  edgeHitrate 0.3274 sumHitrate 0.33529  privacy: 0.52071\n",
      "\n",
      "--Time: Mon Sep 20 18:15:55 2021   Index: 110000   Loss: 0.0 --\n",
      "Reward: [1.41449 0.      0.     ] total reward: 1.41449\n",
      "UEHitrate: 0.00755  edgeHitrate 0.32 sumHitrate 0.32755  privacy: 0.49438\n",
      "\n",
      "--Time: Mon Sep 20 18:16:37 2021   Index: 120000   Loss: 0.0 --\n",
      "Reward: [1.48925 0.      0.     ] total reward: 1.48925\n",
      "UEHitrate: 0.0073  edgeHitrate 0.31324 sumHitrate 0.32054  privacy: 0.44702\n",
      "\n",
      "--Time: Mon Sep 20 18:17:21 2021   Index: 130000   Loss: 0.0 --\n",
      "Reward: [1.58052 0.      0.     ] total reward: 1.58052\n",
      "UEHitrate: 0.00719  edgeHitrate 0.30774 sumHitrate 0.31493  privacy: 0.40594\n",
      "\n",
      "--Time: Mon Sep 20 18:18:14 2021   Index: 140000   Loss: 0.0 --\n",
      "Reward: [1.66033 0.      0.     ] total reward: 1.66033\n",
      "UEHitrate: 0.00705  edgeHitrate 0.30552 sumHitrate 0.31257  privacy: 0.37007\n",
      "\n",
      "--Time: Mon Sep 20 18:19:07 2021   Index: 150000   Loss: 0.0 --\n",
      "Reward: [1.76293 0.      0.     ] total reward: 1.76293\n",
      "UEHitrate: 0.00691  edgeHitrate 0.30283 sumHitrate 0.30974  privacy: 0.34073\n",
      "\n",
      "--Time: Mon Sep 20 18:19:49 2021   Index: 160000   Loss: 0.0 --\n",
      "Reward: [1.85756 0.      0.     ] total reward: 1.85756\n",
      "UEHitrate: 0.00672  edgeHitrate 0.29624 sumHitrate 0.30296  privacy: 0.32011\n",
      "\n",
      "--Time: Mon Sep 20 18:20:39 2021   Index: 170000   Loss: 0.0 --\n",
      "Reward: [1.95855 0.      0.     ] total reward: 1.95855\n",
      "UEHitrate: 0.00658  edgeHitrate 0.29386 sumHitrate 0.30044  privacy: 0.29921\n",
      "\n",
      "--Time: Mon Sep 20 18:21:34 2021   Index: 180000   Loss: 0.0 --\n",
      "Reward: [2.05094 0.      0.     ] total reward: 2.05094\n",
      "UEHitrate: 0.00645  edgeHitrate 0.29239 sumHitrate 0.29884  privacy: 0.2795\n",
      "\n",
      "--Time: Mon Sep 20 18:22:23 2021   Index: 190000   Loss: 0.0 --\n",
      "Reward: [2.16191 0.      0.     ] total reward: 2.16191\n",
      "UEHitrate: 0.00638  edgeHitrate 0.28952 sumHitrate 0.2959  privacy: 0.24684\n",
      "\n",
      "--Time: Mon Sep 20 18:23:05 2021   Index: 200000   Loss: 0.0 --\n",
      "Reward: [2.28294 0.      0.     ] total reward: 2.28294\n",
      "UEHitrate: 0.00623  edgeHitrate 0.28691 sumHitrate 0.29315  privacy: 0.23461\n",
      "\n",
      "--Time: Mon Sep 20 18:23:50 2021   Index: 210000   Loss: 0.0 --\n",
      "Reward: [2.40918 0.      0.     ] total reward: 2.40918\n",
      "UEHitrate: 0.00622  edgeHitrate 0.28596 sumHitrate 0.29218  privacy: 0.21337\n",
      "\n",
      "--Time: Mon Sep 20 18:24:32 2021   Index: 220000   Loss: 0.0 --\n",
      "Reward: [2.54493 0.      0.     ] total reward: 2.54493\n",
      "UEHitrate: 0.00614  edgeHitrate 0.2836 sumHitrate 0.28974  privacy: 0.19268\n",
      "\n",
      "--Time: Mon Sep 20 18:25:16 2021   Index: 230000   Loss: 0.0 --\n",
      "Reward: [2.69486 0.      0.     ] total reward: 2.69486\n",
      "UEHitrate: 0.00609  edgeHitrate 0.27955 sumHitrate 0.28564  privacy: 0.17061\n",
      "\n",
      "--Time: Mon Sep 20 18:25:59 2021   Index: 240000   Loss: 0.0 --\n",
      "Reward: [2.84883 0.      0.     ] total reward: 2.84883\n",
      "UEHitrate: 0.006  edgeHitrate 0.27518 sumHitrate 0.28117  privacy: 0.16147\n",
      "\n",
      "--Time: Mon Sep 20 18:26:48 2021   Index: 250000   Loss: 0.0 --\n",
      "Reward: [3.00149 0.      0.     ] total reward: 3.00149\n",
      "UEHitrate: 0.00595  edgeHitrate 0.27218 sumHitrate 0.27813  privacy: 0.15841\n",
      "\n",
      "--Time: Mon Sep 20 18:27:32 2021   Index: 260000   Loss: 0.0 --\n",
      "Reward: [3.15147 0.      0.     ] total reward: 3.15147\n",
      "UEHitrate: 0.0059  edgeHitrate 0.26943 sumHitrate 0.27533  privacy: 0.15825\n",
      "\n",
      "--Time: Mon Sep 20 18:28:18 2021   Index: 270000   Loss: 0.0 --\n",
      "Reward: [3.29458 0.      0.     ] total reward: 3.29458\n",
      "UEHitrate: 0.00588  edgeHitrate 0.26651 sumHitrate 0.2724  privacy: 0.15488\n",
      "\n",
      "--Time: Mon Sep 20 18:29:00 2021   Index: 280000   Loss: 0.0 --\n",
      "Reward: [3.43447 0.      0.     ] total reward: 3.43447\n",
      "UEHitrate: 0.00582  edgeHitrate 0.26481 sumHitrate 0.27063  privacy: 0.14963\n",
      "\n",
      "--Time: Mon Sep 20 18:29:42 2021   Index: 290000   Loss: 0.0 --\n",
      "Reward: [3.56993 0.      0.     ] total reward: 3.56993\n",
      "UEHitrate: 0.00576  edgeHitrate 0.26472 sumHitrate 0.27047  privacy: 0.14678\n",
      "\n",
      "--Time: Mon Sep 20 18:30:27 2021   Index: 300000   Loss: 0.0 --\n",
      "Reward: [3.68668 0.      0.     ] total reward: 3.68668\n",
      "UEHitrate: 0.00569  edgeHitrate 0.26434 sumHitrate 0.27003  privacy: 0.14916\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 18:30:33 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [3.69903 0.      0.     ] total reward: 3.69903\n",
      "UEHitrate: 0.00569  edgeHitrate 0.26437 sumHitrate 0.27006  privacy: 0.14983\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.     , 0.35486, 0.44033, 0.42332, 0.39999, 0.38713, 0.37559,\n",
       "        0.36134, 0.35246, 0.34377, 0.33529, 0.32755, 0.32054, 0.31493,\n",
       "        0.31257, 0.30974, 0.30296, 0.30044, 0.29884, 0.2959 , 0.29315,\n",
       "        0.29218, 0.28974, 0.28564, 0.28117, 0.27813, 0.27533, 0.2724 ,\n",
       "        0.27063, 0.27047, 0.27006]),\n",
       " array([0.     , 0.0112 , 0.0134 , 0.01207, 0.01087, 0.01036, 0.00962,\n",
       "        0.00897, 0.00855, 0.00811, 0.00789, 0.00755, 0.0073 , 0.00719,\n",
       "        0.00705, 0.00691, 0.00672, 0.00658, 0.00645, 0.00638, 0.00623,\n",
       "        0.00622, 0.00614, 0.00609, 0.006  , 0.00595, 0.0059 , 0.00588,\n",
       "        0.00582, 0.00576, 0.00569]),\n",
       " array([0.     , 0.34367, 0.42693, 0.41125, 0.38912, 0.37677, 0.36598,\n",
       "        0.35237, 0.34391, 0.33566, 0.3274 , 0.32   , 0.31324, 0.30774,\n",
       "        0.30552, 0.30283, 0.29624, 0.29386, 0.29239, 0.28952, 0.28691,\n",
       "        0.28596, 0.2836 , 0.27955, 0.27518, 0.27218, 0.26943, 0.26651,\n",
       "        0.26481, 0.26472, 0.26437]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.02118, 1.48083, 1.08159, 0.87571, 0.79483, 0.72363, 0.66947,\n",
       "       0.62904, 0.60364, 0.55926, 0.52071, 0.49438, 0.44702, 0.40594,\n",
       "       0.37007, 0.34073, 0.32011, 0.29921, 0.2795 , 0.24684, 0.23461,\n",
       "       0.21337, 0.19268, 0.17061, 0.16147, 0.15841, 0.15825, 0.15488,\n",
       "       0.14963, 0.14678, 0.14983])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "py38_ML",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
