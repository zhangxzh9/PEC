{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8812</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4058</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1549</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148          11            21     0   \n",
       "1       203  5779    0        0         7          11             4     0   \n",
       "2       208  4675    0        0        92          13             4     0   \n",
       "3       159   332    0        0        56          11             3     0   \n",
       "4        50   674    0        0       439          11             4     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831   29  2591880        34          11             4     0   \n",
       "300979  158  8448   29  2591880        34          11             4     0   \n",
       "300980  483  6463   29  2591940        35          11             4     0   \n",
       "300981  158  4715   29  2591940        34          11             4     0   \n",
       "300982  483  2021   29  2591940        34          11             4     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0       8812          1            0  \n",
       "1              0       9063          1            0  \n",
       "2              0       3444          1            0  \n",
       "3              0       4058          1            0  \n",
       "4              0       1549          1            0  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0       7061          1            0  \n",
       "300979         0      11316          1            0  \n",
       "300980         0       7061          1            0  \n",
       "300981         0      11316          1            0  \n",
       "300982         0       7061          1            0  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "validUIT = UIT[UIT['day']<24]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT,validUIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000,\n",
       " 500,\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148          11            21     0   \n",
       " 1       203  5779    0        0         7          11             4     0   \n",
       " 2       208  4675    0        0        92          13             4     0   \n",
       " 3       159   332    0        0        56          11             3     0   \n",
       " 4        50   674    0        0       439          11             4     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 198170  264  7442   17  1555140        90          13             4     0   \n",
       " 198171   19  9362   17  1555140       424          13             4     0   \n",
       " 198172   82  9223   17  1555140        94          15             4     0   \n",
       " 198173   35  4164   17  1555140        22          11             4     0   \n",
       " 198174  239  5062   17  1555140        89          13             4     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0       8812          1            0  \n",
       " 1              0       9063          1            0  \n",
       " 2              0       3444          1            0  \n",
       " 3              0       4058          1            0  \n",
       " 4              0       1549          1            0  \n",
       " ...          ...        ...        ...          ...  \n",
       " 198170         0       5639          1            0  \n",
       " 198171         0       4004          1            0  \n",
       " 198172         0       8398          1            0  \n",
       " 198173         0       3934          1            0  \n",
       " 198174         0      11717          1            0  \n",
       " \n",
       " [198175 rows x 12 columns],\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148          11            21     0   \n",
       " 1       203  5779    0        0         7          11             4     0   \n",
       " 2       208  4675    0        0        92          13             4     0   \n",
       " 3       159   332    0        0        56          11             3     0   \n",
       " 4        50   674    0        0       439          11             4     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 253625   57  8385   23  2073540        26          11             4     0   \n",
       " 253626   74  9218   23  2073540       601          20             4     0   \n",
       " 253627    7  2757   23  2073540       133          11             4     0   \n",
       " 253628  240  3252   23  2073540        70           4             1     0   \n",
       " 253629   76  7213   23  2073540        34          11             3     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0       8812          1            0  \n",
       " 1              0       9063          1            0  \n",
       " 2              0       3444          1            0  \n",
       " 3              0       4058          1            0  \n",
       " 4              0       1549          1            0  \n",
       " ...          ...        ...        ...          ...  \n",
       " 253625         0       1903          1            0  \n",
       " 253626         0       4944          1            0  \n",
       " 253627         0       5436          1            0  \n",
       " 253628         0       6608          1            0  \n",
       " 253629         0        571          1            0  \n",
       " \n",
       " [253630 rows x 12 columns])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    " \n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)).sum() - torch.log(v * p + (1-v) * (1-p)).sum())\n",
    "        self.Rh = self.ALPHAh * ( 1 / ( 1 + torch.exp( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "\n",
    "        if self.W[-1] not in np.argwhere(self.lastAction.numpy()): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            if  not train or (train and sample > eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                with torch.no_grad():\n",
    "                    Q_value = QNetwork(self.statusFeature)\n",
    "                    actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                QNetwork.train()\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "\n",
    "            self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "            self.action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                self.action[index] = 1\n",
    "            env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "        else:\n",
    "            self.action = self.lastAction # keep the cache and no request the new video\n",
    "\n",
    "        if train:\n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                        lastAction.to(device), \n",
    "                        self.statusFeature,\n",
    "                        torch.tensor([self.reward.float()]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "                        \n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName+'dnn_h1_exp_'\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 15\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 50000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "\n",
    "        if index % 50000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sat Oct  9 14:16:12 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 14:23:46 2021 Episode: 0   Index: 50000   Loss: 0.31138 --\n",
      "Reward: [0.68656 0.00499 0.0688 ] total reward: 0.76035\n",
      "UEHitrate: 0.00608  edgeHitrate 0.086 sumHitrate 0.09208  privacy: 2.59524\n",
      "\n",
      "--Time: Sat Oct  9 14:34:46 2021 Episode: 0   Index: 100000   Loss: 0.28896 --\n",
      "Reward: [0.63533 0.00475 0.06412] total reward: 0.7042\n",
      "UEHitrate: 0.00567  edgeHitrate 0.08015 sumHitrate 0.08582  privacy: 1.92335\n",
      "\n",
      "--Time: Sat Oct  9 14:45:57 2021 Episode: 0   Index: 150000   Loss: 0.27445 --\n",
      "Reward: [0.60361 0.00517 0.06272] total reward: 0.6715\n",
      "UEHitrate: 0.00618  edgeHitrate 0.07841 sumHitrate 0.08459  privacy: 1.49094\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 14:57:01 2021 Episode: 0   Index: 198174   Loss: 0.26308 --\n",
      "Reward: [0.5851  0.00546 0.06255] total reward: 0.6531\n",
      "UEHitrate: 0.00647  edgeHitrate 0.07818 sumHitrate 0.08466  privacy: 1.17505\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 14:57:01 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 15:01:57 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.82651 0.00425 0.20372] total reward: 1.03449\n",
      "UEHitrate: 0.00822  edgeHitrate 0.25465 sumHitrate 0.26287  privacy: 2.02317\n",
      "\n",
      "--Time: Sat Oct  9 15:06:47 2021 Episode: 0   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.82004 0.0041  0.18843] total reward: 1.01257\n",
      "UEHitrate: 0.0075  edgeHitrate 0.23554 sumHitrate 0.24304  privacy: 1.5724\n",
      "\n",
      "--Time: Sat Oct  9 15:11:51 2021 Episode: 0   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.8054  0.00443 0.18131] total reward: 0.99114\n",
      "UEHitrate: 0.00772  edgeHitrate 0.22663 sumHitrate 0.23435  privacy: 1.27118\n",
      "\n",
      "--Time: Sat Oct  9 15:17:17 2021 Episode: 0   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.78464 0.00461 0.18009] total reward: 0.96934\n",
      "UEHitrate: 0.00769  edgeHitrate 0.22511 sumHitrate 0.2328  privacy: 1.00822\n",
      "\n",
      "--Time: Sat Oct  9 15:23:13 2021 Episode: 0   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.75498 0.0047  0.17769] total reward: 0.93736\n",
      "UEHitrate: 0.00764  edgeHitrate 0.22211 sumHitrate 0.22975  privacy: 0.76386\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 15:23:37 2021 Episode: 0   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.75348 0.00467 0.17734] total reward: 0.93548\n",
      "UEHitrate: 0.0076  edgeHitrate 0.22167 sumHitrate 0.22927  privacy: 0.74832\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h1_exp_ep0_1009-15-23-37\n",
      "\n",
      "--Time: Sat Oct  9 15:23:37 2021 Episode: 1   Index: 0   Loss: 8.04315 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 15:37:03 2021 Episode: 1   Index: 50000   Loss: 0.19536 --\n",
      "Reward: [0.71044 0.00457 0.07534] total reward: 0.79036\n",
      "UEHitrate: 0.00556  edgeHitrate 0.09418 sumHitrate 0.09974  privacy: 2.53387\n",
      "\n",
      "--Time: Sat Oct  9 15:48:52 2021 Episode: 1   Index: 100000   Loss: 0.18649 --\n",
      "Reward: [0.65783 0.00441 0.06816] total reward: 0.7304\n",
      "UEHitrate: 0.00531  edgeHitrate 0.0852 sumHitrate 0.09051  privacy: 1.86777\n",
      "\n",
      "--Time: Sat Oct  9 16:00:33 2021 Episode: 1   Index: 150000   Loss: 0.17919 --\n",
      "Reward: [0.62549 0.0049  0.0676 ] total reward: 0.69799\n",
      "UEHitrate: 0.00593  edgeHitrate 0.08451 sumHitrate 0.09044  privacy: 1.43185\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 16:12:33 2021 Episode: 1   Index: 198174   Loss: 0.17147 --\n",
      "Reward: [0.59869 0.00609 0.0709 ] total reward: 0.67568\n",
      "UEHitrate: 0.00879  edgeHitrate 0.08863 sumHitrate 0.09742  privacy: 1.22274\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 16:12:33 2021 Episode: 1   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 16:17:14 2021 Episode: 1   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54895 0.02685 0.27462] total reward: 0.85042\n",
      "UEHitrate: 0.05202  edgeHitrate 0.34327 sumHitrate 0.39529  privacy: 1.41594\n",
      "\n",
      "--Time: Sat Oct  9 16:22:09 2021 Episode: 1   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.53092 0.02758 0.22107] total reward: 0.77957\n",
      "UEHitrate: 0.05033  edgeHitrate 0.27634 sumHitrate 0.32667  privacy: 1.26712\n",
      "\n",
      "--Time: Sat Oct  9 16:27:13 2021 Episode: 1   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.5224  0.0272  0.20244] total reward: 0.75204\n",
      "UEHitrate: 0.04912  edgeHitrate 0.25305 sumHitrate 0.30217  privacy: 1.19905\n",
      "\n",
      "--Time: Sat Oct  9 16:32:53 2021 Episode: 1   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.51735 0.02561 0.17913] total reward: 0.72209\n",
      "UEHitrate: 0.04532  edgeHitrate 0.22391 sumHitrate 0.26923  privacy: 1.16077\n",
      "\n",
      "--Time: Sat Oct  9 16:39:03 2021 Episode: 1   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.51372 0.02481 0.16272] total reward: 0.70126\n",
      "UEHitrate: 0.04322  edgeHitrate 0.2034 sumHitrate 0.24662  privacy: 1.13549\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 16:39:29 2021 Episode: 1   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.51354 0.02482 0.16154] total reward: 0.69989\n",
      "UEHitrate: 0.04312  edgeHitrate 0.20192 sumHitrate 0.24504  privacy: 1.13393\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 16:39:29 2021 Episode: 2   Index: 0   Loss: 6.14198 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 16:53:27 2021 Episode: 2   Index: 50000   Loss: 0.17913 --\n",
      "Reward: [0.64514 0.00603 0.0803 ] total reward: 0.73147\n",
      "UEHitrate: 0.00768  edgeHitrate 0.10038 sumHitrate 0.10806  privacy: 2.77527\n",
      "\n",
      "--Time: Sat Oct  9 17:06:05 2021 Episode: 2   Index: 100000   Loss: 0.17036 --\n",
      "Reward: [0.6     0.00705 0.07409] total reward: 0.68114\n",
      "UEHitrate: 0.00922  edgeHitrate 0.09261 sumHitrate 0.10183  privacy: 2.35816\n",
      "\n",
      "--Time: Sat Oct  9 17:17:42 2021 Episode: 2   Index: 150000   Loss: 0.16116 --\n",
      "Reward: [0.57385 0.01026 0.08563] total reward: 0.66974\n",
      "UEHitrate: 0.01509  edgeHitrate 0.10704 sumHitrate 0.12213  privacy: 2.09799\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 17:29:29 2021 Episode: 2   Index: 198174   Loss: 0.15532 --\n",
      "Reward: [0.55821 0.01104 0.08157] total reward: 0.65082\n",
      "UEHitrate: 0.01588  edgeHitrate 0.10197 sumHitrate 0.11785  privacy: 1.90063\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 17:29:30 2021 Episode: 2   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 17:34:15 2021 Episode: 2   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54868 0.02708 0.24888] total reward: 0.82463\n",
      "UEHitrate: 0.04946  edgeHitrate 0.31109 sumHitrate 0.36055  privacy: 1.41234\n",
      "\n",
      "--Time: Sat Oct  9 17:38:58 2021 Episode: 2   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.53073 0.02733 0.19185] total reward: 0.74991\n",
      "UEHitrate: 0.04647  edgeHitrate 0.23982 sumHitrate 0.28629  privacy: 1.26891\n",
      "\n",
      "--Time: Sat Oct  9 17:43:43 2021 Episode: 2   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.52221 0.02722 0.17669] total reward: 0.72612\n",
      "UEHitrate: 0.04623  edgeHitrate 0.22086 sumHitrate 0.26709  privacy: 1.20049\n",
      "\n",
      "--Time: Sat Oct  9 17:48:35 2021 Episode: 2   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.5172  0.02575 0.16176] total reward: 0.70471\n",
      "UEHitrate: 0.04305  edgeHitrate 0.2022 sumHitrate 0.24526  privacy: 1.16243\n",
      "\n",
      "--Time: Sat Oct  9 17:54:42 2021 Episode: 2   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.51359 0.02518 0.15031] total reward: 0.68909\n",
      "UEHitrate: 0.04149  edgeHitrate 0.18789 sumHitrate 0.22938  privacy: 1.13695\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 17:55:08 2021 Episode: 2   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.51341 0.0251  0.14911] total reward: 0.68762\n",
      "UEHitrate: 0.0413  edgeHitrate 0.18639 sumHitrate 0.22769  privacy: 1.13538\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 17:55:09 2021 Episode: 3   Index: 0   Loss: 5.67256 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 18:10:00 2021 Episode: 3   Index: 50000   Loss: 0.16762 --\n",
      "Reward: [0.62431 0.02087 0.17929] total reward: 0.82448\n",
      "UEHitrate: 0.0354  edgeHitrate 0.22412 sumHitrate 0.25951  privacy: 2.7283\n",
      "\n",
      "--Time: Sat Oct  9 18:22:28 2021 Episode: 3   Index: 100000   Loss: 0.16323 --\n",
      "Reward: [0.58578 0.02016 0.14455] total reward: 0.75049\n",
      "UEHitrate: 0.03258  edgeHitrate 0.18069 sumHitrate 0.21327  privacy: 2.42683\n",
      "\n",
      "--Time: Sat Oct  9 18:35:10 2021 Episode: 3   Index: 150000   Loss: 0.15828 --\n",
      "Reward: [0.56354 0.0202  0.13722] total reward: 0.72095\n",
      "UEHitrate: 0.0333  edgeHitrate 0.17152 sumHitrate 0.20482  privacy: 2.23125\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 18:47:09 2021 Episode: 3   Index: 198174   Loss: 0.15137 --\n",
      "Reward: [0.55011 0.01929 0.12572] total reward: 0.69512\n",
      "UEHitrate: 0.0311  edgeHitrate 0.15715 sumHitrate 0.18825  privacy: 2.07244\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 18:47:09 2021 Episode: 3   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 18:52:05 2021 Episode: 3   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54451 0.0275  0.25065] total reward: 0.82267\n",
      "UEHitrate: 0.0496  edgeHitrate 0.31331 sumHitrate 0.36291  privacy: 1.29362\n",
      "\n",
      "--Time: Sat Oct  9 18:56:43 2021 Episode: 3   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.52857 0.02804 0.19489] total reward: 0.75151\n",
      "UEHitrate: 0.04718  edgeHitrate 0.24362 sumHitrate 0.2908  privacy: 1.19574\n",
      "\n",
      "--Time: Sat Oct  9 19:01:03 2021 Episode: 3   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.52078 0.02757 0.18054] total reward: 0.72888\n",
      "UEHitrate: 0.04679  edgeHitrate 0.22568 sumHitrate 0.27246  privacy: 1.14481\n",
      "\n",
      "--Time: Sat Oct  9 19:05:48 2021 Episode: 3   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.51614 0.02622 0.16569] total reward: 0.70804\n",
      "UEHitrate: 0.04396  edgeHitrate 0.20711 sumHitrate 0.25108  privacy: 1.11728\n",
      "\n",
      "--Time: Sat Oct  9 19:10:55 2021 Episode: 3   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.51274 0.02581 0.15394] total reward: 0.6925\n",
      "UEHitrate: 0.0425  edgeHitrate 0.19243 sumHitrate 0.23493  privacy: 1.09912\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 19:11:21 2021 Episode: 3   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.51257 0.02574 0.15273] total reward: 0.69104\n",
      "UEHitrate: 0.04233  edgeHitrate 0.19092 sumHitrate 0.23325  privacy: 1.09796\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 19:11:21 2021 Episode: 4   Index: 0   Loss: 5.16108 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 19:26:16 2021 Episode: 4   Index: 50000   Loss: 0.16729 --\n",
      "Reward: [0.60382 0.01933 0.14597] total reward: 0.76911\n",
      "UEHitrate: 0.0288  edgeHitrate 0.18246 sumHitrate 0.21126  privacy: 2.60043\n",
      "\n",
      "--Time: Sat Oct  9 19:39:43 2021 Episode: 4   Index: 100000   Loss: 0.15186 --\n",
      "Reward: [0.57332 0.0193  0.11288] total reward: 0.7055\n",
      "UEHitrate: 0.0279  edgeHitrate 0.1411 sumHitrate 0.169  privacy: 2.41561\n",
      "\n",
      "--Time: Sat Oct  9 19:52:40 2021 Episode: 4   Index: 150000   Loss: 0.14142 --\n",
      "Reward: [0.55462 0.01948 0.11552] total reward: 0.68961\n",
      "UEHitrate: 0.02937  edgeHitrate 0.1444 sumHitrate 0.17377  privacy: 2.28797\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 20:04:26 2021 Episode: 4   Index: 198174   Loss: 0.13787 --\n",
      "Reward: [0.54319 0.01789 0.10827] total reward: 0.66935\n",
      "UEHitrate: 0.02678  edgeHitrate 0.13533 sumHitrate 0.16211  privacy: 2.17283\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 20:04:26 2021 Episode: 4   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 20:09:15 2021 Episode: 4   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54112 0.02304 0.25518] total reward: 0.81934\n",
      "UEHitrate: 0.04446  edgeHitrate 0.31897 sumHitrate 0.36343  privacy: 1.19886\n",
      "\n",
      "--Time: Sat Oct  9 20:13:57 2021 Episode: 4   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.52662 0.02446 0.20489] total reward: 0.75597\n",
      "UEHitrate: 0.04444  edgeHitrate 0.25612 sumHitrate 0.30056  privacy: 1.11987\n",
      "\n",
      "--Time: Sat Oct  9 20:18:40 2021 Episode: 4   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.51947 0.0245  0.19273] total reward: 0.7367\n",
      "UEHitrate: 0.04412  edgeHitrate 0.24091 sumHitrate 0.28503  privacy: 1.08471\n",
      "\n",
      "--Time: Sat Oct  9 20:23:35 2021 Episode: 4   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.51515 0.02357 0.17762] total reward: 0.71634\n",
      "UEHitrate: 0.04181  edgeHitrate 0.22202 sumHitrate 0.26384  privacy: 1.06663\n",
      "\n",
      "--Time: Sat Oct  9 20:29:24 2021 Episode: 4   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.51194 0.02315 0.16592] total reward: 0.70101\n",
      "UEHitrate: 0.04083  edgeHitrate 0.2074 sumHitrate 0.24823  privacy: 1.05543\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 20:29:49 2021 Episode: 4   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.51178 0.02306 0.16478] total reward: 0.69962\n",
      "UEHitrate: 0.04067  edgeHitrate 0.20597 sumHitrate 0.24663  privacy: 1.05486\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 20:29:49 2021 Episode: 5   Index: 0   Loss: 4.64846 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 20:45:24 2021 Episode: 5   Index: 50000   Loss: 0.1602 --\n",
      "Reward: [0.58957 0.01811 0.12043] total reward: 0.72811\n",
      "UEHitrate: 0.0264  edgeHitrate 0.15054 sumHitrate 0.17694  privacy: 2.44717\n",
      "\n",
      "--Time: Sat Oct  9 20:58:46 2021 Episode: 5   Index: 100000   Loss: 0.15526 --\n",
      "Reward: [0.56373 0.01838 0.09986] total reward: 0.68196\n",
      "UEHitrate: 0.02621  edgeHitrate 0.12482 sumHitrate 0.15103  privacy: 2.34039\n",
      "\n",
      "--Time: Sat Oct  9 21:11:49 2021 Episode: 5   Index: 150000   Loss: 0.14412 --\n",
      "Reward: [0.54809 0.0188  0.10008] total reward: 0.66697\n",
      "UEHitrate: 0.02767  edgeHitrate 0.12511 sumHitrate 0.15277  privacy: 2.27171\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 21:24:15 2021 Episode: 5   Index: 198174   Loss: 0.13588 --\n",
      "Reward: [0.53824 0.01847 0.09967] total reward: 0.65638\n",
      "UEHitrate: 0.02729  edgeHitrate 0.12458 sumHitrate 0.15188  privacy: 2.19503\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 21:24:15 2021 Episode: 5   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 21:28:58 2021 Episode: 5   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54095 0.02743 0.24888] total reward: 0.81725\n",
      "UEHitrate: 0.05036  edgeHitrate 0.31109 sumHitrate 0.36145  privacy: 1.20013\n",
      "\n",
      "--Time: Sat Oct  9 21:33:46 2021 Episode: 5   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.52655 0.02713 0.19455] total reward: 0.74824\n",
      "UEHitrate: 0.04711  edgeHitrate 0.24319 sumHitrate 0.2903  privacy: 1.12121\n",
      "\n",
      "--Time: Sat Oct  9 21:38:32 2021 Episode: 5   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.51939 0.02713 0.18179] total reward: 0.72831\n",
      "UEHitrate: 0.04686  edgeHitrate 0.22724 sumHitrate 0.2741  privacy: 1.08567\n",
      "\n",
      "--Time: Sat Oct  9 21:43:19 2021 Episode: 5   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.5151  0.02585 0.16824] total reward: 0.70919\n",
      "UEHitrate: 0.04418  edgeHitrate 0.21029 sumHitrate 0.25447  privacy: 1.06774\n",
      "\n",
      "--Time: Sat Oct  9 21:48:08 2021 Episode: 5   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.51191 0.02543 0.15641] total reward: 0.69375\n",
      "UEHitrate: 0.04294  edgeHitrate 0.19552 sumHitrate 0.23846  privacy: 1.05613\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 21:48:32 2021 Episode: 5   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.51175 0.02538 0.15524] total reward: 0.69237\n",
      "UEHitrate: 0.04281  edgeHitrate 0.19405 sumHitrate 0.23686  privacy: 1.05554\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 21:48:33 2021 Episode: 6   Index: 0   Loss: 3.74403 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 22:04:16 2021 Episode: 6   Index: 50000   Loss: 0.11741 --\n",
      "Reward: [0.57695 0.02267 0.18224] total reward: 0.78185\n",
      "UEHitrate: 0.0375  edgeHitrate 0.2278 sumHitrate 0.26529  privacy: 2.28029\n",
      "\n",
      "--Time: Sat Oct  9 22:17:13 2021 Episode: 6   Index: 100000   Loss: 0.11453 --\n",
      "Reward: [0.55542 0.02159 0.13186] total reward: 0.70887\n",
      "UEHitrate: 0.03293  edgeHitrate 0.16483 sumHitrate 0.19776  privacy: 2.22808\n",
      "\n",
      "--Time: Sat Oct  9 22:29:43 2021 Episode: 6   Index: 150000   Loss: 0.11259 --\n",
      "Reward: [0.54181 0.02207 0.1357 ] total reward: 0.69958\n",
      "UEHitrate: 0.03507  edgeHitrate 0.16962 sumHitrate 0.20469  privacy: 2.19991\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 22:42:25 2021 Episode: 6   Index: 198174   Loss: 0.11105 --\n",
      "Reward: [0.53325 0.02143 0.12962] total reward: 0.68429\n",
      "UEHitrate: 0.03385  edgeHitrate 0.16202 sumHitrate 0.19588  privacy: 2.1584\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 22:42:25 2021 Episode: 6   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 22:47:12 2021 Episode: 6   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54095 0.02657 0.23036] total reward: 0.79789\n",
      "UEHitrate: 0.04534  edgeHitrate 0.28795 sumHitrate 0.33329  privacy: 1.20038\n",
      "\n",
      "--Time: Sat Oct  9 22:51:29 2021 Episode: 6   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.52655 0.02581 0.16944] total reward: 0.72179\n",
      "UEHitrate: 0.04072  edgeHitrate 0.2118 sumHitrate 0.25252  privacy: 1.12135\n",
      "\n",
      "--Time: Sat Oct  9 22:56:15 2021 Episode: 6   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.51939 0.02512 0.15443] total reward: 0.69893\n",
      "UEHitrate: 0.03954  edgeHitrate 0.19304 sumHitrate 0.23258  privacy: 1.0858\n",
      "\n",
      "--Time: Sat Oct  9 23:01:15 2021 Episode: 6   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.51509 0.02382 0.13951] total reward: 0.67842\n",
      "UEHitrate: 0.03666  edgeHitrate 0.17438 sumHitrate 0.21105  privacy: 1.06765\n",
      "\n",
      "--Time: Sat Oct  9 23:06:59 2021 Episode: 6   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.5119  0.02346 0.12873] total reward: 0.66408\n",
      "UEHitrate: 0.03567  edgeHitrate 0.16091 sumHitrate 0.19658  privacy: 1.05609\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 23:07:24 2021 Episode: 6   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.51174 0.02339 0.12778] total reward: 0.66291\n",
      "UEHitrate: 0.03556  edgeHitrate 0.15973 sumHitrate 0.19528  privacy: 1.05551\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 23:07:24 2021 Episode: 7   Index: 0   Loss: 3.21591 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 23:22:39 2021 Episode: 7   Index: 50000   Loss: 0.11142 --\n",
      "Reward: [0.5654  0.02439 0.20952] total reward: 0.7993\n",
      "UEHitrate: 0.04074  edgeHitrate 0.26189 sumHitrate 0.30263  privacy: 2.10035\n",
      "\n",
      "--Time: Sat Oct  9 23:36:09 2021 Episode: 7   Index: 100000   Loss: 0.11002 --\n",
      "Reward: [0.54808 0.02243 0.16122] total reward: 0.73173\n",
      "UEHitrate: 0.03599  edgeHitrate 0.20153 sumHitrate 0.23752  privacy: 2.0862\n",
      "\n",
      "--Time: Sat Oct  9 23:49:10 2021 Episode: 7   Index: 150000   Loss: 0.10916 --\n",
      "Reward: [0.53638 0.02179 0.1503 ] total reward: 0.70847\n",
      "UEHitrate: 0.0351  edgeHitrate 0.18787 sumHitrate 0.22297  privacy: 2.08797\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 00:01:50 2021 Episode: 7   Index: 198174   Loss: 0.10844 --\n",
      "Reward: [0.52897 0.02043 0.14053] total reward: 0.68992\n",
      "UEHitrate: 0.03255  edgeHitrate 0.17566 sumHitrate 0.20821  privacy: 2.07097\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 00:01:50 2021 Episode: 7   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 00:06:36 2021 Episode: 7   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54097 0.02443 0.21275] total reward: 0.77815\n",
      "UEHitrate: 0.04032  edgeHitrate 0.26593 sumHitrate 0.30625  privacy: 1.20027\n",
      "\n",
      "--Time: Sun Oct 10 00:11:15 2021 Episode: 7   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.52656 0.02391 0.15893] total reward: 0.7094\n",
      "UEHitrate: 0.03718  edgeHitrate 0.19867 sumHitrate 0.23585  privacy: 1.1212\n",
      "\n",
      "--Time: Sun Oct 10 00:16:05 2021 Episode: 7   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.5194  0.02334 0.14711] total reward: 0.68986\n",
      "UEHitrate: 0.0366  edgeHitrate 0.18389 sumHitrate 0.22049  privacy: 1.08528\n",
      "\n",
      "--Time: Sun Oct 10 00:21:27 2021 Episode: 7   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.5151  0.02215 0.13488] total reward: 0.67213\n",
      "UEHitrate: 0.03411  edgeHitrate 0.16859 sumHitrate 0.20271  privacy: 1.0672\n",
      "\n",
      "--Time: Sun Oct 10 00:26:54 2021 Episode: 7   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.51192 0.02176 0.12547] total reward: 0.65915\n",
      "UEHitrate: 0.03312  edgeHitrate 0.15684 sumHitrate 0.18996  privacy: 1.05573\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 00:27:18 2021 Episode: 7   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.51176 0.0217  0.12444] total reward: 0.6579\n",
      "UEHitrate: 0.03303  edgeHitrate 0.15555 sumHitrate 0.18858  privacy: 1.05515\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 00:27:19 2021 Episode: 8   Index: 0   Loss: 2.80332 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 00:41:51 2021 Episode: 8   Index: 50000   Loss: 0.11703 --\n",
      "Reward: [0.55795 0.02074 0.17872] total reward: 0.75741\n",
      "UEHitrate: 0.03732  edgeHitrate 0.2234 sumHitrate 0.26071  privacy: 1.9445\n",
      "\n",
      "--Time: Sun Oct 10 00:55:53 2021 Episode: 8   Index: 100000   Loss: 0.11488 --\n",
      "Reward: [0.54216 0.01921 0.15017] total reward: 0.71155\n",
      "UEHitrate: 0.03422  edgeHitrate 0.18772 sumHitrate 0.22194  privacy: 1.93454\n",
      "\n",
      "--Time: Sun Oct 10 01:08:37 2021 Episode: 8   Index: 150000   Loss: 0.11361 --\n",
      "Reward: [0.53195 0.01877 0.15287] total reward: 0.70358\n",
      "UEHitrate: 0.03409  edgeHitrate 0.19109 sumHitrate 0.22518  privacy: 1.95956\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 01:20:28 2021 Episode: 8   Index: 198174   Loss: 0.11234 --\n",
      "Reward: [0.52553 0.0184  0.16184] total reward: 0.70578\n",
      "UEHitrate: 0.03391  edgeHitrate 0.20231 sumHitrate 0.23622  privacy: 1.96463\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 01:20:28 2021 Episode: 8   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 01:25:16 2021 Episode: 8   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.54124 0.02087 0.18393] total reward: 0.74605\n",
      "UEHitrate: 0.04132  edgeHitrate 0.22992 sumHitrate 0.27123  privacy: 1.20833\n",
      "\n",
      "--Time: Sun Oct 10 01:29:55 2021 Episode: 8   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.52671 0.01936 0.16348] total reward: 0.70955\n",
      "UEHitrate: 0.03763  edgeHitrate 0.20435 sumHitrate 0.24198  privacy: 1.12577\n",
      "\n",
      "--Time: Sun Oct 10 01:34:37 2021 Episode: 8   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.5195  0.01933 0.16817] total reward: 0.70701\n",
      "UEHitrate: 0.03686  edgeHitrate 0.21021 sumHitrate 0.24707  privacy: 1.08842\n",
      "\n",
      "--Time: Sun Oct 10 01:38:58 2021 Episode: 8   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.51518 0.01862 0.17184] total reward: 0.70564\n",
      "UEHitrate: 0.03487  edgeHitrate 0.2148 sumHitrate 0.24967  privacy: 1.07011\n",
      "\n",
      "--Time: Sun Oct 10 01:43:25 2021 Episode: 8   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.51197 0.0184  0.17009] total reward: 0.70046\n",
      "UEHitrate: 0.0342  edgeHitrate 0.21261 sumHitrate 0.2468  privacy: 1.05821\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 01:43:52 2021 Episode: 8   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.51181 0.01828 0.16951] total reward: 0.6996\n",
      "UEHitrate: 0.034  edgeHitrate 0.21188 sumHitrate 0.24589  privacy: 1.0576\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 01:43:52 2021 Episode: 9   Index: 0   Loss: 2.22893 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 01:59:35 2021 Episode: 9   Index: 50000   Loss: 0.11804 --\n",
      "Reward: [0.55335 0.02062 0.16484] total reward: 0.73881\n",
      "UEHitrate: 0.04342  edgeHitrate 0.20606 sumHitrate 0.24948  privacy: 1.87164\n",
      "\n",
      "--Time: Sun Oct 10 02:14:10 2021 Episode: 9   Index: 100000   Loss: 0.11644 --\n",
      "Reward: [0.53884 0.01947 0.15542] total reward: 0.71373\n",
      "UEHitrate: 0.04006  edgeHitrate 0.19428 sumHitrate 0.23434  privacy: 1.86157\n",
      "\n",
      "--Time: Sun Oct 10 02:26:39 2021 Episode: 9   Index: 150000   Loss: 0.12278 --\n",
      "Reward: [0.52945 0.01959 0.1717 ] total reward: 0.72075\n",
      "UEHitrate: 0.0484  edgeHitrate 0.21463 sumHitrate 0.26302  privacy: 1.87629\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 02:39:08 2021 Episode: 9   Index: 198174   Loss: 0.12488 --\n",
      "Reward: [0.52348 0.0201  0.20673] total reward: 0.75031\n",
      "UEHitrate: 0.05405  edgeHitrate 0.25841 sumHitrate 0.31246  privacy: 1.87567\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 02:39:08 2021 Episode: 9   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 02:43:52 2021 Episode: 9   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.6259  0.02683 0.28252] total reward: 0.93525\n",
      "UEHitrate: 0.05368  edgeHitrate 0.35315 sumHitrate 0.40683  privacy: 2.51263\n",
      "\n",
      "--Time: Sun Oct 10 02:48:21 2021 Episode: 9   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.56835 0.02822 0.30275] total reward: 0.89932\n",
      "UEHitrate: 0.06088  edgeHitrate 0.37844 sumHitrate 0.43932  privacy: 2.18796\n",
      "\n",
      "--Time: Sun Oct 10 02:53:01 2021 Episode: 9   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.54523 0.02797 0.32187] total reward: 0.89507\n",
      "UEHitrate: 0.06345  edgeHitrate 0.40234 sumHitrate 0.46579  privacy: 2.01988\n",
      "\n",
      "--Time: Sun Oct 10 02:57:46 2021 Episode: 9   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.53371 0.02662 0.33592] total reward: 0.89624\n",
      "UEHitrate: 0.06341  edgeHitrate 0.4199 sumHitrate 0.48331  privacy: 1.908\n",
      "\n",
      "--Time: Sun Oct 10 03:02:51 2021 Episode: 9   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.52624 0.02612 0.33948] total reward: 0.89184\n",
      "UEHitrate: 0.06375  edgeHitrate 0.42435 sumHitrate 0.4881  privacy: 1.82036\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 03:03:14 2021 Episode: 9   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.52584 0.02602 0.3395 ] total reward: 0.89136\n",
      "UEHitrate: 0.06368  edgeHitrate 0.42437 sumHitrate 0.48805  privacy: 1.81481\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h1_exp_ep0_1009-15-23-37'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sun Oct 10 03:03:22 2021   Index: 0   Loss: 0.0 --\n",
      "Reward: [0.5 0.  0. ] total reward: 0.5\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 03:04:30 2021   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.85502 0.00318 0.24862] total reward: 1.10681\n",
      "UEHitrate: 0.0095  edgeHitrate 0.30987 sumHitrate 0.31937  privacy: 2.60748\n",
      "\n",
      "--Time: Sun Oct 10 03:05:34 2021   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.83995 0.00362 0.24119] total reward: 1.08475\n",
      "UEHitrate: 0.00725  edgeHitrate 0.30213 sumHitrate 0.30938  privacy: 2.35088\n",
      "\n",
      "--Time: Sun Oct 10 03:06:41 2021   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.83076 0.00395 0.21845] total reward: 1.05316\n",
      "UEHitrate: 0.00697  edgeHitrate 0.27399 sumHitrate 0.28096  privacy: 2.22801\n",
      "\n",
      "--Time: Sun Oct 10 03:07:55 2021   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.83003 0.00399 0.20907] total reward: 1.04309\n",
      "UEHitrate: 0.00677  edgeHitrate 0.26194 sumHitrate 0.26872  privacy: 2.11675\n",
      "\n",
      "--Time: Sun Oct 10 03:09:03 2021   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.82651 0.00425 0.20372] total reward: 1.03449\n",
      "UEHitrate: 0.00664  edgeHitrate 0.25531 sumHitrate 0.26195  privacy: 2.02317\n",
      "\n",
      "--Time: Sun Oct 10 03:10:12 2021   Index: 60000   Loss: 0.0 --\n",
      "Reward: [0.82595 0.00428 0.20094] total reward: 1.03117\n",
      "UEHitrate: 0.00635  edgeHitrate 0.25181 sumHitrate 0.25816  privacy: 1.92269\n",
      "\n",
      "--Time: Sun Oct 10 03:11:26 2021   Index: 70000   Loss: 0.0 --\n",
      "Reward: [0.82748 0.00417 0.19547] total reward: 1.02712\n",
      "UEHitrate: 0.00599  edgeHitrate 0.24505 sumHitrate 0.25104  privacy: 1.85717\n",
      "\n",
      "--Time: Sun Oct 10 03:12:39 2021   Index: 80000   Loss: 0.0 --\n",
      "Reward: [0.82535 0.00416 0.19358] total reward: 1.02308\n",
      "UEHitrate: 0.00585  edgeHitrate 0.24276 sumHitrate 0.24861  privacy: 1.76463\n",
      "\n",
      "--Time: Sun Oct 10 03:13:45 2021   Index: 90000   Loss: 0.0 --\n",
      "Reward: [0.82375 0.0042  0.19061] total reward: 1.01856\n",
      "UEHitrate: 0.00571  edgeHitrate 0.23893 sumHitrate 0.24464  privacy: 1.67848\n",
      "\n",
      "--Time: Sun Oct 10 03:14:57 2021   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.82004 0.0041  0.18843] total reward: 1.01257\n",
      "UEHitrate: 0.00554  edgeHitrate 0.23623 sumHitrate 0.24177  privacy: 1.5724\n",
      "\n",
      "--Time: Sun Oct 10 03:16:07 2021   Index: 110000   Loss: 0.0 --\n",
      "Reward: [0.81456 0.00414 0.18723] total reward: 1.00594\n",
      "UEHitrate: 0.00538  edgeHitrate 0.23484 sumHitrate 0.24023  privacy: 1.48494\n",
      "\n",
      "--Time: Sun Oct 10 03:17:18 2021   Index: 120000   Loss: 0.0 --\n",
      "Reward: [0.81212 0.00417 0.18355] total reward: 0.99984\n",
      "UEHitrate: 0.00528  edgeHitrate 0.23026 sumHitrate 0.23554  privacy: 1.43214\n",
      "\n",
      "--Time: Sun Oct 10 03:18:30 2021   Index: 130000   Loss: 0.0 --\n",
      "Reward: [0.80953 0.0043  0.18078] total reward: 0.99462\n",
      "UEHitrate: 0.00535  edgeHitrate 0.22677 sumHitrate 0.23211  privacy: 1.37793\n",
      "\n",
      "--Time: Sun Oct 10 03:19:40 2021   Index: 140000   Loss: 0.0 --\n",
      "Reward: [0.80685 0.00436 0.18115] total reward: 0.99235\n",
      "UEHitrate: 0.00539  edgeHitrate 0.22731 sumHitrate 0.23271  privacy: 1.32209\n",
      "\n",
      "--Time: Sun Oct 10 03:20:48 2021   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.8054  0.00443 0.18131] total reward: 0.99114\n",
      "UEHitrate: 0.00539  edgeHitrate 0.22756 sumHitrate 0.23295  privacy: 1.27118\n",
      "\n",
      "--Time: Sun Oct 10 03:21:50 2021   Index: 160000   Loss: 0.0 --\n",
      "Reward: [0.80079 0.00458 0.17942] total reward: 0.98479\n",
      "UEHitrate: 0.00539  edgeHitrate 0.22523 sumHitrate 0.23062  privacy: 1.20669\n",
      "\n",
      "--Time: Sun Oct 10 03:23:08 2021   Index: 170000   Loss: 0.0 --\n",
      "Reward: [0.79637 0.00458 0.18018] total reward: 0.98113\n",
      "UEHitrate: 0.00534  edgeHitrate 0.22618 sumHitrate 0.23151  privacy: 1.1508\n",
      "\n",
      "--Time: Sun Oct 10 03:24:25 2021   Index: 180000   Loss: 0.0 --\n",
      "Reward: [0.79115 0.0046  0.18135] total reward: 0.97711\n",
      "UEHitrate: 0.00522  edgeHitrate 0.22767 sumHitrate 0.23289  privacy: 1.10139\n",
      "\n",
      "--Time: Sun Oct 10 03:25:35 2021   Index: 190000   Loss: 0.0 --\n",
      "Reward: [0.78776 0.00461 0.18117] total reward: 0.97353\n",
      "UEHitrate: 0.00524  edgeHitrate 0.22739 sumHitrate 0.23263  privacy: 1.04912\n",
      "\n",
      "--Time: Sun Oct 10 03:26:49 2021   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.78464 0.00461 0.18009] total reward: 0.96934\n",
      "UEHitrate: 0.00513  edgeHitrate 0.22609 sumHitrate 0.23123  privacy: 1.00822\n",
      "\n",
      "--Time: Sun Oct 10 03:27:59 2021   Index: 210000   Loss: 0.0 --\n",
      "Reward: [0.77772 0.00465 0.18068] total reward: 0.96305\n",
      "UEHitrate: 0.00512  edgeHitrate 0.22687 sumHitrate 0.23199  privacy: 0.95159\n",
      "\n",
      "--Time: Sun Oct 10 03:29:09 2021   Index: 220000   Loss: 0.0 --\n",
      "Reward: [0.77158 0.00467 0.17977] total reward: 0.95602\n",
      "UEHitrate: 0.00507  edgeHitrate 0.22575 sumHitrate 0.23082  privacy: 0.90192\n",
      "\n",
      "--Time: Sun Oct 10 03:30:21 2021   Index: 230000   Loss: 0.0 --\n",
      "Reward: [0.7656  0.00467 0.17884] total reward: 0.94911\n",
      "UEHitrate: 0.00505  edgeHitrate 0.22459 sumHitrate 0.22964  privacy: 0.85207\n",
      "\n",
      "--Time: Sun Oct 10 03:31:35 2021   Index: 240000   Loss: 0.0 --\n",
      "Reward: [0.76051 0.0047  0.1779 ] total reward: 0.94311\n",
      "UEHitrate: 0.00505  edgeHitrate 0.22337 sumHitrate 0.22842  privacy: 0.80474\n",
      "\n",
      "--Time: Sun Oct 10 03:32:46 2021   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.75498 0.0047  0.17769] total reward: 0.93736\n",
      "UEHitrate: 0.00506  edgeHitrate 0.22311 sumHitrate 0.22817  privacy: 0.76386\n",
      "\n",
      "--Time: Sun Oct 10 03:33:58 2021   Index: 260000   Loss: 0.0 --\n",
      "Reward: [0.75073 0.00468 0.1767 ] total reward: 0.93211\n",
      "UEHitrate: 0.00507  edgeHitrate 0.22188 sumHitrate 0.22694  privacy: 0.72363\n",
      "\n",
      "--Time: Sun Oct 10 03:35:06 2021   Index: 270000   Loss: 0.0 --\n",
      "Reward: [0.74608 0.00471 0.17563] total reward: 0.92642\n",
      "UEHitrate: 0.00507  edgeHitrate 0.22054 sumHitrate 0.22561  privacy: 0.69054\n",
      "\n",
      "--Time: Sun Oct 10 03:36:16 2021   Index: 280000   Loss: 0.0 --\n",
      "Reward: [0.74238 0.00472 0.17573] total reward: 0.92283\n",
      "UEHitrate: 0.00504  edgeHitrate 0.22066 sumHitrate 0.22571  privacy: 0.65739\n",
      "\n",
      "--Time: Sun Oct 10 03:37:25 2021   Index: 290000   Loss: 0.0 --\n",
      "Reward: [0.73858 0.00472 0.17753] total reward: 0.92082\n",
      "UEHitrate: 0.00502  edgeHitrate 0.22293 sumHitrate 0.22794  privacy: 0.62339\n",
      "\n",
      "--Time: Sun Oct 10 03:38:34 2021   Index: 300000   Loss: 0.0 --\n",
      "Reward: [0.7342  0.00469 0.17968] total reward: 0.91857\n",
      "UEHitrate: 0.00499  edgeHitrate 0.22557 sumHitrate 0.23056  privacy: 0.59919\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 10 03:38:42 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [0.73384 0.00468 0.17989] total reward: 0.91841\n",
      "UEHitrate: 0.00499  edgeHitrate 0.22583 sumHitrate 0.23081  privacy: 0.59651\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h1_exp_ep0_1009-15-23-37'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.31937, 0.30938, 0.28096, 0.26872, 0.26195, 0.25816,\n",
       "        0.25104, 0.24861, 0.24464, 0.24177, 0.24023, 0.23554, 0.23211,\n",
       "        0.23271, 0.23295, 0.23062, 0.23151, 0.23289, 0.23263, 0.23123,\n",
       "        0.23199, 0.23082, 0.22964, 0.22842, 0.22817, 0.22694, 0.22561,\n",
       "        0.22571, 0.22794, 0.23081]),\n",
       " array([0.     , 0.0095 , 0.00725, 0.00697, 0.00677, 0.00664, 0.00635,\n",
       "        0.00599, 0.00585, 0.00571, 0.00554, 0.00538, 0.00528, 0.00535,\n",
       "        0.00539, 0.00539, 0.00539, 0.00534, 0.00522, 0.00524, 0.00513,\n",
       "        0.00512, 0.00507, 0.00505, 0.00505, 0.00506, 0.00507, 0.00507,\n",
       "        0.00504, 0.00502, 0.00499]),\n",
       " array([0.     , 0.30987, 0.30213, 0.27399, 0.26194, 0.25531, 0.25181,\n",
       "        0.24505, 0.24276, 0.23893, 0.23623, 0.23484, 0.23026, 0.22677,\n",
       "        0.22731, 0.22756, 0.22523, 0.22618, 0.22767, 0.22739, 0.22609,\n",
       "        0.22687, 0.22575, 0.22459, 0.22337, 0.22311, 0.22188, 0.22054,\n",
       "        0.22066, 0.22293, 0.22583]))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4.02118, 2.60748, 2.35088, 2.22801, 2.11675, 2.02317, 1.92269,\n",
       "       1.85717, 1.76463, 1.67848, 1.5724 , 1.48494, 1.43214, 1.37793,\n",
       "       1.32209, 1.27118, 1.20669, 1.1508 , 1.10139, 1.04912, 1.00822,\n",
       "       0.95159, 0.90192, 0.85207, 0.80474, 0.76386, 0.72363, 0.69054,\n",
       "       0.65739, 0.62339, 0.59651])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h1_exp_ep0_1009-15-23-37'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}