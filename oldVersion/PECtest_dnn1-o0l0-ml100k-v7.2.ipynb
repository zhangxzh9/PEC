{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2 增加神经网络输出\n",
    "#v3 修改reward为直接变化量\n",
    "#v4 删除e,S特征 \n",
    "#v7 增加邻居节点特征\n",
    "#v7.1 比例作为reward\n",
    "#v7.2 sigmod reward\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_o0l0_v7.2_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = '/home/ubuntu/data/dataset/ml-100k/'\n",
    "UIT = pd.read_csv(data_path + 'ml_header.trace')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=np.int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<18]\n",
    "validUIT = UIT[UIT['day']<24]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT.shape,validUIT.shape\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0,\"betal\":0}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 15\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    " \n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.simU = simUsers[u]\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        #statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        statusFeature = torch.zeros(size=(4,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        tmp = torch.zeros(size=(env.contentNum,))\n",
    "        for simUser in self.simU:\n",
    "            tmp += self.r[simUser]\n",
    "        statusFeature[3] = tmp/len(self.simU)\n",
    "        #statusFeature[3] = self.e\n",
    "        #statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        self.Rh = self.ALPHAh * ( 1 / ( 1 + torch.exp( 0.5 * torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "        if self.W[-1] not in np.argwhere(self.lastAction.numpy()): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            if  not train or (train and sample > eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                with torch.no_grad():\n",
    "                    Q_value = QNetwork(self.statusFeature)\n",
    "                    actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "                    #actionIndex = list(Q_value.squeeze(1).argsort(descending=True)[0:self.Bu])\n",
    "                QNetwork.train()\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "\n",
    "            self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "            self.action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                self.action[index] = 1\n",
    "            env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "        else:\n",
    "            self.action = self.lastAction # keep the cache and no request the new video\n",
    "\n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "\n",
    "        if train:\n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                        lastAction.to(device), \n",
    "                        self.statusFeature,\n",
    "                        torch.tensor([self.reward.float()]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "                        \n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    '''\n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    '''\n",
    "    #next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    #expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 21:39:01 2021 Episode: 0   Index: 65335   Loss: 0.08968 --\n",
      "Reward: [0.53735 0.      0.     ] total reward: 0.53735\n",
      "UEHitrate: 0.00736  edgeHitrate 0.29635 sumHitrate 0.30371  privacy: 1.77035\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 21:41:42 2021 Episode: 0   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.51989 0.      0.     ] total reward: 0.51989\n",
      "UEHitrate: 0.00747  edgeHitrate 0.59374 sumHitrate 0.60122  privacy: 1.07964\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v7.2_ep0_1011-21-41-42\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 21:43:59 2021 Episode: 1   Index: 65335   Loss: 0.06553 --\n",
      "Reward: [0.53559 0.      0.     ] total reward: 0.53559\n",
      "UEHitrate: 0.00882  edgeHitrate 0.31297 sumHitrate 0.32178  privacy: 1.93649\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 21:46:36 2021 Episode: 1   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.52012 0.      0.     ] total reward: 0.52012\n",
      "UEHitrate: 0.00743  edgeHitrate 0.59557 sumHitrate 0.603  privacy: 1.06894\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v7.2_ep1_1011-21-46-36\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 21:48:53 2021 Episode: 2   Index: 65335   Loss: 0.06311 --\n",
      "Reward: [0.5337 0.     0.    ] total reward: 0.5337\n",
      "UEHitrate: 0.00883  edgeHitrate 0.32492 sumHitrate 0.33375  privacy: 2.03309\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 21:51:34 2021 Episode: 2   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.51997 0.      0.     ] total reward: 0.51997\n",
      "UEHitrate: 0.00761  edgeHitrate 0.58446 sumHitrate 0.59207  privacy: 1.08535\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 21:53:55 2021 Episode: 3   Index: 65335   Loss: 0.06164 --\n",
      "Reward: [0.53226 0.      0.     ] total reward: 0.53226\n",
      "UEHitrate: 0.00683  edgeHitrate 0.34231 sumHitrate 0.34913  privacy: 2.06747\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 21:56:39 2021 Episode: 3   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.51989 0.      0.     ] total reward: 0.51989\n",
      "UEHitrate: 0.00852  edgeHitrate 0.59086 sumHitrate 0.59938  privacy: 1.07861\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 21:59:16 2021 Episode: 4   Index: 65335   Loss: 0.06108 --\n",
      "Reward: [0.53046 0.      0.     ] total reward: 0.53046\n",
      "UEHitrate: 0.00879  edgeHitrate 0.36156 sumHitrate 0.37035  privacy: 2.03553\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 22:01:58 2021 Episode: 4   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.5201 0.     0.    ] total reward: 0.5201\n",
      "UEHitrate: 0.00921  edgeHitrate 0.59363 sumHitrate 0.60284  privacy: 1.06976\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 22:04:43 2021 Episode: 5   Index: 65335   Loss: 0.06048 --\n",
      "Reward: [0.52925 0.      0.     ] total reward: 0.52925\n",
      "UEHitrate: 0.00621  edgeHitrate 0.39321 sumHitrate 0.39943  privacy: 1.96363\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 22:07:51 2021 Episode: 5   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.52008 0.      0.     ] total reward: 0.52008\n",
      "UEHitrate: 0.00878  edgeHitrate 0.61502 sumHitrate 0.6238  privacy: 1.06258\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 22:10:37 2021 Episode: 6   Index: 65335   Loss: 0.05994 --\n",
      "Reward: [0.52798 0.      0.     ] total reward: 0.52798\n",
      "UEHitrate: 0.00698  edgeHitrate 0.42347 sumHitrate 0.43045  privacy: 1.8605\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 22:13:44 2021 Episode: 6   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.51992 0.      0.     ] total reward: 0.51992\n",
      "UEHitrate: 0.01075  edgeHitrate 0.61643 sumHitrate 0.62718  privacy: 1.05792\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 22:16:36 2021 Episode: 7   Index: 65335   Loss: 0.05957 --\n",
      "Reward: [0.52686 0.      0.     ] total reward: 0.52686\n",
      "UEHitrate: 0.0062  edgeHitrate 0.45996 sumHitrate 0.46616  privacy: 1.74202\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 22:19:43 2021 Episode: 7   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.52033 0.      0.     ] total reward: 0.52033\n",
      "UEHitrate: 0.01043  edgeHitrate 0.62242 sumHitrate 0.63285  privacy: 1.04848\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v7.2_ep7_1011-22-19-43\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 22:22:35 2021 Episode: 8   Index: 65335   Loss: 0.05924 --\n",
      "Reward: [0.52609 0.      0.     ] total reward: 0.52609\n",
      "UEHitrate: 0.00701  edgeHitrate 0.49017 sumHitrate 0.49718  privacy: 1.63805\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 22:25:39 2021 Episode: 8   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.52032 0.      0.     ] total reward: 0.52032\n",
      "UEHitrate: 0.00981  edgeHitrate 0.62725 sumHitrate 0.63707  privacy: 1.04336\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Mon Oct 11 22:28:52 2021 Episode: 9   Index: 65335   Loss: 0.0588 --\n",
      "Reward: [0.52568 0.      0.     ] total reward: 0.52568\n",
      "UEHitrate: 0.00649  edgeHitrate 0.51679 sumHitrate 0.52328  privacy: 1.55759\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Mon Oct 11 22:32:39 2021 Episode: 9   Index: 81113   Loss: 0.0 --\n",
      "Reward: [0.52044 0.      0.     ] total reward: 0.52044\n",
      "UEHitrate: 0.0066  edgeHitrate 0.63916 sumHitrate 0.64576  privacy: 1.05661\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v7.2_ep9_1011-22-32-39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(4, 2).to(device)\n",
    "target_net = DQN(4, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Mon Oct 11 22:33:10 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [0.55881 0.      0.     ] total reward: 0.55881\n",
      "UEHitrate: 0.0066  edgeHitrate 0.6658 sumHitrate 0.6724  privacy: 1.25645\n",
      "\n",
      "--Time: Mon Oct 11 22:33:38 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [0.54217 0.      0.     ] total reward: 0.54217\n",
      "UEHitrate: 0.0063  edgeHitrate 0.6839 sumHitrate 0.6902  privacy: 1.15922\n",
      "\n",
      "--Time: Mon Oct 11 22:34:06 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [0.53347 0.      0.     ] total reward: 0.53347\n",
      "UEHitrate: 0.0079  edgeHitrate 0.6848 sumHitrate 0.6927  privacy: 1.12566\n",
      "\n",
      "--Time: Mon Oct 11 22:34:34 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [0.52819 0.      0.     ] total reward: 0.52819\n",
      "UEHitrate: 0.00708  edgeHitrate 0.68588 sumHitrate 0.69295  privacy: 1.09805\n",
      "\n",
      "--Time: Mon Oct 11 22:35:02 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [0.52529 0.      0.     ] total reward: 0.52529\n",
      "UEHitrate: 0.00678  edgeHitrate 0.67388 sumHitrate 0.68066  privacy: 1.07984\n",
      "\n",
      "--Time: Mon Oct 11 22:35:30 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [0.52318 0.      0.     ] total reward: 0.52318\n",
      "UEHitrate: 0.0067  edgeHitrate 0.6684 sumHitrate 0.6751  privacy: 1.07193\n",
      "\n",
      "--Time: Mon Oct 11 22:35:59 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [0.52177 0.      0.     ] total reward: 0.52177\n",
      "UEHitrate: 0.00659  edgeHitrate 0.65411 sumHitrate 0.6607  privacy: 1.06402\n",
      "\n",
      "--Time: Mon Oct 11 22:36:27 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [0.52051 0.      0.     ] total reward: 0.52051\n",
      "UEHitrate: 0.00661  edgeHitrate 0.63991 sumHitrate 0.64652  privacy: 1.05747\n",
      "\n",
      "--Time: Mon Oct 11 22:36:55 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [0.51919 0.      0.     ] total reward: 0.51919\n",
      "UEHitrate: 0.00724  edgeHitrate 0.64196 sumHitrate 0.6492  privacy: 1.05465\n",
      "\n",
      "--Time: Mon Oct 11 22:37:23 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [0.51813 0.      0.     ] total reward: 0.51813\n",
      "UEHitrate: 0.0075  edgeHitrate 0.63488 sumHitrate 0.64238  privacy: 1.04753\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Oct 11 22:37:23 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [0.51813 0.      0.     ] total reward: 0.51813\n",
      "UEHitrate: 0.0075  edgeHitrate 0.63488 sumHitrate 0.64238  privacy: 1.04753\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(4, 2).to(device)\n",
    "target_net = DQN(4, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.6724 , 0.6902 , 0.6927 , 0.69295, 0.68066, 0.6751 , 0.6607 ,\n",
       "        0.64652, 0.6492 , 0.64238, 0.64238]),\n",
       " array([0.0066 , 0.0063 , 0.0079 , 0.00708, 0.00678, 0.0067 , 0.00659,\n",
       "        0.00661, 0.00724, 0.0075 , 0.0075 ]),\n",
       " array([0.6658 , 0.6839 , 0.6848 , 0.68588, 0.67388, 0.6684 , 0.65411,\n",
       "        0.63991, 0.64196, 0.63488, 0.63488]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.25645, 1.15922, 1.12566, 1.09805, 1.07984, 1.07193, 1.06402,\n",
       "       1.05747, 1.05465, 1.04753, 1.04753])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/ml100k/dnn_o0l0_v7.2_ep9_1011-22-32-39'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a71a010a29be3df77e49d8f42909021441fd02d28ccbb313c9d0d3f90d3253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
