{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>1030</td>\n",
       "      <td>101001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11807</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15068</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>1035</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5375</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1030</td>\n",
       "      <td>10202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5992</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3468</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148        1030        101001     0   \n",
       "1       203  5779    0        0         7        1030         10203     0   \n",
       "2       208  4675    0        0        92        1035         10203     0   \n",
       "3       159   332    0        0        56        1030         10202     0   \n",
       "4        50   674    0        0       439        1030         10203     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831   29  2591880        34        1030         10203     0   \n",
       "300979  158  8448   29  2591880        34        1030         10203     0   \n",
       "300980  483  6463   29  2591940        35        1030         10203     0   \n",
       "300981  158  4715   29  2591940        34        1030         10203     0   \n",
       "300982  483  2021   29  2591940        34        1030         10203     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0      11807          1            2  \n",
       "1              0      15068          1            2  \n",
       "2              0       5375          1            2  \n",
       "3              0       5992          1            2  \n",
       "4              0       3468          1            2  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0      10010          1            2  \n",
       "300979         0      23340          1            2  \n",
       "300980         0      10010          1            2  \n",
       "300981         0      23340          1            2  \n",
       "300982         0      10010          1            2  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,\n",
       " 500,\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148        1030        101001     0   \n",
       " 1       203  5779    0        0         7        1030         10203     0   \n",
       " 2       208  4675    0        0        92        1035         10203     0   \n",
       " 3       159   332    0        0        56        1030         10202     0   \n",
       " 4        50   674    0        0       439        1030         10203     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 198170  264  7442   17  1555140        90        1035         10203     0   \n",
       " 198171   19  9362   17  1555140       424        1035         10203     0   \n",
       " 198172   82  9223   17  1555140        94        1037         10203     0   \n",
       " 198173   35  4164   17  1555140        22        1030         10203     0   \n",
       " 198174  239  5062   17  1555140        89        1035         10203     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0      11807          1            2  \n",
       " 1              0      15068          1            2  \n",
       " 2              0       5375          1            2  \n",
       " 3              0       5992          1            2  \n",
       " 4              0       3468          1            2  \n",
       " ...          ...        ...        ...          ...  \n",
       " 198170         0       7592          1            2  \n",
       " 198171         0       5938          1            2  \n",
       " 198172         0      11393          1            2  \n",
       " 198173         0       5866          1            2  \n",
       " 198174         0      23746          1            2  \n",
       " \n",
       " [198175 rows x 12 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=15,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p) , \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)).sum() - torch.log(v * p + (1-v) * (1-p)).sum())\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl *  ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "\n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "        \n",
    "        if train: \n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                    lastAction.to(device), \n",
    "                    self.statusFeature,\n",
    "                    torch.tensor([self.reward.float()]).to(device),\n",
    "                    torch.tensor([self.W[-1]]).to(device))\n",
    "        \n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "        \n",
    "\n",
    "        if  not train or (train and sample > eps_threshold):\n",
    "            QNetwork.eval()\n",
    "            with torch.no_grad():\n",
    "                Q_value = QNetwork(self.statusFeature)\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "            QNetwork.train()\n",
    "        else:\n",
    "            actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort()[0:10])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELPATH =  './model_dict/dnn_h0_'\n",
    "rewardPara = {\"alpha\":0,\"betao\":1,\"betal\":1}\n",
    "latency = [0.1,1,0.9]\n",
    "\n",
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Mon Sep 20 00:55:44 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 01:03:36 2021 Episode: 0   Index: 50000   Loss: 0.31137 --\n",
      "Reward: [0.      0.0115  0.09216] total reward: 0.10366\n",
      "UEHitrate: 0.00526  edgeHitrate 0.10282 sumHitrate 0.10808  privacy: 2.52796\n",
      "\n",
      "--Time: Mon Sep 20 01:12:31 2021 Episode: 0   Index: 100000   Loss: 0.29461 --\n",
      "Reward: [0.      0.01107 0.08406] total reward: 0.09513\n",
      "UEHitrate: 0.00522  edgeHitrate 0.09363 sumHitrate 0.09885  privacy: 1.95224\n",
      "\n",
      "--Time: Mon Sep 20 01:21:35 2021 Episode: 0   Index: 150000   Loss: 0.28344 --\n",
      "Reward: [0.      0.01197 0.0811 ] total reward: 0.09307\n",
      "UEHitrate: 0.00505  edgeHitrate 0.09049 sumHitrate 0.09555  privacy: 1.59552\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 01:30:27 2021 Episode: 0   Index: 198174   Loss: 0.27497 --\n",
      "Reward: [0.      0.01255 0.08402] total reward: 0.09657\n",
      "UEHitrate: 0.00504  edgeHitrate 0.09385 sumHitrate 0.09889  privacy: 1.34844\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h0_ep0_0920-01-30-27\n",
      "\n",
      "--Time: Mon Sep 20 01:30:28 2021 Episode: 1   Index: 0   Loss: 7.19852 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 01:39:42 2021 Episode: 1   Index: 50000   Loss: 0.21356 --\n",
      "Reward: [0.      0.02026 0.14958] total reward: 0.16983\n",
      "UEHitrate: 0.01168  edgeHitrate 0.16746 sumHitrate 0.17914  privacy: 2.59125\n",
      "\n",
      "--Time: Mon Sep 20 01:48:36 2021 Episode: 1   Index: 100000   Loss: 0.20144 --\n",
      "Reward: [0.      0.0262  0.17792] total reward: 0.20412\n",
      "UEHitrate: 0.01684  edgeHitrate 0.19938 sumHitrate 0.21622  privacy: 2.09748\n",
      "\n",
      "--Time: Mon Sep 20 01:56:46 2021 Episode: 1   Index: 150000   Loss: 0.19125 --\n",
      "Reward: [0.      0.0294  0.20027] total reward: 0.22967\n",
      "UEHitrate: 0.01897  edgeHitrate 0.22479 sumHitrate 0.24376  privacy: 1.76649\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 02:04:57 2021 Episode: 1   Index: 198174   Loss: 0.18307 --\n",
      "Reward: [0.      0.03116 0.21618] total reward: 0.24734\n",
      "UEHitrate: 0.01983  edgeHitrate 0.24273 sumHitrate 0.26256  privacy: 1.52424\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h0_ep1_0920-02-04-57\n",
      "\n",
      "--Time: Mon Sep 20 02:04:58 2021 Episode: 2   Index: 0   Loss: 5.8313 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 02:13:21 2021 Episode: 2   Index: 50000   Loss: 0.1903 --\n",
      "Reward: [0.      0.03068 0.21245] total reward: 0.24313\n",
      "UEHitrate: 0.01944  edgeHitrate 0.23808 sumHitrate 0.25751  privacy: 2.74269\n",
      "\n",
      "--Time: Mon Sep 20 02:21:08 2021 Episode: 2   Index: 100000   Loss: 0.18052 --\n",
      "Reward: [0.      0.03512 0.22776] total reward: 0.26288\n",
      "UEHitrate: 0.02321  edgeHitrate 0.25546 sumHitrate 0.27867  privacy: 2.22284\n",
      "\n",
      "--Time: Mon Sep 20 02:29:24 2021 Episode: 2   Index: 150000   Loss: 0.17196 --\n",
      "Reward: [0.      0.03788 0.24683] total reward: 0.28472\n",
      "UEHitrate: 0.02491  edgeHitrate 0.27704 sumHitrate 0.30194  privacy: 1.89777\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 02:37:24 2021 Episode: 2   Index: 198174   Loss: 0.16528 --\n",
      "Reward: [0.      0.03637 0.2595 ] total reward: 0.29587\n",
      "UEHitrate: 0.02371  edgeHitrate 0.29087 sumHitrate 0.31459  privacy: 1.63436\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h0_ep2_0920-02-37-24\n",
      "\n",
      "--Time: Mon Sep 20 02:37:24 2021 Episode: 3   Index: 0   Loss: 5.14703 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 02:46:23 2021 Episode: 3   Index: 50000   Loss: 0.16964 --\n",
      "Reward: [0.      0.01068 0.09743] total reward: 0.10811\n",
      "UEHitrate: 0.0044  edgeHitrate 0.1083 sumHitrate 0.1127  privacy: 2.53299\n",
      "\n",
      "--Time: Mon Sep 20 02:54:21 2021 Episode: 3   Index: 100000   Loss: 0.15827 --\n",
      "Reward: [0.      0.02398 0.16504] total reward: 0.18902\n",
      "UEHitrate: 0.01444  edgeHitrate 0.18499 sumHitrate 0.19943  privacy: 2.03948\n",
      "\n",
      "--Time: Mon Sep 20 03:02:47 2021 Episode: 3   Index: 150000   Loss: 0.1505 --\n",
      "Reward: [0.      0.02456 0.16492] total reward: 0.18948\n",
      "UEHitrate: 0.0148  edgeHitrate 0.18492 sumHitrate 0.19972  privacy: 1.56589\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 03:10:25 2021 Episode: 3   Index: 198174   Loss: 0.14242 --\n",
      "Reward: [0.      0.02175 0.14577] total reward: 0.16752\n",
      "UEHitrate: 0.01227  edgeHitrate 0.1633 sumHitrate 0.17557  privacy: 1.15988\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 03:10:25 2021 Episode: 4   Index: 0   Loss: 4.92435 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 03:18:33 2021 Episode: 4   Index: 50000   Loss: 0.17394 --\n",
      "Reward: [0.      0.00963 0.07041] total reward: 0.08004\n",
      "UEHitrate: 0.00362  edgeHitrate 0.07846 sumHitrate 0.08208  privacy: 2.53351\n",
      "\n",
      "--Time: Mon Sep 20 03:26:18 2021 Episode: 4   Index: 100000   Loss: 0.15917 --\n",
      "Reward: [0.      0.00982 0.08257] total reward: 0.09238\n",
      "UEHitrate: 0.00391  edgeHitrate 0.09193 sumHitrate 0.09584  privacy: 1.75445\n",
      "\n",
      "--Time: Mon Sep 20 03:34:31 2021 Episode: 4   Index: 150000   Loss: 0.14459 --\n",
      "Reward: [0.      0.01098 0.09104] total reward: 0.10202\n",
      "UEHitrate: 0.00439  edgeHitrate 0.10141 sumHitrate 0.10579  privacy: 1.22706\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 03:42:12 2021 Episode: 4   Index: 198174   Loss: 0.13594 --\n",
      "Reward: [0.      0.01158 0.08908] total reward: 0.10065\n",
      "UEHitrate: 0.00437  edgeHitrate 0.09925 sumHitrate 0.10362  privacy: 0.83319\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 03:42:12 2021 Episode: 5   Index: 0   Loss: 4.36957 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 03:49:59 2021 Episode: 5   Index: 50000   Loss: 0.17429 --\n",
      "Reward: [0.      0.01    0.06368] total reward: 0.07368\n",
      "UEHitrate: 0.00328  edgeHitrate 0.07122 sumHitrate 0.0745  privacy: 2.51762\n",
      "\n",
      "--Time: Mon Sep 20 03:58:29 2021 Episode: 5   Index: 100000   Loss: 0.15423 --\n",
      "Reward: [0.      0.01019 0.08464] total reward: 0.09483\n",
      "UEHitrate: 0.00369  edgeHitrate 0.09444 sumHitrate 0.09813  privacy: 1.68242\n",
      "\n",
      "--Time: Mon Sep 20 04:07:46 2021 Episode: 5   Index: 150000   Loss: 0.13911 --\n",
      "Reward: [0.      0.01129 0.0829 ] total reward: 0.09419\n",
      "UEHitrate: 0.00411  edgeHitrate 0.0927 sumHitrate 0.09681  privacy: 1.11143\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 04:16:43 2021 Episode: 5   Index: 198174   Loss: 0.12903 --\n",
      "Reward: [0.      0.01221 0.08201] total reward: 0.09422\n",
      "UEHitrate: 0.00449  edgeHitrate 0.09178 sumHitrate 0.09627  privacy: 0.73803\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 04:16:43 2021 Episode: 6   Index: 0   Loss: 3.20884 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 04:25:37 2021 Episode: 6   Index: 50000   Loss: 0.1226 --\n",
      "Reward: [0.      0.00976 0.06287] total reward: 0.07264\n",
      "UEHitrate: 0.0033  edgeHitrate 0.0702 sumHitrate 0.0735  privacy: 2.51188\n",
      "\n",
      "--Time: Mon Sep 20 04:34:47 2021 Episode: 6   Index: 100000   Loss: 0.11335 --\n",
      "Reward: [0.      0.01001 0.06263] total reward: 0.07264\n",
      "UEHitrate: 0.00378  edgeHitrate 0.06992 sumHitrate 0.0737  privacy: 1.78564\n",
      "\n",
      "--Time: Mon Sep 20 04:44:04 2021 Episode: 6   Index: 150000   Loss: 0.10577 --\n",
      "Reward: [0.      0.01177 0.08106] total reward: 0.09283\n",
      "UEHitrate: 0.00473  edgeHitrate 0.09059 sumHitrate 0.09531  privacy: 1.19011\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 04:53:00 2021 Episode: 6   Index: 198174   Loss: 0.10059 --\n",
      "Reward: [0.      0.01243 0.07939] total reward: 0.09182\n",
      "UEHitrate: 0.00488  edgeHitrate 0.08875 sumHitrate 0.09364  privacy: 0.8279\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 04:53:01 2021 Episode: 7   Index: 0   Loss: 2.65685 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 05:01:43 2021 Episode: 7   Index: 50000   Loss: 0.11617 --\n",
      "Reward: [0.      0.00952 0.0624 ] total reward: 0.07193\n",
      "UEHitrate: 0.0031  edgeHitrate 0.06984 sumHitrate 0.07294  privacy: 2.50665\n",
      "\n",
      "--Time: Mon Sep 20 05:10:53 2021 Episode: 7   Index: 100000   Loss: 0.10902 --\n",
      "Reward: [0.      0.00976 0.06466] total reward: 0.07443\n",
      "UEHitrate: 0.00345  edgeHitrate 0.07222 sumHitrate 0.07567  privacy: 1.77507\n",
      "\n",
      "--Time: Mon Sep 20 05:20:05 2021 Episode: 7   Index: 150000   Loss: 0.10022 --\n",
      "Reward: [0.      0.01167 0.08276] total reward: 0.09443\n",
      "UEHitrate: 0.00437  edgeHitrate 0.09249 sumHitrate 0.09686  privacy: 1.04179\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 05:29:32 2021 Episode: 7   Index: 198174   Loss: 0.09428 --\n",
      "Reward: [0.      0.01256 0.08343] total reward: 0.09599\n",
      "UEHitrate: 0.00467  edgeHitrate 0.09333 sumHitrate 0.09799  privacy: 0.68844\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 05:29:32 2021 Episode: 8   Index: 0   Loss: 2.3206 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 05:38:24 2021 Episode: 8   Index: 50000   Loss: 0.1028 --\n",
      "Reward: [0.      0.00969 0.06539] total reward: 0.07508\n",
      "UEHitrate: 0.00308  edgeHitrate 0.07294 sumHitrate 0.07602  privacy: 2.50425\n",
      "\n",
      "--Time: Mon Sep 20 05:47:44 2021 Episode: 8   Index: 100000   Loss: 0.09743 --\n",
      "Reward: [0.      0.00999 0.06257] total reward: 0.07255\n",
      "UEHitrate: 0.00349  edgeHitrate 0.06988 sumHitrate 0.07337  privacy: 1.76845\n",
      "\n",
      "--Time: Mon Sep 20 05:57:07 2021 Episode: 8   Index: 150000   Loss: 0.09102 --\n",
      "Reward: [0.      0.01585 0.10107] total reward: 0.11692\n",
      "UEHitrate: 0.00765  edgeHitrate 0.11343 sumHitrate 0.12108  privacy: 1.30764\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 06:06:14 2021 Episode: 8   Index: 198174   Loss: 0.08685 --\n",
      "Reward: [0.      0.02433 0.14525] total reward: 0.16959\n",
      "UEHitrate: 0.01418  edgeHitrate 0.16314 sumHitrate 0.17732  privacy: 1.21112\n",
      "----------------------------------------------------------------\n",
      "\n",
      "--Time: Mon Sep 20 06:06:14 2021 Episode: 9   Index: 0   Loss: 1.60987 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 06:15:22 2021 Episode: 9   Index: 50000   Loss: 0.10143 --\n",
      "Reward: [0.      0.03916 0.31742] total reward: 0.35659\n",
      "UEHitrate: 0.0263  edgeHitrate 0.35607 sumHitrate 0.38237  privacy: 1.89567\n",
      "\n",
      "--Time: Mon Sep 20 06:24:14 2021 Episode: 9   Index: 100000   Loss: 0.09897 --\n",
      "Reward: [0.      0.04561 0.32068] total reward: 0.36629\n",
      "UEHitrate: 0.03246  edgeHitrate 0.35918 sumHitrate 0.39164  privacy: 1.82213\n",
      "\n",
      "--Time: Mon Sep 20 06:33:22 2021 Episode: 9   Index: 150000   Loss: 0.09699 --\n",
      "Reward: [0.      0.05044 0.33008] total reward: 0.38052\n",
      "UEHitrate: 0.03545  edgeHitrate 0.37002 sumHitrate 0.40547  privacy: 1.76106\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 06:42:27 2021 Episode: 9   Index: 198174   Loss: 0.09538 --\n",
      "Reward: [0.      0.05161 0.33915] total reward: 0.39075\n",
      "UEHitrate: 0.03618  edgeHitrate 0.38038 sumHitrate 0.41656  privacy: 1.70641\n",
      "----------------------------------------------------------------\n",
      "./model_dict/dnn_h0_ep9_0920-06-42-27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 10\n",
    "TARGET_UPDATE = 1\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = trainUIT.shape[0]*5\n",
    "agentStep = 0\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "        agentStep += 1\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 50000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+'ep{}_'.format(bestEpisode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "        print(bestPath)\n",
    "    print()\n",
    "    env = ENV(userNum,contentNum,latency)\n",
    "    UEs = {}\n",
    "    sumReward = sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Mon Sep 20 06:42:27 2021   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Mon Sep 20 06:43:21 2021   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.      0.03947 0.30705] total reward: 0.34652\n",
      "UEHitrate: 0.0265  edgeHitrate 0.34217 sumHitrate 0.36866  privacy: 0.9521\n",
      "\n",
      "--Time: Mon Sep 20 06:44:14 2021   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.      0.05588 0.36066] total reward: 0.41653\n",
      "UEHitrate: 0.03885  edgeHitrate 0.40273 sumHitrate 0.44158  privacy: 0.86163\n",
      "\n",
      "--Time: Mon Sep 20 06:45:04 2021   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.      0.06542 0.3518 ] total reward: 0.41722\n",
      "UEHitrate: 0.0478  edgeHitrate 0.39295 sumHitrate 0.44075  privacy: 0.87033\n",
      "\n",
      "--Time: Mon Sep 20 06:45:52 2021   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.      0.07021 0.34897] total reward: 0.41917\n",
      "UEHitrate: 0.05085  edgeHitrate 0.39057 sumHitrate 0.44141  privacy: 0.87011\n",
      "\n",
      "--Time: Mon Sep 20 06:46:47 2021   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.      0.07419 0.35173] total reward: 0.42592\n",
      "UEHitrate: 0.05388  edgeHitrate 0.39393 sumHitrate 0.44781  privacy: 0.87071\n",
      "\n",
      "--Time: Mon Sep 20 06:47:40 2021   Index: 60000   Loss: 0.0 --\n",
      "Reward: [0.      0.07433 0.35363] total reward: 0.42797\n",
      "UEHitrate: 0.05413  edgeHitrate 0.39594 sumHitrate 0.45008  privacy: 0.87291\n",
      "\n",
      "--Time: Mon Sep 20 06:48:33 2021   Index: 70000   Loss: 0.0 --\n",
      "Reward: [0.      0.07435 0.35147] total reward: 0.42582\n",
      "UEHitrate: 0.05401  edgeHitrate 0.39421 sumHitrate 0.44822  privacy: 0.87479\n",
      "\n",
      "--Time: Mon Sep 20 06:49:26 2021   Index: 80000   Loss: 0.0 --\n",
      "Reward: [0.      0.0758  0.35623] total reward: 0.43202\n",
      "UEHitrate: 0.05495  edgeHitrate 0.3996 sumHitrate 0.45454  privacy: 0.87655\n",
      "\n",
      "--Time: Mon Sep 20 06:50:21 2021   Index: 90000   Loss: 0.0 --\n",
      "Reward: [0.      0.07558 0.35894] total reward: 0.43451\n",
      "UEHitrate: 0.05492  edgeHitrate 0.4021 sumHitrate 0.45702  privacy: 0.87809\n",
      "\n",
      "--Time: Mon Sep 20 06:51:12 2021   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.      0.07479 0.36221] total reward: 0.437\n",
      "UEHitrate: 0.05476  edgeHitrate 0.40548 sumHitrate 0.46024  privacy: 0.87937\n",
      "\n",
      "--Time: Mon Sep 20 06:52:07 2021   Index: 110000   Loss: 0.0 --\n",
      "Reward: [0.      0.07555 0.36541] total reward: 0.44096\n",
      "UEHitrate: 0.05517  edgeHitrate 0.40958 sumHitrate 0.46475  privacy: 0.88053\n",
      "\n",
      "--Time: Mon Sep 20 06:53:01 2021   Index: 120000   Loss: 0.0 --\n",
      "Reward: [0.      0.07495 0.36789] total reward: 0.44285\n",
      "UEHitrate: 0.05498  edgeHitrate 0.41228 sumHitrate 0.46726  privacy: 0.88166\n",
      "\n",
      "--Time: Mon Sep 20 06:53:52 2021   Index: 130000   Loss: 0.0 --\n",
      "Reward: [0.      0.07555 0.36942] total reward: 0.44497\n",
      "UEHitrate: 0.05535  edgeHitrate 0.41408 sumHitrate 0.46943  privacy: 0.88436\n",
      "\n",
      "--Time: Mon Sep 20 06:54:44 2021   Index: 140000   Loss: 0.0 --\n",
      "Reward: [0.      0.07506 0.37374] total reward: 0.4488\n",
      "UEHitrate: 0.05522  edgeHitrate 0.4187 sumHitrate 0.47393  privacy: 0.88617\n",
      "\n",
      "--Time: Mon Sep 20 06:55:41 2021   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.      0.07498 0.37707] total reward: 0.45205\n",
      "UEHitrate: 0.05484  edgeHitrate 0.4227 sumHitrate 0.47754  privacy: 0.8885\n",
      "\n",
      "--Time: Mon Sep 20 06:56:31 2021   Index: 160000   Loss: 0.0 --\n",
      "Reward: [0.      0.07482 0.37729] total reward: 0.45212\n",
      "UEHitrate: 0.05426  edgeHitrate 0.4233 sumHitrate 0.47756  privacy: 0.89144\n",
      "\n",
      "--Time: Mon Sep 20 06:57:28 2021   Index: 170000   Loss: 0.0 --\n",
      "Reward: [0.      0.07391 0.38112] total reward: 0.45503\n",
      "UEHitrate: 0.0537  edgeHitrate 0.42743 sumHitrate 0.48113  privacy: 0.89327\n",
      "\n",
      "--Time: Mon Sep 20 06:58:22 2021   Index: 180000   Loss: 0.0 --\n",
      "Reward: [0.      0.07368 0.38383] total reward: 0.45751\n",
      "UEHitrate: 0.05361  edgeHitrate 0.43038 sumHitrate 0.48399  privacy: 0.89443\n",
      "\n",
      "--Time: Mon Sep 20 06:59:16 2021   Index: 190000   Loss: 0.0 --\n",
      "Reward: [0.      0.07377 0.38645] total reward: 0.46022\n",
      "UEHitrate: 0.05373  edgeHitrate 0.43342 sumHitrate 0.48716  privacy: 0.89541\n",
      "\n",
      "--Time: Mon Sep 20 07:00:09 2021   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.      0.07385 0.3883 ] total reward: 0.46215\n",
      "UEHitrate: 0.05412  edgeHitrate 0.43535 sumHitrate 0.48947  privacy: 0.89679\n",
      "\n",
      "--Time: Mon Sep 20 07:01:04 2021   Index: 210000   Loss: 0.0 --\n",
      "Reward: [0.      0.07357 0.38963] total reward: 0.4632\n",
      "UEHitrate: 0.05396  edgeHitrate 0.43684 sumHitrate 0.4908  privacy: 0.89692\n",
      "\n",
      "--Time: Mon Sep 20 07:01:56 2021   Index: 220000   Loss: 0.0 --\n",
      "Reward: [0.      0.07362 0.38891] total reward: 0.46254\n",
      "UEHitrate: 0.05385  edgeHitrate 0.43635 sumHitrate 0.4902  privacy: 0.89787\n",
      "\n",
      "--Time: Mon Sep 20 07:02:39 2021   Index: 230000   Loss: 0.0 --\n",
      "Reward: [0.      0.07334 0.3883 ] total reward: 0.46164\n",
      "UEHitrate: 0.0537  edgeHitrate 0.43573 sumHitrate 0.48944  privacy: 0.89923\n",
      "\n",
      "--Time: Mon Sep 20 07:03:18 2021   Index: 240000   Loss: 0.0 --\n",
      "Reward: [0.      0.07301 0.38813] total reward: 0.46114\n",
      "UEHitrate: 0.05354  edgeHitrate 0.43549 sumHitrate 0.48904  privacy: 0.90083\n",
      "\n",
      "--Time: Mon Sep 20 07:03:54 2021   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.      0.07283 0.38951] total reward: 0.46234\n",
      "UEHitrate: 0.05342  edgeHitrate 0.43701 sumHitrate 0.49043  privacy: 0.90209\n",
      "\n",
      "--Time: Mon Sep 20 07:04:36 2021   Index: 260000   Loss: 0.0 --\n",
      "Reward: [0.      0.07246 0.3891 ] total reward: 0.46155\n",
      "UEHitrate: 0.05338  edgeHitrate 0.43643 sumHitrate 0.48981  privacy: 0.90319\n",
      "\n",
      "--Time: Mon Sep 20 07:05:16 2021   Index: 270000   Loss: 0.0 --\n",
      "Reward: [0.      0.07228 0.38871] total reward: 0.46099\n",
      "UEHitrate: 0.05352  edgeHitrate 0.43588 sumHitrate 0.4894  privacy: 0.90459\n",
      "\n",
      "--Time: Mon Sep 20 07:05:56 2021   Index: 280000   Loss: 0.0 --\n",
      "Reward: [0.      0.07218 0.38923] total reward: 0.46141\n",
      "UEHitrate: 0.05354  edgeHitrate 0.43644 sumHitrate 0.48998  privacy: 0.90573\n",
      "\n",
      "--Time: Mon Sep 20 07:06:36 2021   Index: 290000   Loss: 0.0 --\n",
      "Reward: [0.      0.07192 0.39195] total reward: 0.46386\n",
      "UEHitrate: 0.05348  edgeHitrate 0.43944 sumHitrate 0.49292  privacy: 0.90632\n",
      "\n",
      "--Time: Mon Sep 20 07:07:17 2021   Index: 300000   Loss: 0.0 --\n",
      "Reward: [0.      0.0719  0.39544] total reward: 0.46735\n",
      "UEHitrate: 0.0537  edgeHitrate 0.44324 sumHitrate 0.49694  privacy: 0.90674\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Mon Sep 20 07:07:21 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [0.      0.07198 0.39565] total reward: 0.46763\n",
      "UEHitrate: 0.05378  edgeHitrate 0.44349 sumHitrate 0.49727  privacy: 0.90679\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.     , 0.36866, 0.44158, 0.44075, 0.44141, 0.44781, 0.45008,\n",
       "        0.44822, 0.45454, 0.45702, 0.46024, 0.46475, 0.46726, 0.46943,\n",
       "        0.47393, 0.47754, 0.47756, 0.48113, 0.48399, 0.48716, 0.48947,\n",
       "        0.4908 , 0.4902 , 0.48944, 0.48904, 0.49043, 0.48981, 0.4894 ,\n",
       "        0.48998, 0.49292, 0.49727]),\n",
       " array([0.     , 0.0265 , 0.03885, 0.0478 , 0.05085, 0.05388, 0.05413,\n",
       "        0.05401, 0.05495, 0.05492, 0.05476, 0.05517, 0.05498, 0.05535,\n",
       "        0.05522, 0.05484, 0.05426, 0.0537 , 0.05361, 0.05373, 0.05412,\n",
       "        0.05396, 0.05385, 0.0537 , 0.05354, 0.05342, 0.05338, 0.05352,\n",
       "        0.05354, 0.05348, 0.05378]),\n",
       " array([0.     , 0.34217, 0.40273, 0.39295, 0.39057, 0.39393, 0.39594,\n",
       "        0.39421, 0.3996 , 0.4021 , 0.40548, 0.40958, 0.41228, 0.41408,\n",
       "        0.4187 , 0.4227 , 0.4233 , 0.42743, 0.43038, 0.43342, 0.43535,\n",
       "        0.43684, 0.43635, 0.43573, 0.43549, 0.43701, 0.43643, 0.43588,\n",
       "        0.43644, 0.43944, 0.44349]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.02118, 0.9521 , 0.86163, 0.87033, 0.87011, 0.87071, 0.87291,\n",
       "       0.87479, 0.87655, 0.87809, 0.87937, 0.88053, 0.88166, 0.88436,\n",
       "       0.88617, 0.8885 , 0.89144, 0.89327, 0.89443, 0.89541, 0.89679,\n",
       "       0.89692, 0.89787, 0.89923, 0.90083, 0.90209, 0.90319, 0.90459,\n",
       "       0.90573, 0.90632, 0.90679])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "py38_ML",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
