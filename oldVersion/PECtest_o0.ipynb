{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>video_type</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "      <th>level4</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3006</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63</td>\n",
       "      <td>2232</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>2166</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63</td>\n",
       "      <td>4578</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>78</td>\n",
       "      <td>1821</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79614</th>\n",
       "      <td>133</td>\n",
       "      <td>3911</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79615</th>\n",
       "      <td>133</td>\n",
       "      <td>1684</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79616</th>\n",
       "      <td>185</td>\n",
       "      <td>1552</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79617</th>\n",
       "      <td>61</td>\n",
       "      <td>4078</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79618</th>\n",
       "      <td>133</td>\n",
       "      <td>2568</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79619 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         u     i  day  video_type  level1  level2  level3  level4   time\n",
       "0        3  3006    0          11       0       0       0       0      0\n",
       "1       63  2232    0          11       0       0       0       0      0\n",
       "2       68  2166    0          11       0       0       0       0      0\n",
       "3       63  4578    0          11       0       0       0       0      0\n",
       "4       78  1821    0          47       0       0       0       0      0\n",
       "...    ...   ...  ...         ...     ...     ...     ...     ...    ...\n",
       "79614  133  3911   29          13       0       0       0       0  43197\n",
       "79615  133  1684   29          13       0       0       0       0  43198\n",
       "79616  185  1552   29          11       0       0       0       0  43198\n",
       "79617   61  4078   29          13       0       0       0       0  43199\n",
       "79618  133  2568   29          13       0       0       0       0  43199\n",
       "\n",
       "[79619 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R79619_U200_V5000/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,\n",
       " 200,\n",
       "          u     i  day  video_type  level1  level2  level3  level4   time\n",
       " 0        3  3006    0          11       0       0       0       0      0\n",
       " 1       63  2232    0          11       0       0       0       0      0\n",
       " 2       68  2166    0          11       0       0       0       0      0\n",
       " 3       63  4578    0          11       0       0       0       0      0\n",
       " 4       78  1821    0          47       0       0       0       0      0\n",
       " ...    ...   ...  ...         ...     ...     ...     ...     ...    ...\n",
       " 50232  113   238   17          11       0       0       0       0  25916\n",
       " 50233  106   655   17          11       0       0       0       0  25916\n",
       " 50234  113   240   17          11       0       0       0       0  25918\n",
       " 50235   30   652   17          11       0       0       0       0  25919\n",
       " 50236   40  3397   17          11       0       0       0       0  25919\n",
       " \n",
       " [50237 rows x 9 columns])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=10,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "\n",
    "        self.p = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p) , \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                self.l_edge,\n",
    "                self.l_cp)\n",
    "\n",
    "    def reset(self):\n",
    "        self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "        self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "        self.e = np.zeros(shape=self.contentNum)\n",
    "        self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "        self.B = np.full(shape=self.userNum,fill_value=10,dtype=int)\n",
    "        self.pipe = collections.OrderedDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#每个神经网络单独作为一个reward进行训练\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']\n",
    "        self.EPS_START = 0.9\n",
    "        self.EPS_END = 0.1\n",
    "        self.EPS_DECAY = 10\n",
    "        \n",
    "        self.t = 0\n",
    "        \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e):\n",
    "\n",
    "        self.Rh =   self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)) - torch.log(lastru * lastp + (1-lastru) * (1-lastp)))\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        self.Rl =   self.BETAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        return  self.Rh\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "\n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.Bu,self.l_edge,self.l_cp,self.e)\n",
    "        \n",
    "        if train:\n",
    "            \n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                    lastAction.to(device), \n",
    "                    self.statusFeature,\n",
    "                    self.reward.float().to(device),\n",
    "                    torch.tensor([self.W[-1]]).to(device))\n",
    "        \n",
    "        sample = random.random()\n",
    "        eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) *  np.exp(-1. * self.t / self.EPS_DECAY)\n",
    "        self.t = self.t + 1\n",
    "        \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "        if  not train or (train and sample > eps_threshold):\n",
    "            QNetwork.eval()\n",
    "            with torch.no_grad():\n",
    "                Q_value = QNetwork(self.statusFeature)\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort()[0:self.Bu])\n",
    "            QNetwork.train()\n",
    "        else:\n",
    "            actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.63\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.masked_select(policy_net(state_batch),state_action_mask_bacth)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort()[0:10])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.masked_select(Q_value,action_mask).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "\n",
    "memory = ReplayMemory(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELPATH =  './model_dict/dnn_o0_'\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.0,\"betal\":5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: Tue Sep 14 00:41:38 2021 --Episode: 0   Index: 0   Reward: 0.0   Loss: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 00:45:20 2021 --Episode: 0   Index: 10000   Reward: 2.005133823688904   Loss: 3.018893286401052e-05\n",
      "UEHitrate: 0.024197580241975804  edgeHitrate 0.287971202879712 sumHitrate 0.31216878312168783  privacy: tensor(2.2175, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 00:48:46 2021 --Episode: 0   Index: 20000   Reward: 2.028179178001645   Loss: 3.347078857326655e-05\n",
      "UEHitrate: 0.035548222588870554  edgeHitrate 0.3765311734413279 sumHitrate 0.4120793960301985  privacy: tensor(2.0580, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 00:52:07 2021 --Episode: 0   Index: 30000   Reward: 1.7656587513523945   Loss: 3.343396898715388e-05\n",
      "UEHitrate: 0.03876537448751708  edgeHitrate 0.3489550348321723 sumHitrate 0.3877204093196893  privacy: tensor(1.9902, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 00:55:41 2021 --Episode: 0   Index: 40000   Reward: 1.6734560548013655   Loss: 3.1320491874862365e-05\n",
      "UEHitrate: 0.0431739206519837  edgeHitrate 0.33994150146246344 sumHitrate 0.38311542211444716  privacy: tensor(1.9413, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 00:59:12 2021 --Episode: 0   Index: 50000   Reward: 1.609229273410837   Loss: 3.030189925378048e-05\n",
      "UEHitrate: 0.045619087618247636  edgeHitrate 0.3362532749345013 sumHitrate 0.3818723625527489  privacy: tensor(1.8943, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 00:59:17 2021 --End episode: 0   Reward: 1.6081399666565661   Loss: 3.027764491621653e-05\n",
      "UEHitrate: 0.045723271692179074  edgeHitrate 0.33622628739773475 sumHitrate 0.3819495590899138  privacy: tensor(1.8933, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 00:59:17 2021 --Episode: 1   Index: 0   Reward: 0.0   Loss: 0.0002707421372178942\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:02:41 2021 --Episode: 1   Index: 10000   Reward: 1.739230419202373   Loss: 2.6508254387051183e-05\n",
      "UEHitrate: 0.0374962503749625  edgeHitrate 0.26677332266773324 sumHitrate 0.30426957304269575  privacy: tensor(2.1976, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:06:11 2021 --Episode: 1   Index: 20000   Reward: 1.615875051484295   Loss: 2.745434732759715e-05\n",
      "UEHitrate: 0.04249787510624469  edgeHitrate 0.3062346882655867 sumHitrate 0.3487325633718314  privacy: tensor(2.0382, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:09:48 2021 --Episode: 1   Index: 30000   Reward: 1.573240739262053   Loss: 2.8628868389518044e-05\n",
      "UEHitrate: 0.04496516782773907  edgeHitrate 0.31985600479984 sumHitrate 0.36482117262757907  privacy: tensor(1.9754, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:13:15 2021 --Episode: 1   Index: 40000   Reward: 1.5294704790533462   Loss: 2.8389906451223975e-05\n",
      "UEHitrate: 0.046898827529311765  edgeHitrate 0.32024199395015124 sumHitrate 0.36714082147946303  privacy: tensor(1.9325, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:16:49 2021 --Episode: 1   Index: 50000   Reward: 1.4455454320946675   Loss: 2.7516821088634125e-05\n",
      "UEHitrate: 0.04797904041919162  edgeHitrate 0.3102937941241175 sumHitrate 0.35827283454330916  privacy: tensor(1.8896, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 01:16:54 2021 --End episode: 1   Reward: 1.4433005667862189   Loss: 2.7485855967500403e-05\n",
      "UEHitrate: 0.048032326771104965  edgeHitrate 0.3099110217568724 sumHitrate 0.3579433485279774  privacy: tensor(1.8881, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 01:16:55 2021 --Episode: 2   Index: 0   Reward: 0.0   Loss: 0.00028433313127607107\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:20:25 2021 --Episode: 2   Index: 10000   Reward: 1.5422408427182237   Loss: 2.2934585486344036e-05\n",
      "UEHitrate: 0.030496950304969503  edgeHitrate 0.22727727227277272 sumHitrate 0.25777422257774224  privacy: tensor(2.1971, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:23:48 2021 --Episode: 2   Index: 20000   Reward: 1.4939233989347667   Loss: 2.4054348253612106e-05\n",
      "UEHitrate: 0.03674816259187041  edgeHitrate 0.28113594320283986 sumHitrate 0.31788410579471027  privacy: tensor(2.0370, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:27:13 2021 --Episode: 2   Index: 30000   Reward: 1.4958190358735273   Loss: 2.6479950292643185e-05\n",
      "UEHitrate: 0.039732008933035565  edgeHitrate 0.3028565714476184 sumHitrate 0.342588580380654  privacy: tensor(1.9679, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:30:34 2021 --Episode: 2   Index: 40000   Reward: 1.4570233060413282   Loss: 2.654091593054369e-05\n",
      "UEHitrate: 0.04349891252718682  edgeHitrate 0.30421739456513586 sumHitrate 0.3477163070923227  privacy: tensor(1.9261, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:34:02 2021 --Episode: 2   Index: 50000   Reward: 1.3759046459098665   Loss: 2.6029168057049174e-05\n",
      "UEHitrate: 0.04369912601747965  edgeHitrate 0.29419411611767765 sumHitrate 0.3378932421351573  privacy: tensor(1.8898, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 01:34:06 2021 --End episode: 2   Reward: 1.3739247898169131   Loss: 2.6004930812080564e-05\n",
      "UEHitrate: 0.0436331787328065  edgeHitrate 0.29398650397117665 sumHitrate 0.3376196827039831  privacy: tensor(1.8890, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 01:34:07 2021 --Episode: 3   Index: 0   Reward: 0.0   Loss: 0.00018067444034386426\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:37:38 2021 --Episode: 3   Index: 10000   Reward: 1.4090085518399829   Loss: 2.0851659731639904e-05\n",
      "UEHitrate: 0.0163983601639836  edgeHitrate 0.20087991200879912 sumHitrate 0.21727827217278273  privacy: tensor(2.1947, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:41:09 2021 --Episode: 3   Index: 20000   Reward: 1.4246827431726137   Loss: 2.261881687972319e-05\n",
      "UEHitrate: 0.027898605069746514  edgeHitrate 0.2670866456677166 sumHitrate 0.2949852507374631  privacy: tensor(2.0467, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:44:36 2021 --Episode: 3   Index: 30000   Reward: 1.422110220237951   Loss: 2.507950022718073e-05\n",
      "UEHitrate: 0.03323222559248025  edgeHitrate 0.2891236958768041 sumHitrate 0.3223559214692844  privacy: tensor(1.9771, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:48:06 2021 --Episode: 3   Index: 40000   Reward: 1.3536556096018737   Loss: 2.5187824308666092e-05\n",
      "UEHitrate: 0.0365740856478588  edgeHitrate 0.28241793955151123 sumHitrate 0.31899202519937003  privacy: tensor(1.9352, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:51:31 2021 --Episode: 3   Index: 50000   Reward: 1.2926060804920527   Loss: 2.4374245674434183e-05\n",
      "UEHitrate: 0.037339253214935704  edgeHitrate 0.27641447171056577 sumHitrate 0.31375372492550146  privacy: tensor(1.8976, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 01:51:35 2021 --End episode: 3   Reward: 1.2897157742363468   Loss: 2.4357591620988037e-05\n",
      "UEHitrate: 0.0372832772657603  edgeHitrate 0.2760714214622689 sumHitrate 0.3133546987280291  privacy: tensor(1.8964, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 01:51:35 2021 --Episode: 4   Index: 0   Reward: 0.0   Loss: 0.00016782757302280515\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:54:55 2021 --Episode: 4   Index: 10000   Reward: 1.5576426105798087   Loss: 2.212457111752853e-05\n",
      "UEHitrate: 0.0235976402359764  edgeHitrate 0.22847715228477153 sumHitrate 0.25207479252074794  privacy: tensor(2.2049, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 01:58:20 2021 --Episode: 4   Index: 20000   Reward: 1.4772894471555262   Loss: 2.4560192403754247e-05\n",
      "UEHitrate: 0.03399830008499575  edgeHitrate 0.27698615069246535 sumHitrate 0.3109844507774611  privacy: tensor(2.0547, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:01:44 2021 --Episode: 4   Index: 30000   Reward: 1.4190595628026403   Loss: 2.534856526465301e-05\n",
      "UEHitrate: 0.0384320522649245  edgeHitrate 0.2862904569847672 sumHitrate 0.3247225092496917  privacy: tensor(1.9841, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:04:49 2021 --Episode: 4   Index: 40000   Reward: 1.3804254771653557   Loss: 2.540304474971246e-05\n",
      "UEHitrate: 0.04122396940076498  edgeHitrate 0.2861928451788705 sumHitrate 0.3274168145796355  privacy: tensor(1.9389, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:08:06 2021 --Episode: 4   Index: 50000   Reward: 1.3509234487194761   Loss: 2.5044262580742424e-05\n",
      "UEHitrate: 0.042279154416911664  edgeHitrate 0.2877742445151097 sumHitrate 0.33005339893202135  privacy: tensor(1.8951, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 02:08:11 2021 --End episode: 4   Reward: 1.350200248868138   Loss: 2.5035663069918765e-05\n",
      "UEHitrate: 0.042319406015486595  edgeHitrate 0.2878555646236837 sumHitrate 0.33017497063917034  privacy: tensor(1.8943, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 02:08:11 2021 --Episode: 5   Index: 0   Reward: 0.0   Loss: 0.00023055550991557539\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:11:25 2021 --Episode: 5   Index: 10000   Reward: 1.4768494298053922   Loss: 2.370898506553546e-05\n",
      "UEHitrate: 0.021897810218978103  edgeHitrate 0.21197880211978803 sumHitrate 0.2338766123387661  privacy: tensor(2.2042, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:14:41 2021 --Episode: 5   Index: 20000   Reward: 1.3030746935905912   Loss: 2.3470552914575823e-05\n",
      "UEHitrate: 0.029498525073746312  edgeHitrate 0.23843807809609519 sumHitrate 0.2679366031698415  privacy: tensor(2.0464, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:17:58 2021 --Episode: 5   Index: 30000   Reward: 1.2899444554057553   Loss: 2.3759905853610663e-05\n",
      "UEHitrate: 0.03416552781573948  edgeHitrate 0.2592580247325089 sumHitrate 0.29342355254824837  privacy: tensor(1.9839, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:21:17 2021 --Episode: 5   Index: 40000   Reward: 1.2888852799598638   Loss: 2.412928597044596e-05\n",
      "UEHitrate: 0.037799055023624406  edgeHitrate 0.2674433139171521 sumHitrate 0.3052423689407765  privacy: tensor(1.9339, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:24:38 2021 --Episode: 5   Index: 50000   Reward: 1.253266407590603   Loss: 2.3975288911516374e-05\n",
      "UEHitrate: 0.03941921161576768  edgeHitrate 0.2677546449071019 sumHitrate 0.30717385652286955  privacy: tensor(1.8919, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 02:24:43 2021 --End episode: 5   Reward: 1.252484567116273   Loss: 2.395830168638406e-05\n",
      "UEHitrate: 0.03943308716682923  edgeHitrate 0.26781057786093915 sumHitrate 0.3072436650277684  privacy: tensor(1.8910, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 02:24:44 2021 --Episode: 6   Index: 0   Reward: 0.0   Loss: 0.00018198203179053962\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:27:59 2021 --Episode: 6   Index: 10000   Reward: 1.4070774709661138   Loss: 2.1216221082270073e-05\n",
      "UEHitrate: 0.022297770222977704  edgeHitrate 0.19308069193080693 sumHitrate 0.21537846215378462  privacy: tensor(2.1993, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:31:22 2021 --Episode: 6   Index: 20000   Reward: 1.396662944541152   Loss: 2.3019928587784632e-05\n",
      "UEHitrate: 0.03309834508274586  edgeHitrate 0.25638718064096794 sumHitrate 0.28948552572371383  privacy: tensor(2.0383, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:34:36 2021 --Episode: 6   Index: 30000   Reward: 1.368034579099594   Loss: 2.4093899404562343e-05\n",
      "UEHitrate: 0.03693210226325789  edgeHitrate 0.2734908836372121 sumHitrate 0.31042298590046996  privacy: tensor(1.9725, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:38:03 2021 --Episode: 6   Index: 40000   Reward: 1.3818998153612647   Loss: 2.491391636329502e-05\n",
      "UEHitrate: 0.04067398315042124  edgeHitrate 0.2860428489287768 sumHitrate 0.326716832079198  privacy: tensor(1.9301, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:41:17 2021 --Episode: 6   Index: 50000   Reward: 1.3206021930023901   Loss: 2.4777056007057e-05\n",
      "UEHitrate: 0.04303913921721565  edgeHitrate 0.28099438011239775 sumHitrate 0.3240335193296134  privacy: tensor(1.8901, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 02:41:21 2021 --End episode: 6   Reward: 1.3202278267133245   Loss: 2.4761475972837284e-05\n",
      "UEHitrate: 0.04305591496307502  edgeHitrate 0.2810478332702988 sumHitrate 0.3241037482333738  privacy: tensor(1.8888, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 02:41:21 2021 --Episode: 7   Index: 0   Reward: 0.0   Loss: 0.0001960919180419296\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:44:42 2021 --Episode: 7   Index: 10000   Reward: 1.4380362048095325   Loss: 2.141532534522142e-05\n",
      "UEHitrate: 0.0197980201979802  edgeHitrate 0.19638036196380362 sumHitrate 0.2161783821617838  privacy: tensor(2.2317, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:48:02 2021 --Episode: 7   Index: 20000   Reward: 1.40088572229188   Loss: 2.292284963133573e-05\n",
      "UEHitrate: 0.029698515074246288  edgeHitrate 0.2550872456377181 sumHitrate 0.2847857607119644  privacy: tensor(2.0528, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:51:25 2021 --Episode: 7   Index: 30000   Reward: 1.3905400505440217   Loss: 2.4589581449656602e-05\n",
      "UEHitrate: 0.032598913369554346  edgeHitrate 0.27685743808539715 sumHitrate 0.3094563514549515  privacy: tensor(1.9875, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:54:54 2021 --Episode: 7   Index: 40000   Reward: 1.3805495420459497   Loss: 2.504829000285749e-05\n",
      "UEHitrate: 0.03769905752356191  edgeHitrate 0.2841928951776206 sumHitrate 0.3218919527011825  privacy: tensor(1.9405, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 02:58:20 2021 --Episode: 7   Index: 50000   Reward: 1.3162443756923274   Loss: 2.470217596881941e-05\n",
      "UEHitrate: 0.04011919761604768  edgeHitrate 0.2785544289114218 sumHitrate 0.31867362652746944  privacy: tensor(1.8953, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 02:58:24 2021 --End episode: 7   Reward: 1.3151600463428175   Loss: 2.468778908395746e-05\n",
      "UEHitrate: 0.04008997352548918  edgeHitrate 0.27857953301351596 sumHitrate 0.3186695065390051  privacy: tensor(1.8947, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 02:58:24 2021 --Episode: 8   Index: 0   Reward: 0.0   Loss: 0.0002307314280187711\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:01:48 2021 --Episode: 8   Index: 10000   Reward: 1.5837000627458178   Loss: 2.2771270186390286e-05\n",
      "UEHitrate: 0.030896910308969103  edgeHitrate 0.22567743225677434 sumHitrate 0.2565743425657434  privacy: tensor(2.1593, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:05:08 2021 --Episode: 8   Index: 20000   Reward: 1.2303028943222787   Loss: 2.1973943878456227e-05\n",
      "UEHitrate: 0.029498525073746312  edgeHitrate 0.21638918054097295 sumHitrate 0.24588770561471926  privacy: tensor(2.0118, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:08:32 2021 --Episode: 8   Index: 30000   Reward: 1.2035211854178043   Loss: 2.1657401346461852e-05\n",
      "UEHitrate: 0.03646545115162828  edgeHitrate 0.23412552914902837 sumHitrate 0.27059098030065665  privacy: tensor(1.9534, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:11:55 2021 --Episode: 8   Index: 40000   Reward: 1.2102858351974297   Loss: 2.2243425882120416e-05\n",
      "UEHitrate: 0.04147396315092123  edgeHitrate 0.24509387265318366 sumHitrate 0.2865678358041049  privacy: tensor(1.9149, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:15:17 2021 --Episode: 8   Index: 50000   Reward: 1.1852973761608523   Loss: 2.2335225477870664e-05\n",
      "UEHitrate: 0.04399912001759965  edgeHitrate 0.249135017299654 sumHitrate 0.29313413731725363  privacy: tensor(1.8745, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 03:15:22 2021 --End episode: 8   Reward: 1.1846698851457895   Loss: 2.2331255840764e-05\n",
      "UEHitrate: 0.04405119732468101  edgeHitrate 0.2491191751099787 sumHitrate 0.2931703724346597  privacy: tensor(1.8740, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Time: Tue Sep 14 03:15:22 2021 --Episode: 9   Index: 0   Reward: 0.0   Loss: 0.00019096062169410288\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: tensor(2.3921, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:18:40 2021 --Episode: 9   Index: 10000   Reward: 1.4847239720730043   Loss: 2.1532620704343487e-05\n",
      "UEHitrate: 0.015298470152984701  edgeHitrate 0.21307869213078692 sumHitrate 0.2283771622837716  privacy: tensor(2.1934, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:21:38 2021 --Episode: 9   Index: 20000   Reward: 1.5134383361587902   Loss: 2.4341394143362008e-05\n",
      "UEHitrate: 0.034098295085245735  edgeHitrate 0.2853357332133393 sumHitrate 0.3194340282985851  privacy: tensor(2.0370, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:24:05 2021 --Episode: 9   Index: 30000   Reward: 1.4439722934325412   Loss: 2.5786009540954662e-05\n",
      "UEHitrate: 0.03916536115462818  edgeHitrate 0.2930902303256558 sumHitrate 0.332255591480284  privacy: tensor(1.9761, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:26:26 2021 --Episode: 9   Index: 40000   Reward: 1.407246147210509   Loss: 2.572087343943569e-05\n",
      "UEHitrate: 0.04417389565260869  edgeHitrate 0.2934676633084173 sumHitrate 0.337641558961026  privacy: tensor(1.9300, dtype=torch.float64)\n",
      "\n",
      "Time: Tue Sep 14 03:28:46 2021 --Episode: 9   Index: 50000   Reward: 1.3671293816329284   Loss: 2.5483865809823718e-05\n",
      "UEHitrate: 0.045919081618367635  edgeHitrate 0.29249415011699764 sumHitrate 0.3384132317353653  privacy: tensor(1.8902, dtype=torch.float64)\n",
      "\n",
      "----------------------------------------------------------------\n",
      "Time: Tue Sep 14 03:28:49 2021 --End episode: 9   Reward: 1.3668115745332137   Loss: 2.546933636017093e-05\n",
      "UEHitrate: 0.045922328164500266  edgeHitrate 0.292553297370464 sumHitrate 0.33847562553496424  privacy: tensor(1.8890, dtype=torch.float64)\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_episodes = 10\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  0\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = 0\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward += float(ue.reward.sum())\n",
    "        \n",
    "        if index % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if index % 10000 == 0:\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"Time:\",time.asctime( time.localtime(time.time())),\"--Episode:\",i_episode,\"  Index:\",index,\"  Reward:\",sumReward/(index+1),\"  Loss:\",float(loss/(index+1)))\n",
    "            print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1),\" privacy:\",psi/len(UEs))\n",
    "            \n",
    "            print()\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        \n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Time:\",time.asctime( time.localtime(time.time())),\"--End episode:\",i_episode,\"  Reward:\",sumReward/(index+1),\"  Loss:\",loss/(index+1))\n",
    "    print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1),\" privacy:\",psi/len(UEs))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "\n",
    "    if sumReward > bestReward:\n",
    "        bestReward = sumReward\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+'ep{}_'.format(bestEpisode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "    \n",
    "\n",
    "\n",
    "    env = ENV(userNum,contentNum)\n",
    "    UEs = {}\n",
    "    sumReward = 0\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  0\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = 0\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward += ue.reward.sum()\n",
    "        \n",
    "        if index % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += optimize_model()\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if index % 10000 == 0:\n",
    "            print(\"Time:\",time.asctime( time.localtime(time.time())),\"--Episode:\",i_episode,\"  Index:\",index,\"  Reward:\",sumReward/(index+1),\"  Loss:\",float(loss/(index+1)))\n",
    "            print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1))\n",
    "            print()\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            \n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Time:\",time.asctime( time.localtime(time.time())),\"--End episode:\",i_episode,\"  Reward:\",sumReward/(index+1),\"  Loss:\",loss/(index+1))\n",
    "    print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "\n",
    "    if sumReward > bestReward:\n",
    "        bestReward = sumReward\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+time.strftime('ep{}_'.format(bestEpisode)+\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "    \n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) *     (1-p)).sum()\n",
    "\n",
    "    print(psi/contentNum)\n",
    "        \n",
    "\n",
    "    env = ENV(userNum,contentNum)\n",
    "    UEs = {}\n",
    "    sumReward = 0\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_episodes = 5\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  0\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = 0\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward += ue.reward.sum()\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += optimize_model()\n",
    "        \n",
    "        if index % 1000 == 0:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if index % 10000 == 0:\n",
    "            print(\"Time:\",time.asctime( time.localtime(time.time())),\"--Episode:\",i_episode,\"  Index:\",index,\"  Reward:\",sumReward/(index+1),\"  Loss:\",float(loss/(index+1)))\n",
    "            print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1))\n",
    "            print()\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            \n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Time:\",time.asctime( time.localtime(time.time())),\"--End episode:\",i_episode,\"  Reward:\",sumReward/(index+1),\"  Loss:\",loss/(index+1))\n",
    "    print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "\n",
    "    if sumReward > bestReward:\n",
    "        bestReward = sumReward\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+time.strftime('ep{}_'.format(bestEpisode)+\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "    \n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) *     (1-p)).sum()\n",
    "\n",
    "    print(psi/contentNum)\n",
    "        \n",
    "\n",
    "    env = ENV(userNum,contentNum)\n",
    "    UEs = {}\n",
    "    sumReward = 0\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 3\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  0\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = 0\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward += ue.reward.sum()\n",
    "        \n",
    "        if index % 100 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += optimize_model()\n",
    "        \n",
    "        if index % 1000 == 0:\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        if index % 10000 == 0:\n",
    "            print(\"Time:\",time.asctime( time.localtime(time.time())),\"--Episode:\",i_episode,\"  Index:\",index,\"  Reward:\",sumReward/(index+1),\"  Loss:\",float(loss/(index+1)))\n",
    "            print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1))\n",
    "            print()\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            \n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"Time:\",time.asctime( time.localtime(time.time())),\"--End episode:\",i_episode,\"  Reward:\",sumReward/(index+1),\"  Loss:\",loss/(index+1))\n",
    "    print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "\n",
    "    if sumReward > bestReward:\n",
    "        bestReward = sumReward\n",
    "        bestLoss = loss\n",
    "        bestUEHit = UEHit\n",
    "        bestEdgeHit = edgeHit\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = MODELPATH+time.strftime('ep{}_'.format(bestEpisode)+\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "        torch.save(policy_net.state_dict(),bestPath)\n",
    "    \n",
    "\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) *     (1-p)).sum()\n",
    "\n",
    "    print(psi/contentNum)\n",
    "        \n",
    "\n",
    "    env.reset()\n",
    "    UEs = {}\n",
    "    sumReward = 0\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = 0\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "\n",
    "for index,trace in trainUIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    sumReward += ue.reward.sum()\n",
    "    \n",
    "    if index % 10000 == 0:\n",
    "        print(\"Time:\",time.asctime( time.localtime(time.time())),\"--Episode:\",i_episode,\"  Index:\",index,\" Reward:\",sumReward/(index+1),)\n",
    "        print(\"UEHitrate:\",UEHit.sum()/(index+1),\" edgeHitrate\",edgeHit/(index+1),\"sumHitrate\",(edgeHit+UEHit.sum())/(index+1))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "psi/contentNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
