{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8812</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4058</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1549</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148          11            21     0   \n",
       "1       203  5779    0        0         7          11             4     0   \n",
       "2       208  4675    0        0        92          13             4     0   \n",
       "3       159   332    0        0        56          11             3     0   \n",
       "4        50   674    0        0       439          11             4     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831   29  2591880        34          11             4     0   \n",
       "300979  158  8448   29  2591880        34          11             4     0   \n",
       "300980  483  6463   29  2591940        35          11             4     0   \n",
       "300981  158  4715   29  2591940        34          11             4     0   \n",
       "300982  483  2021   29  2591940        34          11             4     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0       8812          1            0  \n",
       "1              0       9063          1            0  \n",
       "2              0       3444          1            0  \n",
       "3              0       4058          1            0  \n",
       "4              0       1549          1            0  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0       7061          1            0  \n",
       "300979         0      11316          1            0  \n",
       "300980         0       7061          1            0  \n",
       "300981         0      11316          1            0  \n",
       "300982         0       7061          1            0  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "validUIT = UIT[UIT['day']<24]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000,\n",
       " 500,\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148          11            21     0   \n",
       " 1       203  5779    0        0         7          11             4     0   \n",
       " 2       208  4675    0        0        92          13             4     0   \n",
       " 3       159   332    0        0        56          11             3     0   \n",
       " 4        50   674    0        0       439          11             4     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 198170  264  7442   17  1555140        90          13             4     0   \n",
       " 198171   19  9362   17  1555140       424          13             4     0   \n",
       " 198172   82  9223   17  1555140        94          15             4     0   \n",
       " 198173   35  4164   17  1555140        22          11             4     0   \n",
       " 198174  239  5062   17  1555140        89          13             4     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0       8812          1            0  \n",
       " 1              0       9063          1            0  \n",
       " 2              0       3444          1            0  \n",
       " 3              0       4058          1            0  \n",
       " 4              0       1549          1            0  \n",
       " ...          ...        ...        ...          ...  \n",
       " 198170         0       5639          1            0  \n",
       " 198171         0       4004          1            0  \n",
       " 198172         0       8398          1            0  \n",
       " 198173         0       3934          1            0  \n",
       " 198174         0      11717          1            0  \n",
       " \n",
       " [198175 rows x 12 columns],\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148          11            21     0   \n",
       " 1       203  5779    0        0         7          11             4     0   \n",
       " 2       208  4675    0        0        92          13             4     0   \n",
       " 3       159   332    0        0        56          11             3     0   \n",
       " 4        50   674    0        0       439          11             4     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 253625   57  8385   23  2073540        26          11             4     0   \n",
       " 253626   74  9218   23  2073540       601          20             4     0   \n",
       " 253627    7  2757   23  2073540       133          11             4     0   \n",
       " 253628  240  3252   23  2073540        70           4             1     0   \n",
       " 253629   76  7213   23  2073540        34          11             3     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0       8812          1            0  \n",
       " 1              0       9063          1            0  \n",
       " 2              0       3444          1            0  \n",
       " 3              0       4058          1            0  \n",
       " 4              0       1549          1            0  \n",
       " ...          ...        ...        ...          ...  \n",
       " 253625         0       1903          1            0  \n",
       " 253626         0       4944          1            0  \n",
       " 253627         0       5436          1            0  \n",
       " 253628         0       6608          1            0  \n",
       " 253629         0        571          1            0  \n",
       " \n",
       " [253630 rows x 12 columns])"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    " \n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p), \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                torch.from_numpy(self.l))\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "\n",
    "    def statusEmbedding(self):\n",
    "        statusFeature = torch.zeros(size=(5,env.contentNum)).to(device)\n",
    "        \n",
    "        statusFeature[0] = self.v\n",
    "        statusFeature[1] = self.r[self.u]\n",
    "        statusFeature[2] = self.p\n",
    "        statusFeature[3] = self.e\n",
    "        statusFeature[4] = self.S\n",
    "\n",
    "        #statusFeature[5] = status['r']\n",
    "        return statusFeature.T\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,l,e,v):\n",
    "        \n",
    "        self.Rh =   self.ALPHAh * (torch.log(v * p + (1-v) * (1-p)).sum() / torch.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)).sum() - torch.log(v * p + (1-v) * (1-p)).sum())\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + torch.exp( torch.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        #self.Rh =   self.ALPHAh * torch.log(ru * p + (1-ru) * (1-p))\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * action[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - action[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  self.Rh + self.Ro + self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        self.lastStatusFeature = self.statusFeature\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.r , self.p , self.e, self.S, self.l = env.getStatus()\n",
    "        self.statusFeature = self.statusEmbedding()\n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.l,self.e,self.v)\n",
    "\n",
    "        if self.W[-1] not in np.argwhere(self.lastAction.numpy()): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            if  not train or (train and sample > eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                with torch.no_grad():\n",
    "                    Q_value = QNetwork(self.statusFeature)\n",
    "                    actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                QNetwork.train()\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "\n",
    "            self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "            self.action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                self.action[index] = 1\n",
    "            env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "        else:\n",
    "            self.action = self.lastAction # keep the cache and no request the new video\n",
    "\n",
    "        if train:\n",
    "            lastAction = torch.cat(((1-self.lastAction).unsqueeze(1),self.lastAction.unsqueeze(1)),1)\n",
    "            memory.push(self.lastStatusFeature, \n",
    "                        lastAction.to(device), \n",
    "                        self.statusFeature,\n",
    "                        torch.tensor([self.reward.float()]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "                        \n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName+'dnn_h1_'\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 15\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if index % 25 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "        if index % 100 == 0 :\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            if index % 50000 == 0 :\n",
    "                psi = 0\n",
    "                p = torch.from_numpy(env.p)\n",
    "                for u in UEs:\n",
    "                    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "                print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "                print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "                print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "                print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.action)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "\n",
    "        if index % 50000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation----------------------------------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sat Oct  9 14:19:01 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 14:28:45 2021 Episode: 0   Index: 50000   Loss: 0.31137 --\n",
      "Reward: [0.2944  0.00501 0.06872] total reward: 0.36813\n",
      "UEHitrate: 0.0061  edgeHitrate 0.0859 sumHitrate 0.092  privacy: 2.59515\n",
      "\n",
      "--Time: Sat Oct  9 14:40:07 2021 Episode: 0   Index: 100000   Loss: 0.29417 --\n",
      "Reward: [0.35983 0.00475 0.06402] total reward: 0.42861\n",
      "UEHitrate: 0.00568  edgeHitrate 0.08003 sumHitrate 0.08571  privacy: 1.92321\n",
      "\n",
      "--Time: Sat Oct  9 14:51:33 2021 Episode: 0   Index: 150000   Loss: 0.28306 --\n",
      "Reward: [0.43629 0.00517 0.06268] total reward: 0.50414\n",
      "UEHitrate: 0.00619  edgeHitrate 0.07835 sumHitrate 0.08453  privacy: 1.4909\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 15:03:13 2021 Episode: 0   Index: 198174   Loss: 0.27341 --\n",
      "Reward: [0.51324 0.00546 0.06264] total reward: 0.58135\n",
      "UEHitrate: 0.00652  edgeHitrate 0.0783 sumHitrate 0.08483  privacy: 1.17501\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 15:03:13 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 15:07:31 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.9994  0.01784 0.23558] total reward: 1.25282\n",
      "UEHitrate: 0.04632  edgeHitrate 0.29447 sumHitrate 0.34079  privacy: 0.81166\n",
      "\n",
      "--Time: Sat Oct  9 15:13:14 2021 Episode: 0   Index: 100000   Loss: 0.0 --\n",
      "Reward: [1.1875  0.01716 0.21805] total reward: 1.42271\n",
      "UEHitrate: 0.04238  edgeHitrate 0.27257 sumHitrate 0.31495  privacy: 0.63984\n",
      "\n",
      "--Time: Sat Oct  9 15:19:21 2021 Episode: 0   Index: 150000   Loss: 0.0 --\n",
      "Reward: [1.42177 0.01702 0.20241] total reward: 1.64119\n",
      "UEHitrate: 0.04007  edgeHitrate 0.25301 sumHitrate 0.29308  privacy: 0.52046\n",
      "\n",
      "--Time: Sat Oct  9 15:24:59 2021 Episode: 0   Index: 200000   Loss: 0.0 --\n",
      "Reward: [1.62022 0.01627 0.19555] total reward: 1.83204\n",
      "UEHitrate: 0.0373  edgeHitrate 0.24444 sumHitrate 0.28174  privacy: 0.43955\n",
      "\n",
      "--Time: Sat Oct  9 15:30:22 2021 Episode: 0   Index: 250000   Loss: 0.0 --\n",
      "Reward: [1.87493 0.01594 0.19284] total reward: 2.08371\n",
      "UEHitrate: 0.03612  edgeHitrate 0.24105 sumHitrate 0.27717  privacy: 0.36289\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 15:30:49 2021 Episode: 0   Index: 253629   Loss: 0.0 --\n",
      "Reward: [1.89388 0.01588 0.19244] total reward: 2.1022\n",
      "UEHitrate: 0.03593  edgeHitrate 0.24056 sumHitrate 0.27649  privacy: 0.35974\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h1_ep0_1009-15-30-49\n",
      "\n",
      "--Time: Sat Oct  9 15:30:49 2021 Episode: 1   Index: 0   Loss: 7.75575 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 15:42:33 2021 Episode: 1   Index: 50000   Loss: 0.20804 --\n",
      "Reward: [0.29301 0.00458 0.07365] total reward: 0.37123\n",
      "UEHitrate: 0.00554  edgeHitrate 0.09206 sumHitrate 0.0976  privacy: 2.7257\n",
      "\n",
      "--Time: Sat Oct  9 15:54:53 2021 Episode: 1   Index: 100000   Loss: 0.20173 --\n",
      "Reward: [0.34496 0.00455 0.06906] total reward: 0.41857\n",
      "UEHitrate: 0.00558  edgeHitrate 0.08633 sumHitrate 0.09191  privacy: 2.18071\n",
      "\n",
      "--Time: Sat Oct  9 16:07:22 2021 Episode: 1   Index: 150000   Loss: 0.19515 --\n",
      "Reward: [0.4016  0.00612 0.07575] total reward: 0.48347\n",
      "UEHitrate: 0.00865  edgeHitrate 0.09469 sumHitrate 0.10333  privacy: 1.75996\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 16:19:26 2021 Episode: 1   Index: 198174   Loss: 0.18901 --\n",
      "Reward: [0.45661 0.00706 0.07561] total reward: 0.53928\n",
      "UEHitrate: 0.01021  edgeHitrate 0.09451 sumHitrate 0.10473  privacy: 1.46823\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 16:19:26 2021 Episode: 1   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 16:24:15 2021 Episode: 1   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.4329  0.01495 0.26579] total reward: 0.71364\n",
      "UEHitrate: 0.0441  edgeHitrate 0.33223 sumHitrate 0.37633  privacy: 2.35852\n",
      "\n",
      "--Time: Sat Oct  9 16:29:23 2021 Episode: 1   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.47873 0.01565 0.24517] total reward: 0.73955\n",
      "UEHitrate: 0.04566  edgeHitrate 0.30647 sumHitrate 0.35213  privacy: 2.1481\n",
      "\n",
      "--Time: Sat Oct  9 16:35:11 2021 Episode: 1   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.50618 0.0163  0.23708] total reward: 0.75955\n",
      "UEHitrate: 0.04649  edgeHitrate 0.29634 sumHitrate 0.34284  privacy: 2.03857\n",
      "\n",
      "--Time: Sat Oct  9 16:41:22 2021 Episode: 1   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.52664 0.01639 0.23355] total reward: 0.77658\n",
      "UEHitrate: 0.04533  edgeHitrate 0.29194 sumHitrate 0.33727  privacy: 1.95119\n",
      "\n",
      "--Time: Sat Oct  9 16:46:41 2021 Episode: 1   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.54194 0.01667 0.22864] total reward: 0.78725\n",
      "UEHitrate: 0.04524  edgeHitrate 0.2858 sumHitrate 0.33103  privacy: 1.88129\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 16:47:07 2021 Episode: 1   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.543   0.01658 0.2282 ] total reward: 0.78777\n",
      "UEHitrate: 0.04497  edgeHitrate 0.28525 sumHitrate 0.33021  privacy: 1.87701\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 16:47:07 2021 Episode: 2   Index: 0   Loss: 6.96615 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 17:00:37 2021 Episode: 2   Index: 50000   Loss: 0.18497 --\n",
      "Reward: [0.30339 0.0091  0.09521] total reward: 0.4077\n",
      "UEHitrate: 0.0186  edgeHitrate 0.11902 sumHitrate 0.13762  privacy: 2.7679\n",
      "\n",
      "--Time: Sat Oct  9 17:13:55 2021 Episode: 2   Index: 100000   Loss: 0.17897 --\n",
      "Reward: [0.33952 0.00934 0.08486] total reward: 0.43373\n",
      "UEHitrate: 0.01893  edgeHitrate 0.10608 sumHitrate 0.12501  privacy: 2.36257\n",
      "\n",
      "--Time: Sat Oct  9 17:26:09 2021 Episode: 2   Index: 150000   Loss: 0.17345 --\n",
      "Reward: [0.37948 0.00856 0.07355] total reward: 0.4616\n",
      "UEHitrate: 0.01621  edgeHitrate 0.09194 sumHitrate 0.10815  privacy: 2.10052\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 17:38:24 2021 Episode: 2   Index: 198174   Loss: 0.16462 --\n",
      "Reward: [0.41236 0.00868 0.07132] total reward: 0.49235\n",
      "UEHitrate: 0.01562  edgeHitrate 0.08915 sumHitrate 0.10477  privacy: 1.9025\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 17:38:24 2021 Episode: 2   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 17:43:35 2021 Episode: 2   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.80719 0.02267 0.17672] total reward: 1.00658\n",
      "UEHitrate: 0.03726  edgeHitrate 0.2209 sumHitrate 0.25815  privacy: 1.20061\n",
      "\n",
      "--Time: Sat Oct  9 17:48:35 2021 Episode: 2   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.85327 0.02213 0.13293] total reward: 1.00833\n",
      "UEHitrate: 0.03424  edgeHitrate 0.16617 sumHitrate 0.20041  privacy: 1.12136\n",
      "\n",
      "--Time: Sat Oct  9 17:54:51 2021 Episode: 2   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.8734  0.02127 0.12059] total reward: 1.01527\n",
      "UEHitrate: 0.03285  edgeHitrate 0.15074 sumHitrate 0.18359  privacy: 1.08579\n",
      "\n",
      "--Time: Sat Oct  9 18:00:31 2021 Episode: 2   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.88752 0.02004 0.11116] total reward: 1.01872\n",
      "UEHitrate: 0.03036  edgeHitrate 0.13895 sumHitrate 0.16932  privacy: 1.06979\n",
      "\n",
      "--Time: Sat Oct  9 18:05:40 2021 Episode: 2   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.89781 0.0196  0.10439] total reward: 1.0218\n",
      "UEHitrate: 0.02927  edgeHitrate 0.13049 sumHitrate 0.15976  privacy: 1.05862\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 18:06:00 2021 Episode: 2   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.8985  0.01954 0.10366] total reward: 1.0217\n",
      "UEHitrate: 0.02917  edgeHitrate 0.12957 sumHitrate 0.15875  privacy: 1.05794\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 18:06:00 2021 Episode: 3   Index: 0   Loss: 5.75204 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 18:19:30 2021 Episode: 3   Index: 50000   Loss: 0.15928 --\n",
      "Reward: [0.3227  0.01953 0.15692] total reward: 0.49916\n",
      "UEHitrate: 0.03094  edgeHitrate 0.19616 sumHitrate 0.2271  privacy: 2.71433\n",
      "\n",
      "--Time: Sat Oct  9 18:31:38 2021 Episode: 3   Index: 100000   Loss: 0.15504 --\n",
      "Reward: [0.34908 0.01817 0.1202 ] total reward: 0.48746\n",
      "UEHitrate: 0.0275  edgeHitrate 0.15025 sumHitrate 0.17775  privacy: 2.42628\n",
      "\n",
      "--Time: Sat Oct  9 18:43:50 2021 Episode: 3   Index: 150000   Loss: 0.1516 --\n",
      "Reward: [0.38029 0.01774 0.10266] total reward: 0.50069\n",
      "UEHitrate: 0.02637  edgeHitrate 0.12833 sumHitrate 0.15469  privacy: 2.23411\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 18:56:24 2021 Episode: 3   Index: 198174   Loss: 0.14715 --\n",
      "Reward: [0.4046  0.01687 0.092  ] total reward: 0.51346\n",
      "UEHitrate: 0.02456  edgeHitrate 0.115 sumHitrate 0.13956  privacy: 2.07627\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 18:56:24 2021 Episode: 3   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 19:01:45 2021 Episode: 3   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.80724 0.02285 0.19785] total reward: 1.02795\n",
      "UEHitrate: 0.03706  edgeHitrate 0.24732 sumHitrate 0.28437  privacy: 1.20047\n",
      "\n",
      "--Time: Sat Oct  9 19:07:07 2021 Episode: 3   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.8533  0.02234 0.14696] total reward: 1.0226\n",
      "UEHitrate: 0.03423  edgeHitrate 0.1837 sumHitrate 0.21793  privacy: 1.12131\n",
      "\n",
      "--Time: Sat Oct  9 19:12:40 2021 Episode: 3   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.87344 0.02153 0.1324 ] total reward: 1.02737\n",
      "UEHitrate: 0.03287  edgeHitrate 0.1655 sumHitrate 0.19837  privacy: 1.08579\n",
      "\n",
      "--Time: Sat Oct  9 19:17:56 2021 Episode: 3   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.88754 0.02034 0.12088] total reward: 1.02876\n",
      "UEHitrate: 0.03055  edgeHitrate 0.15109 sumHitrate 0.18165  privacy: 1.06979\n",
      "\n",
      "--Time: Sat Oct  9 19:23:29 2021 Episode: 3   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.89782 0.01986 0.11246] total reward: 1.03015\n",
      "UEHitrate: 0.02941  edgeHitrate 0.14058 sumHitrate 0.16999  privacy: 1.0586\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 19:23:49 2021 Episode: 3   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.89851 0.01979 0.11164] total reward: 1.02994\n",
      "UEHitrate: 0.02928  edgeHitrate 0.13955 sumHitrate 0.16883  privacy: 1.05793\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 19:23:50 2021 Episode: 4   Index: 0   Loss: 5.28761 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 19:37:40 2021 Episode: 4   Index: 50000   Loss: 0.1288 --\n",
      "Reward: [0.34591 0.01889 0.13806] total reward: 0.50286\n",
      "UEHitrate: 0.03076  edgeHitrate 0.17258 sumHitrate 0.20334  privacy: 2.60097\n",
      "\n",
      "--Time: Sat Oct  9 19:50:04 2021 Episode: 4   Index: 100000   Loss: 0.12606 --\n",
      "Reward: [0.36494 0.01786 0.1043 ] total reward: 0.4871\n",
      "UEHitrate: 0.02709  edgeHitrate 0.13038 sumHitrate 0.15747  privacy: 2.41436\n",
      "\n",
      "--Time: Sat Oct  9 20:02:34 2021 Episode: 4   Index: 150000   Loss: 0.12411 --\n",
      "Reward: [0.38877 0.01732 0.09153] total reward: 0.49763\n",
      "UEHitrate: 0.02592  edgeHitrate 0.11441 sumHitrate 0.14033  privacy: 2.2883\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 20:15:06 2021 Episode: 4   Index: 198174   Loss: 0.12242 --\n",
      "Reward: [0.40725 0.01599 0.08313] total reward: 0.50638\n",
      "UEHitrate: 0.02337  edgeHitrate 0.10392 sumHitrate 0.12729  privacy: 2.17203\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 20:15:06 2021 Episode: 4   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 20:20:15 2021 Episode: 4   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.80745 0.0265  0.24032] total reward: 1.07427\n",
      "UEHitrate: 0.04712  edgeHitrate 0.30039 sumHitrate 0.34751  privacy: 1.20013\n",
      "\n",
      "--Time: Sat Oct  9 20:25:09 2021 Episode: 4   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.8535  0.02646 0.18127] total reward: 1.06122\n",
      "UEHitrate: 0.04373  edgeHitrate 0.22659 sumHitrate 0.27032  privacy: 1.12119\n",
      "\n",
      "--Time: Sat Oct  9 20:30:56 2021 Episode: 4   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.87355 0.026   0.1595 ] total reward: 1.05906\n",
      "UEHitrate: 0.04214  edgeHitrate 0.19938 sumHitrate 0.24152  privacy: 1.08566\n",
      "\n",
      "--Time: Sat Oct  9 20:36:10 2021 Episode: 4   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.88766 0.02455 0.14078] total reward: 1.05299\n",
      "UEHitrate: 0.03859  edgeHitrate 0.17597 sumHitrate 0.21456  privacy: 1.06973\n",
      "\n",
      "--Time: Sat Oct  9 20:41:06 2021 Episode: 4   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.89796 0.02403 0.12815] total reward: 1.05014\n",
      "UEHitrate: 0.03687  edgeHitrate 0.16018 sumHitrate 0.19706  privacy: 1.05857\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 20:41:28 2021 Episode: 4   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.89865 0.02394 0.12718] total reward: 1.04977\n",
      "UEHitrate: 0.03673  edgeHitrate 0.15898 sumHitrate 0.19571  privacy: 1.0579\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 20:41:28 2021 Episode: 5   Index: 0   Loss: 4.77832 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 20:55:10 2021 Episode: 5   Index: 50000   Loss: 0.1254 --\n",
      "Reward: [0.37761 0.01969 0.15036] total reward: 0.54766\n",
      "UEHitrate: 0.03052  edgeHitrate 0.18796 sumHitrate 0.21848  privacy: 2.45562\n",
      "\n",
      "--Time: Sat Oct  9 21:07:51 2021 Episode: 5   Index: 100000   Loss: 0.1234 --\n",
      "Reward: [0.39029 0.01887 0.11386] total reward: 0.52303\n",
      "UEHitrate: 0.0278  edgeHitrate 0.14233 sumHitrate 0.17013  privacy: 2.3423\n",
      "\n",
      "--Time: Sat Oct  9 21:20:23 2021 Episode: 5   Index: 150000   Loss: 0.12195 --\n",
      "Reward: [0.40819 0.01874 0.10015] total reward: 0.52709\n",
      "UEHitrate: 0.02767  edgeHitrate 0.12519 sumHitrate 0.15285  privacy: 2.27326\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 21:33:09 2021 Episode: 5   Index: 198174   Loss: 0.12062 --\n",
      "Reward: [0.42237 0.01816 0.09192] total reward: 0.53245\n",
      "UEHitrate: 0.02625  edgeHitrate 0.1149 sumHitrate 0.14115  privacy: 2.19772\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 21:33:09 2021 Episode: 5   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 21:38:16 2021 Episode: 5   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.80759 0.02732 0.23785] total reward: 1.07276\n",
      "UEHitrate: 0.04804  edgeHitrate 0.29731 sumHitrate 0.34535  privacy: 1.19997\n",
      "\n",
      "--Time: Sat Oct  9 21:43:19 2021 Episode: 5   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.85358 0.02781 0.18905] total reward: 1.07043\n",
      "UEHitrate: 0.04636  edgeHitrate 0.23631 sumHitrate 0.28267  privacy: 1.12102\n",
      "\n",
      "--Time: Sat Oct  9 21:49:17 2021 Episode: 5   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.87369 0.02778 0.17406] total reward: 1.07553\n",
      "UEHitrate: 0.04632  edgeHitrate 0.21757 sumHitrate 0.26389  privacy: 1.0855\n",
      "\n",
      "--Time: Sat Oct  9 21:54:41 2021 Episode: 5   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.8878  0.02593 0.1548 ] total reward: 1.06853\n",
      "UEHitrate: 0.04218  edgeHitrate 0.1935 sumHitrate 0.23568  privacy: 1.06958\n",
      "\n",
      "--Time: Sat Oct  9 22:00:39 2021 Episode: 5   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.89809 0.02515 0.13976] total reward: 1.06299\n",
      "UEHitrate: 0.03976  edgeHitrate 0.1747 sumHitrate 0.21446  privacy: 1.05843\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 22:01:05 2021 Episode: 5   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.89877 0.02506 0.13857] total reward: 1.0624\n",
      "UEHitrate: 0.03957  edgeHitrate 0.17321 sumHitrate 0.21278  privacy: 1.05774\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 22:01:06 2021 Episode: 6   Index: 0   Loss: 4.18331 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 22:14:52 2021 Episode: 6   Index: 50000   Loss: 0.12611 --\n",
      "Reward: [0.4081 0.0231 0.164 ] total reward: 0.5952\n",
      "UEHitrate: 0.03656  edgeHitrate 0.205 sumHitrate 0.24156  privacy: 2.28977\n",
      "\n",
      "--Time: Sat Oct  9 22:27:51 2021 Episode: 6   Index: 100000   Loss: 0.12416 --\n",
      "Reward: [0.41688 0.01934 0.10808] total reward: 0.5443\n",
      "UEHitrate: 0.02831  edgeHitrate 0.1351 sumHitrate 0.16341  privacy: 2.23265\n",
      "\n",
      "--Time: Sat Oct  9 22:40:40 2021 Episode: 6   Index: 150000   Loss: 0.1227 --\n",
      "Reward: [0.43173 0.01534 0.08363] total reward: 0.53071\n",
      "UEHitrate: 0.02187  edgeHitrate 0.10454 sumHitrate 0.12641  privacy: 2.20704\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct  9 22:53:18 2021 Episode: 6   Index: 198174   Loss: 0.1207 --\n",
      "Reward: [0.4417  0.01321 0.07142] total reward: 0.52633\n",
      "UEHitrate: 0.01827  edgeHitrate 0.08927 sumHitrate 0.10755  privacy: 2.16505\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 22:53:18 2021 Episode: 6   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 22:58:17 2021 Episode: 6   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.80804 0.01742 0.2066 ] total reward: 1.03207\n",
      "UEHitrate: 0.0331  edgeHitrate 0.25825 sumHitrate 0.29135  privacy: 1.19965\n",
      "\n",
      "--Time: Sat Oct  9 23:03:32 2021 Episode: 6   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.85398 0.01644 0.15745] total reward: 1.02788\n",
      "UEHitrate: 0.03017  edgeHitrate 0.19682 sumHitrate 0.22699  privacy: 1.12086\n",
      "\n",
      "--Time: Sat Oct  9 23:09:03 2021 Episode: 6   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.87406 0.01643 0.1454 ] total reward: 1.03589\n",
      "UEHitrate: 0.03015  edgeHitrate 0.18175 sumHitrate 0.21191  privacy: 1.08539\n",
      "\n",
      "--Time: Sat Oct  9 23:14:49 2021 Episode: 6   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.88809 0.01555 0.12896] total reward: 1.0326\n",
      "UEHitrate: 0.02742  edgeHitrate 0.1612 sumHitrate 0.18862  privacy: 1.06951\n",
      "\n",
      "--Time: Sat Oct  9 23:21:00 2021 Episode: 6   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.89833 0.01522 0.1172 ] total reward: 1.03075\n",
      "UEHitrate: 0.02586  edgeHitrate 0.1465 sumHitrate 0.17236  privacy: 1.05835\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sat Oct  9 23:21:24 2021 Episode: 6   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.899   0.01516 0.11627] total reward: 1.03044\n",
      "UEHitrate: 0.02569  edgeHitrate 0.14534 sumHitrate 0.17103  privacy: 1.05769\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sat Oct  9 23:21:24 2021 Episode: 7   Index: 0   Loss: 3.59448 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Oct  9 23:35:29 2021 Episode: 7   Index: 50000   Loss: 0.12583 --\n",
      "Reward: [0.44516 0.00565 0.05891] total reward: 0.50973\n",
      "UEHitrate: 0.00714  edgeHitrate 0.07364 sumHitrate 0.08078  privacy: 2.13082\n",
      "\n",
      "--Time: Sat Oct  9 23:48:28 2021 Episode: 7   Index: 100000   Loss: 0.12326 --\n",
      "Reward: [0.45122 0.00508 0.0463 ] total reward: 0.50259\n",
      "UEHitrate: 0.00607  edgeHitrate 0.05787 sumHitrate 0.06394  privacy: 2.1057\n",
      "\n",
      "--Time: Sun Oct 10 00:01:34 2021 Episode: 7   Index: 150000   Loss: 0.12154 --\n",
      "Reward: [0.46072 0.00541 0.04204] total reward: 0.50817\n",
      "UEHitrate: 0.00629  edgeHitrate 0.05255 sumHitrate 0.05883  privacy: 2.1071\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 00:14:15 2021 Episode: 7   Index: 198174   Loss: 0.11998 --\n",
      "Reward: [0.46841 0.00559 0.03948] total reward: 0.51348\n",
      "UEHitrate: 0.00633  edgeHitrate 0.04936 sumHitrate 0.05569  privacy: 2.08918\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 00:14:15 2021 Episode: 7   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 00:19:05 2021 Episode: 7   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.81091 0.0168  0.21409] total reward: 1.0418\n",
      "UEHitrate: 0.03206  edgeHitrate 0.26761 sumHitrate 0.29967  privacy: 1.19342\n",
      "\n",
      "--Time: Sun Oct 10 00:24:18 2021 Episode: 7   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.85688 0.01519 0.16713] total reward: 1.0392\n",
      "UEHitrate: 0.02785  edgeHitrate 0.20891 sumHitrate 0.23676  privacy: 1.11624\n",
      "\n",
      "--Time: Sun Oct 10 00:29:30 2021 Episode: 7   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.87701 0.01488 0.14414] total reward: 1.03603\n",
      "UEHitrate: 0.02624  edgeHitrate 0.18017 sumHitrate 0.20641  privacy: 1.08214\n",
      "\n",
      "--Time: Sun Oct 10 00:34:32 2021 Episode: 7   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.89094 0.01409 0.12613] total reward: 1.03116\n",
      "UEHitrate: 0.02372  edgeHitrate 0.15766 sumHitrate 0.18139  privacy: 1.06675\n",
      "\n",
      "--Time: Sun Oct 10 00:39:46 2021 Episode: 7   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.90113 0.01389 0.11543] total reward: 1.03045\n",
      "UEHitrate: 0.02261  edgeHitrate 0.14429 sumHitrate 0.1669  privacy: 1.05598\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 00:40:13 2021 Episode: 7   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.9018  0.01383 0.11455] total reward: 1.03017\n",
      "UEHitrate: 0.02248  edgeHitrate 0.14318 sumHitrate 0.16566  privacy: 1.05532\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 00:40:13 2021 Episode: 8   Index: 0   Loss: 2.97238 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 00:54:44 2021 Episode: 8   Index: 50000   Loss: 0.12356 --\n",
      "Reward: [0.40083 0.00445 0.04363] total reward: 0.44891\n",
      "UEHitrate: 0.00474  edgeHitrate 0.05454 sumHitrate 0.05928  privacy: 2.18704\n",
      "\n",
      "--Time: Sun Oct 10 01:08:14 2021 Episode: 8   Index: 100000   Loss: 0.12148 --\n",
      "Reward: [0.42904 0.00431 0.0376 ] total reward: 0.47095\n",
      "UEHitrate: 0.00455  edgeHitrate 0.047 sumHitrate 0.05155  privacy: 2.07581\n",
      "\n",
      "--Time: Sun Oct 10 01:21:53 2021 Episode: 8   Index: 150000   Loss: 0.11989 --\n",
      "Reward: [0.44771 0.00482 0.03542] total reward: 0.48795\n",
      "UEHitrate: 0.00515  edgeHitrate 0.04428 sumHitrate 0.04943  privacy: 2.05879\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 01:35:35 2021 Episode: 8   Index: 198174   Loss: 0.11856 --\n",
      "Reward: [0.4604  0.00517 0.03449] total reward: 0.50006\n",
      "UEHitrate: 0.00547  edgeHitrate 0.04311 sumHitrate 0.04858  privacy: 2.03665\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 01:35:35 2021 Episode: 8   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 01:41:24 2021 Episode: 8   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.48382 0.00434 0.04219] total reward: 0.53035\n",
      "UEHitrate: 0.00468  edgeHitrate 0.05274 sumHitrate 0.05742  privacy: 1.85966\n",
      "\n",
      "--Time: Sun Oct 10 01:46:11 2021 Episode: 8   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.54006 0.00427 0.03537] total reward: 0.5797\n",
      "UEHitrate: 0.00452  edgeHitrate 0.04421 sumHitrate 0.04873  privacy: 1.64945\n",
      "\n",
      "--Time: Sun Oct 10 01:51:55 2021 Episode: 8   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.56587 0.00481 0.03354] total reward: 0.60422\n",
      "UEHitrate: 0.00511  edgeHitrate 0.04192 sumHitrate 0.04703  privacy: 1.59324\n",
      "\n",
      "--Time: Sun Oct 10 01:56:44 2021 Episode: 8   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.58665 0.00515 0.0325 ] total reward: 0.6243\n",
      "UEHitrate: 0.00543  edgeHitrate 0.04062 sumHitrate 0.04605  privacy: 1.55535\n",
      "\n",
      "--Time: Sun Oct 10 02:02:39 2021 Episode: 8   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.60226 0.00537 0.03214] total reward: 0.63977\n",
      "UEHitrate: 0.00567  edgeHitrate 0.04018 sumHitrate 0.04585  privacy: 1.52931\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 02:03:07 2021 Episode: 8   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.60348 0.00536 0.03206] total reward: 0.64089\n",
      "UEHitrate: 0.00566  edgeHitrate 0.04007 sumHitrate 0.04573  privacy: 1.52657\n",
      "------------------------------validation----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 02:03:07 2021 Episode: 9   Index: 0   Loss: 2.3162 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 02:17:14 2021 Episode: 9   Index: 50000   Loss: 0.09767 --\n",
      "Reward: [0.32229 0.00466 0.04856] total reward: 0.37551\n",
      "UEHitrate: 0.00514  edgeHitrate 0.0607 sumHitrate 0.06584  privacy: 2.70433\n",
      "\n",
      "--Time: Sun Oct 10 02:30:33 2021 Episode: 9   Index: 100000   Loss: 0.09436 --\n",
      "Reward: [0.39003 0.00444 0.03931] total reward: 0.43379\n",
      "UEHitrate: 0.00477  edgeHitrate 0.04914 sumHitrate 0.05391  privacy: 2.40016\n",
      "\n",
      "--Time: Sun Oct 10 02:44:14 2021 Episode: 9   Index: 150000   Loss: 0.09302 --\n",
      "Reward: [0.43604 0.00495 0.03612] total reward: 0.4771\n",
      "UEHitrate: 0.00529  edgeHitrate 0.04515 sumHitrate 0.05044  privacy: 2.26257\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 10 02:57:35 2021 Episode: 9   Index: 198174   Loss: 0.0917 --\n",
      "Reward: [0.4626  0.00525 0.03459] total reward: 0.50244\n",
      "UEHitrate: 0.00559  edgeHitrate 0.04324 sumHitrate 0.04883  privacy: 2.1732\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "--Time: Sun Oct 10 02:57:35 2021 Episode: 9   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 03:02:51 2021 Episode: 9   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.33587 0.00451 0.10488] total reward: 0.44526\n",
      "UEHitrate: 0.00654  edgeHitrate 0.1311 sumHitrate 0.13764  privacy: 2.72606\n",
      "\n",
      "--Time: Sun Oct 10 03:08:06 2021 Episode: 9   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.41006 0.00459 0.10908] total reward: 0.52373\n",
      "UEHitrate: 0.00669  edgeHitrate 0.13635 sumHitrate 0.14304  privacy: 2.41087\n",
      "\n",
      "--Time: Sun Oct 10 03:14:22 2021 Episode: 9   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.46123 0.00513 0.09503] total reward: 0.5614\n",
      "UEHitrate: 0.00695  edgeHitrate 0.11879 sumHitrate 0.12575  privacy: 2.26606\n",
      "\n",
      "--Time: Sun Oct 10 03:20:41 2021 Episode: 9   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.49237 0.00541 0.08457] total reward: 0.58235\n",
      "UEHitrate: 0.00697  edgeHitrate 0.10571 sumHitrate 0.11268  privacy: 2.17077\n",
      "\n",
      "--Time: Sun Oct 10 03:26:33 2021 Episode: 9   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.51618 0.00558 0.0784 ] total reward: 0.60016\n",
      "UEHitrate: 0.00701  edgeHitrate 0.098 sumHitrate 0.10501  privacy: 2.09414\n",
      "\n",
      "------------------------------validation----------------------------------\n",
      "--Time: Sun Oct 10 03:26:59 2021 Episode: 9   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.51794 0.00556 0.07793] total reward: 0.60144\n",
      "UEHitrate: 0.00697  edgeHitrate 0.09741 sumHitrate 0.10439  privacy: 2.08969\n",
      "------------------------------validation----------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h1_ep0_1009-15-30-49'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#test\n",
    "\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sun Oct 10 03:27:06 2021   Index: 0   Loss: 0.0 --\n",
      "Reward: [1.31032 0.      0.     ] total reward: 1.31032\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sun Oct 10 03:28:24 2021   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.77955 0.0078  0.21254] total reward: 0.99989\n",
      "UEHitrate: 0.0187  edgeHitrate 0.26717 sumHitrate 0.28587  privacy: 1.26312\n",
      "\n",
      "--Time: Sun Oct 10 03:29:32 2021   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.85708 0.01313 0.24675] total reward: 1.11696\n",
      "UEHitrate: 0.0344  edgeHitrate 0.31013 sumHitrate 0.34453  privacy: 1.04927\n",
      "\n",
      "--Time: Sun Oct 10 03:30:39 2021   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.91185 0.01565 0.23623] total reward: 1.16372\n",
      "UEHitrate: 0.04043  edgeHitrate 0.29639 sumHitrate 0.33682  privacy: 0.93989\n",
      "\n",
      "--Time: Sun Oct 10 03:31:46 2021   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.95791 0.01687 0.23399] total reward: 1.20878\n",
      "UEHitrate: 0.04167  edgeHitrate 0.29434 sumHitrate 0.33602  privacy: 0.87273\n",
      "\n",
      "--Time: Sun Oct 10 03:32:50 2021   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.9994  0.01784 0.23558] total reward: 1.25282\n",
      "UEHitrate: 0.04398  edgeHitrate 0.29625 sumHitrate 0.34023  privacy: 0.81166\n",
      "\n",
      "--Time: Sun Oct 10 03:33:59 2021   Index: 60000   Loss: 0.0 --\n",
      "Reward: [1.03971 0.01799 0.23364] total reward: 1.29134\n",
      "UEHitrate: 0.04352  edgeHitrate 0.29378 sumHitrate 0.33729  privacy: 0.77056\n",
      "\n",
      "--Time: Sun Oct 10 03:35:09 2021   Index: 70000   Loss: 0.0 --\n",
      "Reward: [1.07554 0.01793 0.22829] total reward: 1.32176\n",
      "UEHitrate: 0.04297  edgeHitrate 0.28717 sumHitrate 0.33014  privacy: 0.74463\n",
      "\n",
      "--Time: Sun Oct 10 03:36:25 2021   Index: 80000   Loss: 0.0 --\n",
      "Reward: [1.10897 0.01779 0.22808] total reward: 1.35483\n",
      "UEHitrate: 0.04279  edgeHitrate 0.28687 sumHitrate 0.32966  privacy: 0.71092\n",
      "\n",
      "--Time: Sun Oct 10 03:37:28 2021   Index: 90000   Loss: 0.0 --\n",
      "Reward: [1.14509 0.01768 0.22275] total reward: 1.38553\n",
      "UEHitrate: 0.04184  edgeHitrate 0.28022 sumHitrate 0.32206  privacy: 0.67455\n",
      "\n",
      "--Time: Sun Oct 10 03:38:36 2021   Index: 100000   Loss: 0.0 --\n",
      "Reward: [1.1875  0.01716 0.21805] total reward: 1.42271\n",
      "UEHitrate: 0.04032  edgeHitrate 0.27435 sumHitrate 0.31467  privacy: 0.63984\n",
      "\n",
      "--Time: Sun Oct 10 03:39:34 2021   Index: 110000   Loss: 0.0 --\n",
      "Reward: [1.23468 0.01711 0.21599] total reward: 1.46779\n",
      "UEHitrate: 0.0396  edgeHitrate 0.27192 sumHitrate 0.31152  privacy: 0.59657\n",
      "\n",
      "--Time: Sun Oct 10 03:40:25 2021   Index: 120000   Loss: 0.0 --\n",
      "Reward: [1.28329 0.01709 0.21054] total reward: 1.51093\n",
      "UEHitrate: 0.03896  edgeHitrate 0.26529 sumHitrate 0.30425  privacy: 0.57085\n",
      "\n",
      "--Time: Sun Oct 10 03:41:21 2021   Index: 130000   Loss: 0.0 --\n",
      "Reward: [1.3303  0.01736 0.20626] total reward: 1.55392\n",
      "UEHitrate: 0.03884  edgeHitrate 0.26002 sumHitrate 0.29886  privacy: 0.54873\n",
      "\n",
      "--Time: Sun Oct 10 03:42:15 2021   Index: 140000   Loss: 0.0 --\n",
      "Reward: [1.37653 0.01704 0.20484] total reward: 1.59841\n",
      "UEHitrate: 0.03796  edgeHitrate 0.25833 sumHitrate 0.29629  privacy: 0.52466\n",
      "\n",
      "--Time: Sun Oct 10 03:43:11 2021   Index: 150000   Loss: 0.0 --\n",
      "Reward: [1.42177 0.01702 0.20241] total reward: 1.64119\n",
      "UEHitrate: 0.03753  edgeHitrate 0.25526 sumHitrate 0.29279  privacy: 0.52046\n",
      "\n",
      "--Time: Sun Oct 10 03:44:15 2021   Index: 160000   Loss: 0.0 --\n",
      "Reward: [1.46238 0.01695 0.19948] total reward: 1.67882\n",
      "UEHitrate: 0.03694  edgeHitrate 0.25159 sumHitrate 0.28854  privacy: 0.50518\n",
      "\n",
      "--Time: Sun Oct 10 03:45:07 2021   Index: 170000   Loss: 0.0 --\n",
      "Reward: [1.49767 0.0167  0.19832] total reward: 1.71269\n",
      "UEHitrate: 0.03618  edgeHitrate 0.25012 sumHitrate 0.2863  privacy: 0.49587\n",
      "\n",
      "--Time: Sun Oct 10 03:46:01 2021   Index: 180000   Loss: 0.0 --\n",
      "Reward: [1.5344  0.01654 0.19858] total reward: 1.74952\n",
      "UEHitrate: 0.03561  edgeHitrate 0.25045 sumHitrate 0.28605  privacy: 0.47336\n",
      "\n",
      "--Time: Sun Oct 10 03:46:58 2021   Index: 190000   Loss: 0.0 --\n",
      "Reward: [1.57791 0.01641 0.1969 ] total reward: 1.79121\n",
      "UEHitrate: 0.03511  edgeHitrate 0.24835 sumHitrate 0.28346  privacy: 0.45578\n",
      "\n",
      "--Time: Sun Oct 10 03:47:49 2021   Index: 200000   Loss: 0.0 --\n",
      "Reward: [1.62022 0.01627 0.19555] total reward: 1.83204\n",
      "UEHitrate: 0.03456  edgeHitrate 0.24669 sumHitrate 0.28125  privacy: 0.43955\n",
      "\n",
      "--Time: Sun Oct 10 03:48:47 2021   Index: 210000   Loss: 0.0 --\n",
      "Reward: [1.66628 0.01616 0.19603] total reward: 1.87847\n",
      "UEHitrate: 0.03422  edgeHitrate 0.24727 sumHitrate 0.28149  privacy: 0.42253\n",
      "\n",
      "--Time: Sun Oct 10 03:49:38 2021   Index: 220000   Loss: 0.0 --\n",
      "Reward: [1.71234 0.01614 0.19582] total reward: 1.9243\n",
      "UEHitrate: 0.03405  edgeHitrate 0.24705 sumHitrate 0.2811  privacy: 0.40113\n",
      "\n",
      "--Time: Sun Oct 10 03:50:26 2021   Index: 230000   Loss: 0.0 --\n",
      "Reward: [1.76411 0.01607 0.19485] total reward: 1.97503\n",
      "UEHitrate: 0.03369  edgeHitrate 0.24579 sumHitrate 0.27949  privacy: 0.37593\n",
      "\n",
      "--Time: Sun Oct 10 03:51:17 2021   Index: 240000   Loss: 0.0 --\n",
      "Reward: [1.8193  0.01599 0.19378] total reward: 2.02907\n",
      "UEHitrate: 0.03342  edgeHitrate 0.24445 sumHitrate 0.27787  privacy: 0.36709\n",
      "\n",
      "--Time: Sun Oct 10 03:52:14 2021   Index: 250000   Loss: 0.0 --\n",
      "Reward: [1.87493 0.01594 0.19284] total reward: 2.08371\n",
      "UEHitrate: 0.03337  edgeHitrate 0.24329 sumHitrate 0.27666  privacy: 0.36289\n",
      "\n",
      "--Time: Sun Oct 10 03:53:08 2021   Index: 260000   Loss: 0.0 --\n",
      "Reward: [1.9256  0.01585 0.19141] total reward: 2.13285\n",
      "UEHitrate: 0.03312  edgeHitrate 0.24146 sumHitrate 0.27459  privacy: 0.35875\n",
      "\n",
      "--Time: Sun Oct 10 03:54:02 2021   Index: 270000   Loss: 0.0 --\n",
      "Reward: [1.97848 0.01581 0.19011] total reward: 2.18439\n",
      "UEHitrate: 0.03296  edgeHitrate 0.23981 sumHitrate 0.27277  privacy: 0.35568\n",
      "\n",
      "--Time: Sun Oct 10 03:54:41 2021   Index: 280000   Loss: 0.0 --\n",
      "Reward: [2.02445 0.01576 0.18941] total reward: 2.22961\n",
      "UEHitrate: 0.03275  edgeHitrate 0.23892 sumHitrate 0.27167  privacy: 0.35351\n",
      "\n",
      "--Time: Sun Oct 10 03:55:20 2021   Index: 290000   Loss: 0.0 --\n",
      "Reward: [2.06816 0.01566 0.1898 ] total reward: 2.27361\n",
      "UEHitrate: 0.03239  edgeHitrate 0.23939 sumHitrate 0.27177  privacy: 0.35323\n",
      "\n",
      "--Time: Sun Oct 10 03:55:59 2021   Index: 300000   Loss: 0.0 --\n",
      "Reward: [2.10489 0.01545 0.19046] total reward: 2.3108\n",
      "UEHitrate: 0.03193  edgeHitrate 0.24017 sumHitrate 0.2721  privacy: 0.35362\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 10 03:56:03 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [2.10783 0.01542 0.19048] total reward: 2.31373\n",
      "UEHitrate: 0.03188  edgeHitrate 0.2402 sumHitrate 0.27208  privacy: 0.35385\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h1_ep0_1009-15-30-49'"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.28587, 0.34453, 0.33682, 0.33602, 0.34023, 0.33729,\n",
       "        0.33014, 0.32966, 0.32206, 0.31467, 0.31152, 0.30425, 0.29886,\n",
       "        0.29629, 0.29279, 0.28854, 0.2863 , 0.28605, 0.28346, 0.28125,\n",
       "        0.28149, 0.2811 , 0.27949, 0.27787, 0.27666, 0.27459, 0.27277,\n",
       "        0.27167, 0.27177, 0.27208]),\n",
       " array([0.     , 0.0187 , 0.0344 , 0.04043, 0.04167, 0.04398, 0.04352,\n",
       "        0.04297, 0.04279, 0.04184, 0.04032, 0.0396 , 0.03896, 0.03884,\n",
       "        0.03796, 0.03753, 0.03694, 0.03618, 0.03561, 0.03511, 0.03456,\n",
       "        0.03422, 0.03405, 0.03369, 0.03342, 0.03337, 0.03312, 0.03296,\n",
       "        0.03275, 0.03239, 0.03188]),\n",
       " array([0.     , 0.26717, 0.31013, 0.29639, 0.29434, 0.29625, 0.29378,\n",
       "        0.28717, 0.28687, 0.28022, 0.27435, 0.27192, 0.26529, 0.26002,\n",
       "        0.25833, 0.25526, 0.25159, 0.25012, 0.25045, 0.24835, 0.24669,\n",
       "        0.24727, 0.24705, 0.24579, 0.24445, 0.24329, 0.24146, 0.23981,\n",
       "        0.23892, 0.23939, 0.2402 ]))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4.02118, 1.26312, 1.04927, 0.93989, 0.87273, 0.81166, 0.77056,\n",
       "       0.74463, 0.71092, 0.67455, 0.63984, 0.59657, 0.57085, 0.54873,\n",
       "       0.52466, 0.52046, 0.50518, 0.49587, 0.47336, 0.45578, 0.43955,\n",
       "       0.42253, 0.40113, 0.37593, 0.36709, 0.36289, 0.35875, 0.35568,\n",
       "       0.35351, 0.35323, 0.35385])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "bestPath"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h1_ep0_1009-15-30-49'"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}