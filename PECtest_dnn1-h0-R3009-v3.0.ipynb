{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1 修改时序上bug\n",
    "#v2 增加边缘特征\n",
    "#v3 考虑长期收益\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_h0_v3.0_'\n",
    "\n",
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=np.int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":0,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    #expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = self.statusEmbedding(r,p,e)\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        statusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        if train and len(self.W)>1: #d\n",
    "            lastAction = np.row_stack((1-self.lastAction,self.lastAction)).T\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature.to(device), \n",
    "                        torch.from_numpy(lastAction).to(device), \n",
    "                        statusFeature.to(device),\n",
    "                        torch.tensor([self.reward]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "        self.lastStatusFeature = statusFeature\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            #eps_threshold = 0\n",
    "            if  not train or (train and sample >= eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                #with torch.no_grad():\n",
    "                Q_value = QNetwork(statusFeature).detach()\n",
    "                QNetwork.train()\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                \n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "            \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 13:39:12 2021 Episode: 0   Index: 198174   Loss: 0.00466 --\n",
      "Reward: [0.      0.00712 0.06279] total reward: 0.06991\n",
      "UEHitrate: 0.0098  edgeHitrate 0.07849 sumHitrate 0.08829  privacy: 1.08944\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 13:52:53 2021 Episode: 0   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.02434 0.17264] total reward: 0.19698\n",
      "UEHitrate: 0.05672  edgeHitrate 0.2158 sumHitrate 0.27252  privacy: 1.08626\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h0_v3.0_ep0_1016-13-52-53\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 15:18:23 2021 Episode: 1   Index: 198174   Loss: 0.00812 --\n",
      "Reward: [0.      0.01058 0.11437] total reward: 0.12496\n",
      "UEHitrate: 0.01962  edgeHitrate 0.14296 sumHitrate 0.16258  privacy: 1.02349\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 15:31:58 2021 Episode: 1   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.01015 0.30356] total reward: 0.31372\n",
      "UEHitrate: 0.03268  edgeHitrate 0.37945 sumHitrate 0.41213  privacy: 1.20255\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_h0_v3.0_ep1_1016-15-31-58\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 16:59:02 2021 Episode: 2   Index: 198174   Loss: 0.008 --\n",
      "Reward: [0.      0.00717 0.09167] total reward: 0.09883\n",
      "UEHitrate: 0.01292  edgeHitrate 0.11459 sumHitrate 0.1275  privacy: 1.13821\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 17:12:31 2021 Episode: 2   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.01757 0.19477] total reward: 0.21233\n",
      "UEHitrate: 0.06446  edgeHitrate 0.24346 sumHitrate 0.30792  privacy: 0.42496\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 18:38:24 2021 Episode: 3   Index: 198174   Loss: 0.00933 --\n",
      "Reward: [0.      0.00864 0.10529] total reward: 0.11393\n",
      "UEHitrate: 0.01937  edgeHitrate 0.13161 sumHitrate 0.15098  privacy: 1.40017\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 18:52:09 2021 Episode: 3   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.0128  0.14394] total reward: 0.15673\n",
      "UEHitrate: 0.03453  edgeHitrate 0.17992 sumHitrate 0.21446  privacy: 0.94759\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 20:15:52 2021 Episode: 4   Index: 198174   Loss: 0.01152 --\n",
      "Reward: [0.      0.01493 0.13418] total reward: 0.14911\n",
      "UEHitrate: 0.03823  edgeHitrate 0.16773 sumHitrate 0.20596  privacy: 1.73249\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 20:28:27 2021 Episode: 4   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.0197  0.27061] total reward: 0.29031\n",
      "UEHitrate: 0.07132  edgeHitrate 0.33826 sumHitrate 0.40957  privacy: 0.8451\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 21:50:06 2021 Episode: 5   Index: 198174   Loss: 0.00965 --\n",
      "Reward: [0.      0.00881 0.10071] total reward: 0.10953\n",
      "UEHitrate: 0.01763  edgeHitrate 0.12589 sumHitrate 0.14352  privacy: 1.74017\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 22:06:38 2021 Episode: 5   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.01263 0.19108] total reward: 0.20371\n",
      "UEHitrate: 0.03726  edgeHitrate 0.23886 sumHitrate 0.27611  privacy: 1.7243\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 23:41:41 2021 Episode: 6   Index: 198174   Loss: 0.01536 --\n",
      "Reward: [0.      0.01742 0.14922] total reward: 0.16664\n",
      "UEHitrate: 0.04678  edgeHitrate 0.18652 sumHitrate 0.2333  privacy: 1.86516\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sun Oct 17 00:03:36 2021 Episode: 6   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.00753 0.08289] total reward: 0.09042\n",
      "UEHitrate: 0.01505  edgeHitrate 0.10361 sumHitrate 0.11866  privacy: 2.11011\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 17 01:43:05 2021 Episode: 7   Index: 198174   Loss: 0.02663 --\n",
      "Reward: [0.      0.0217  0.21677] total reward: 0.23846\n",
      "UEHitrate: 0.06231  edgeHitrate 0.27096 sumHitrate 0.33327  privacy: 1.99978\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sun Oct 17 01:58:09 2021 Episode: 7   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.03366 0.2145 ] total reward: 0.24816\n",
      "UEHitrate: 0.06404  edgeHitrate 0.26813 sumHitrate 0.33216  privacy: 2.07289\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 17 03:35:34 2021 Episode: 8   Index: 198174   Loss: 0.01742 --\n",
      "Reward: [0.      0.01904 0.17946] total reward: 0.1985\n",
      "UEHitrate: 0.05821  edgeHitrate 0.22432 sumHitrate 0.28253  privacy: 1.77646\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sun Oct 17 03:48:56 2021 Episode: 8   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.02441 0.12235] total reward: 0.14677\n",
      "UEHitrate: 0.04163  edgeHitrate 0.15294 sumHitrate 0.19457  privacy: 1.34372\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 17 05:24:20 2021 Episode: 9   Index: 198174   Loss: 0.02334 --\n",
      "Reward: [0.      0.02311 0.20441] total reward: 0.22751\n",
      "UEHitrate: 0.05919  edgeHitrate 0.25551 sumHitrate 0.3147  privacy: 1.7202\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sun Oct 17 05:36:33 2021 Episode: 9   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.      0.00789 0.13127] total reward: 0.13915\n",
      "UEHitrate: 0.01812  edgeHitrate 0.16408 sumHitrate 0.1822  privacy: 1.91571\n",
      "------------------------------validation---------------------------#-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(4000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if (index+1) % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        '''\n",
    "        if (index+1) % 1000 == 0 :\n",
    "            psi = 0\n",
    "            for u in UEs:\n",
    "                psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "        '''\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sun Oct 17 05:37:07 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [0.      0.0063  0.11016] total reward: 0.11646\n",
      "UEHitrate: 0.0111  edgeHitrate 0.1377 sumHitrate 0.1488  privacy: 1.76804\n",
      "\n",
      "--Time: Sun Oct 17 05:37:37 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [0.      0.00686 0.17116] total reward: 0.17802\n",
      "UEHitrate: 0.0137  edgeHitrate 0.21395 sumHitrate 0.22765  privacy: 1.5731\n",
      "\n",
      "--Time: Sun Oct 17 05:38:06 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [0.      0.00746 0.18491] total reward: 0.19237\n",
      "UEHitrate: 0.01757  edgeHitrate 0.23113 sumHitrate 0.2487  privacy: 1.50829\n",
      "\n",
      "--Time: Sun Oct 17 05:38:35 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [0.      0.00804 0.19726] total reward: 0.2053\n",
      "UEHitrate: 0.0214  edgeHitrate 0.24657 sumHitrate 0.26798  privacy: 1.46777\n",
      "\n",
      "--Time: Sun Oct 17 05:39:04 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [0.      0.00853 0.20832] total reward: 0.21685\n",
      "UEHitrate: 0.02362  edgeHitrate 0.2604 sumHitrate 0.28402  privacy: 1.43907\n",
      "\n",
      "--Time: Sun Oct 17 05:39:32 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [0.      0.00887 0.21743] total reward: 0.22629\n",
      "UEHitrate: 0.02533  edgeHitrate 0.27178 sumHitrate 0.29712  privacy: 1.41072\n",
      "\n",
      "--Time: Sun Oct 17 05:40:01 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [0.      0.0087  0.22409] total reward: 0.23279\n",
      "UEHitrate: 0.0256  edgeHitrate 0.28011 sumHitrate 0.30571  privacy: 1.38747\n",
      "\n",
      "--Time: Sun Oct 17 05:40:30 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [0.      0.00888 0.23398] total reward: 0.24286\n",
      "UEHitrate: 0.02701  edgeHitrate 0.29247 sumHitrate 0.31949  privacy: 1.36871\n",
      "\n",
      "--Time: Sun Oct 17 05:40:59 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [0.      0.00901 0.23928] total reward: 0.24829\n",
      "UEHitrate: 0.02768  edgeHitrate 0.2991 sumHitrate 0.32678  privacy: 1.34893\n",
      "\n",
      "--Time: Sun Oct 17 05:41:28 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [0.      0.00916 0.2444 ] total reward: 0.25356\n",
      "UEHitrate: 0.02858  edgeHitrate 0.3055 sumHitrate 0.33408  privacy: 1.32802\n",
      "\n",
      "--Time: Sun Oct 17 05:41:57 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [0.      0.00919 0.24956] total reward: 0.25875\n",
      "UEHitrate: 0.0287  edgeHitrate 0.31195 sumHitrate 0.34065  privacy: 1.31626\n",
      "\n",
      "--Time: Sun Oct 17 05:42:26 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [0.      0.00925 0.25442] total reward: 0.26367\n",
      "UEHitrate: 0.02908  edgeHitrate 0.31803 sumHitrate 0.3471  privacy: 1.30422\n",
      "\n",
      "--Time: Sun Oct 17 05:42:55 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [0.      0.00942 0.25815] total reward: 0.26757\n",
      "UEHitrate: 0.02964  edgeHitrate 0.32269 sumHitrate 0.35233  privacy: 1.2918\n",
      "\n",
      "--Time: Sun Oct 17 05:43:24 2021   Index: 139999   Loss: 0.0 --\n",
      "Reward: [0.      0.00947 0.2646 ] total reward: 0.27407\n",
      "UEHitrate: 0.02992  edgeHitrate 0.33075 sumHitrate 0.36067  privacy: 1.28065\n",
      "\n",
      "--Time: Sun Oct 17 05:43:53 2021   Index: 149999   Loss: 0.0 --\n",
      "Reward: [0.      0.00959 0.27093] total reward: 0.28052\n",
      "UEHitrate: 0.03049  edgeHitrate 0.33867 sumHitrate 0.36915  privacy: 1.27051\n",
      "\n",
      "--Time: Sun Oct 17 05:44:22 2021   Index: 159999   Loss: 0.0 --\n",
      "Reward: [0.      0.00975 0.27348] total reward: 0.28323\n",
      "UEHitrate: 0.03092  edgeHitrate 0.34185 sumHitrate 0.37278  privacy: 1.25932\n",
      "\n",
      "--Time: Sun Oct 17 05:44:51 2021   Index: 169999   Loss: 0.0 --\n",
      "Reward: [0.      0.00991 0.27855] total reward: 0.28846\n",
      "UEHitrate: 0.03172  edgeHitrate 0.34819 sumHitrate 0.37991  privacy: 1.24938\n",
      "\n",
      "--Time: Sun Oct 17 05:45:20 2021   Index: 179999   Loss: 0.0 --\n",
      "Reward: [0.      0.01008 0.28243] total reward: 0.29251\n",
      "UEHitrate: 0.03251  edgeHitrate 0.35303 sumHitrate 0.38554  privacy: 1.24513\n",
      "\n",
      "--Time: Sun Oct 17 05:45:49 2021   Index: 189999   Loss: 0.0 --\n",
      "Reward: [0.      0.01009 0.28709] total reward: 0.29718\n",
      "UEHitrate: 0.03259  edgeHitrate 0.35887 sumHitrate 0.39146  privacy: 1.23932\n",
      "\n",
      "--Time: Sun Oct 17 05:46:17 2021   Index: 199999   Loss: 0.0 --\n",
      "Reward: [0.      0.01001 0.29082] total reward: 0.30083\n",
      "UEHitrate: 0.03222  edgeHitrate 0.36353 sumHitrate 0.39576  privacy: 1.23581\n",
      "\n",
      "--Time: Sun Oct 17 05:46:46 2021   Index: 209999   Loss: 0.0 --\n",
      "Reward: [0.      0.01009 0.29467] total reward: 0.30476\n",
      "UEHitrate: 0.03253  edgeHitrate 0.36834 sumHitrate 0.40087  privacy: 1.22904\n",
      "\n",
      "--Time: Sun Oct 17 05:47:15 2021   Index: 219999   Loss: 0.0 --\n",
      "Reward: [0.      0.0101  0.29694] total reward: 0.30704\n",
      "UEHitrate: 0.03239  edgeHitrate 0.37117 sumHitrate 0.40356  privacy: 1.22155\n",
      "\n",
      "--Time: Sun Oct 17 05:47:43 2021   Index: 229999   Loss: 0.0 --\n",
      "Reward: [0.      0.01016 0.29848] total reward: 0.30863\n",
      "UEHitrate: 0.03253  edgeHitrate 0.3731 sumHitrate 0.40562  privacy: 1.21526\n",
      "\n",
      "--Time: Sun Oct 17 05:48:12 2021   Index: 239999   Loss: 0.0 --\n",
      "Reward: [0.      0.01013 0.30055] total reward: 0.31069\n",
      "UEHitrate: 0.0324  edgeHitrate 0.37569 sumHitrate 0.4081  privacy: 1.20874\n",
      "\n",
      "--Time: Sun Oct 17 05:48:40 2021   Index: 249999   Loss: 0.0 --\n",
      "Reward: [0.      0.01018 0.30315] total reward: 0.31333\n",
      "UEHitrate: 0.03272  edgeHitrate 0.37893 sumHitrate 0.41166  privacy: 1.20457\n",
      "\n",
      "--Time: Sun Oct 17 05:49:08 2021   Index: 259999   Loss: 0.0 --\n",
      "Reward: [0.      0.01012 0.30459] total reward: 0.31471\n",
      "UEHitrate: 0.03254  edgeHitrate 0.38074 sumHitrate 0.41328  privacy: 1.19995\n",
      "\n",
      "--Time: Sun Oct 17 05:49:37 2021   Index: 269999   Loss: 0.0 --\n",
      "Reward: [0.      0.01009 0.30574] total reward: 0.31584\n",
      "UEHitrate: 0.03235  edgeHitrate 0.38218 sumHitrate 0.41453  privacy: 1.19582\n",
      "\n",
      "--Time: Sun Oct 17 05:50:05 2021   Index: 279999   Loss: 0.0 --\n",
      "Reward: [0.      0.01012 0.30744] total reward: 0.31757\n",
      "UEHitrate: 0.03247  edgeHitrate 0.3843 sumHitrate 0.41678  privacy: 1.19184\n",
      "\n",
      "--Time: Sun Oct 17 05:50:33 2021   Index: 289999   Loss: 0.0 --\n",
      "Reward: [0.      0.01018 0.30993] total reward: 0.32012\n",
      "UEHitrate: 0.03284  edgeHitrate 0.38741 sumHitrate 0.42026  privacy: 1.18892\n",
      "\n",
      "--Time: Sun Oct 17 05:51:01 2021   Index: 299999   Loss: 0.0 --\n",
      "Reward: [0.      0.01022 0.31395] total reward: 0.32417\n",
      "UEHitrate: 0.03319  edgeHitrate 0.39244 sumHitrate 0.42562  privacy: 1.1866\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 17 05:51:04 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [0.      0.01021 0.31434] total reward: 0.32455\n",
      "UEHitrate: 0.03318  edgeHitrate 0.39293 sumHitrate 0.42611  privacy: 1.18627\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.1488 , 0.22765, 0.2487 , 0.26798, 0.28402, 0.29712, 0.30571,\n",
       "        0.31949, 0.32678, 0.33408, 0.34065, 0.3471 , 0.35233, 0.36067,\n",
       "        0.36915, 0.37278, 0.37991, 0.38554, 0.39146, 0.39576, 0.40087,\n",
       "        0.40356, 0.40562, 0.4081 , 0.41166, 0.41328, 0.41453, 0.41678,\n",
       "        0.42026, 0.42562, 0.42611]),\n",
       " array([0.0111 , 0.0137 , 0.01757, 0.0214 , 0.02362, 0.02533, 0.0256 ,\n",
       "        0.02701, 0.02768, 0.02858, 0.0287 , 0.02908, 0.02964, 0.02992,\n",
       "        0.03049, 0.03092, 0.03172, 0.03251, 0.03259, 0.03222, 0.03253,\n",
       "        0.03239, 0.03253, 0.0324 , 0.03272, 0.03254, 0.03235, 0.03247,\n",
       "        0.03284, 0.03319, 0.03318]),\n",
       " array([0.1377 , 0.21395, 0.23113, 0.24657, 0.2604 , 0.27178, 0.28011,\n",
       "        0.29247, 0.2991 , 0.3055 , 0.31195, 0.31803, 0.32269, 0.33075,\n",
       "        0.33867, 0.34185, 0.34819, 0.35303, 0.35887, 0.36353, 0.36834,\n",
       "        0.37117, 0.3731 , 0.37569, 0.37893, 0.38074, 0.38218, 0.3843 ,\n",
       "        0.38741, 0.39244, 0.39293]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.76804, 1.5731 , 1.50829, 1.46777, 1.43907, 1.41072, 1.38747,\n",
       "       1.36871, 1.34893, 1.32802, 1.31626, 1.30422, 1.2918 , 1.28065,\n",
       "       1.27051, 1.25932, 1.24938, 1.24513, 1.23932, 1.23581, 1.22904,\n",
       "       1.22155, 1.21526, 1.20874, 1.20457, 1.19995, 1.19582, 1.19184,\n",
       "       1.18892, 1.1866 , 1.18627])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a71a010a29be3df77e49d8f42909021441fd02d28ccbb313c9d0d3f90d3253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
