{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((198175, 12), (253630, 12), 10000, 500)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#v2 增加神经网络输出\n",
    "#v3 修改reward为直接变化量\n",
    "#v4 删除e,S特征 \n",
    "#v7 增加邻居节点特征\n",
    "#V8 修改时序上bug\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_o0l0_v8.0_'\n",
    "\n",
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=np.int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0,\"betal\":0}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainUIT.shape,validUIT.shape,contentNum,userNum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    '''\n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    '''\n",
    "    #next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    #expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        #self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        statusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        if train and len(self.W)>1: #d\n",
    "            lastAction = np.row_stack((1-self.lastAction,self.lastAction)).T\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature.to(device), \n",
    "                        torch.from_numpy(lastAction).to(device), \n",
    "                        statusFeature.to(device),\n",
    "                        torch.tensor([self.reward]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "        self.lastStatusFeature = statusFeature\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            #eps_threshold = 0\n",
    "            if  not train or (train and sample >= eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                #with torch.no_grad():\n",
    "                Q_value = QNetwork(statusFeature).detach()\n",
    "                QNetwork.train()\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                \n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "            \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 01:33:34 2021 Episode: 0   Index: 198174   Loss: 0.58268 --\n",
      "Reward: [0.59937 0.      0.     ] total reward: 0.59937\n",
      "UEHitrate: 0.00838  edgeHitrate 0.077 sumHitrate 0.08537  privacy: 0.95292\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 01:53:35 2021 Episode: 0   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.64734 0.      0.     ] total reward: 0.64734\n",
      "UEHitrate: 0.03451  edgeHitrate 0.20661 sumHitrate 0.24112  privacy: 1.40373\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep0_1016-01-53-35\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 02:15:54 2021 Episode: 1   Index: 198174   Loss: 0.50356 --\n",
      "Reward: [0.61035 0.      0.     ] total reward: 0.61035\n",
      "UEHitrate: 0.01124  edgeHitrate 0.08757 sumHitrate 0.09882  privacy: 1.04857\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 02:32:30 2021 Episode: 1   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.648 0.    0.   ] total reward: 0.648\n",
      "UEHitrate: 0.07341  edgeHitrate 0.30056 sumHitrate 0.37396  privacy: 1.42571\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep1_1016-02-32-30\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 02:56:41 2021 Episode: 2   Index: 198174   Loss: 0.59625 --\n",
      "Reward: [0.49312 0.      0.     ] total reward: 0.49312\n",
      "UEHitrate: 0.00858  edgeHitrate 0.09354 sumHitrate 0.10212  privacy: 1.32115\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 03:16:18 2021 Episode: 2   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.81256 0.      0.     ] total reward: 0.81256\n",
      "UEHitrate: 0.04626  edgeHitrate 0.29379 sumHitrate 0.34005  privacy: 1.1466\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep2_1016-03-16-18\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 03:36:16 2021 Episode: 3   Index: 198174   Loss: 0.33339 --\n",
      "Reward: [0.47457 0.      0.     ] total reward: 0.47457\n",
      "UEHitrate: 0.03643  edgeHitrate 0.14787 sumHitrate 0.1843  privacy: 1.60575\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 03:59:00 2021 Episode: 3   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.49403 0.      0.     ] total reward: 0.49403\n",
      "UEHitrate: 0.05623  edgeHitrate 0.34598 sumHitrate 0.4022  privacy: 2.04033\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 04:24:55 2021 Episode: 4   Index: 198174   Loss: 0.34102 --\n",
      "Reward: [0.43005 0.      0.     ] total reward: 0.43005\n",
      "UEHitrate: 0.04028  edgeHitrate 0.16506 sumHitrate 0.20534  privacy: 1.72416\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 04:45:34 2021 Episode: 4   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.46199 0.      0.     ] total reward: 0.46199\n",
      "UEHitrate: 0.03884  edgeHitrate 0.14725 sumHitrate 0.1861  privacy: 1.8835\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 05:10:53 2021 Episode: 5   Index: 198174   Loss: 0.3348 --\n",
      "Reward: [0.40122 0.      0.     ] total reward: 0.40122\n",
      "UEHitrate: 0.05521  edgeHitrate 0.20235 sumHitrate 0.25756  privacy: 2.06775\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 05:33:56 2021 Episode: 5   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.53288 0.      0.     ] total reward: 0.53288\n",
      "UEHitrate: 0.06532  edgeHitrate 0.24786 sumHitrate 0.31317  privacy: 1.77257\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 05:57:10 2021 Episode: 6   Index: 198174   Loss: 0.33828 --\n",
      "Reward: [0.40789 0.      0.     ] total reward: 0.40789\n",
      "UEHitrate: 0.0533  edgeHitrate 0.2566 sumHitrate 0.3099  privacy: 2.11834\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 06:19:43 2021 Episode: 6   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.47024 0.      0.     ] total reward: 0.47024\n",
      "UEHitrate: 0.06479  edgeHitrate 0.31934 sumHitrate 0.38413  privacy: 2.10461\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 06:46:05 2021 Episode: 7   Index: 198174   Loss: 0.33322 --\n",
      "Reward: [0.41375 0.      0.     ] total reward: 0.41375\n",
      "UEHitrate: 0.04437  edgeHitrate 0.15033 sumHitrate 0.1947  privacy: 2.18094\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 07:05:08 2021 Episode: 7   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.59333 0.      0.     ] total reward: 0.59333\n",
      "UEHitrate: 0.06762  edgeHitrate 0.23074 sumHitrate 0.29836  privacy: 1.49596\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 07:27:51 2021 Episode: 8   Index: 198174   Loss: 0.33233 --\n",
      "Reward: [0.41722 0.      0.     ] total reward: 0.41722\n",
      "UEHitrate: 0.06194  edgeHitrate 0.24564 sumHitrate 0.30758  privacy: 2.14979\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 07:46:50 2021 Episode: 8   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.74425 0.      0.     ] total reward: 0.74425\n",
      "UEHitrate: 0.07376  edgeHitrate 0.29461 sumHitrate 0.36836  privacy: 1.17966\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 08:13:47 2021 Episode: 9   Index: 198174   Loss: 0.32897 --\n",
      "Reward: [0.44907 0.      0.     ] total reward: 0.44907\n",
      "UEHitrate: 0.06418  edgeHitrate 0.2739 sumHitrate 0.33808  privacy: 2.02095\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 08:31:31 2021 Episode: 9   Index: 253629   Loss: 0.0 --\n",
      "Reward: [0.67054 0.      0.     ] total reward: 0.67054\n",
      "UEHitrate: 0.07283  edgeHitrate 0.32186 sumHitrate 0.39469  privacy: 1.29746\n",
      "------------------------------validation---------------------------#-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(4000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if (index+1) % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sat Oct 16 08:32:14 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [0.59808 0.      0.     ] total reward: 0.59808\n",
      "UEHitrate: 0.0262  edgeHitrate 0.302 sumHitrate 0.3282  privacy: 2.05546\n",
      "\n",
      "--Time: Sat Oct 16 08:32:52 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [0.62097 0.      0.     ] total reward: 0.62097\n",
      "UEHitrate: 0.0408  edgeHitrate 0.3332 sumHitrate 0.374  privacy: 1.80329\n",
      "\n",
      "--Time: Sat Oct 16 08:33:30 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [0.64592 0.      0.     ] total reward: 0.64592\n",
      "UEHitrate: 0.0518  edgeHitrate 0.3238 sumHitrate 0.3756  privacy: 1.65748\n",
      "\n",
      "--Time: Sat Oct 16 08:34:13 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [0.66393 0.      0.     ] total reward: 0.66393\n",
      "UEHitrate: 0.0551  edgeHitrate 0.32793 sumHitrate 0.38302  privacy: 1.55182\n",
      "\n",
      "--Time: Sat Oct 16 08:34:56 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [0.68382 0.      0.     ] total reward: 0.68382\n",
      "UEHitrate: 0.05892  edgeHitrate 0.33602 sumHitrate 0.39494  privacy: 1.47151\n",
      "\n",
      "--Time: Sat Oct 16 08:35:39 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [0.69929 0.      0.     ] total reward: 0.69929\n",
      "UEHitrate: 0.05963  edgeHitrate 0.33875 sumHitrate 0.39838  privacy: 1.40802\n",
      "\n",
      "--Time: Sat Oct 16 08:36:24 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [0.71317 0.      0.     ] total reward: 0.71317\n",
      "UEHitrate: 0.05953  edgeHitrate 0.33631 sumHitrate 0.39584  privacy: 1.36122\n",
      "\n",
      "--Time: Sat Oct 16 08:37:11 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [0.72435 0.      0.     ] total reward: 0.72435\n",
      "UEHitrate: 0.05982  edgeHitrate 0.33821 sumHitrate 0.39804  privacy: 1.32698\n",
      "\n",
      "--Time: Sat Oct 16 08:38:00 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [0.73427 0.      0.     ] total reward: 0.73427\n",
      "UEHitrate: 0.05876  edgeHitrate 0.3361 sumHitrate 0.39486  privacy: 1.29634\n",
      "\n",
      "--Time: Sat Oct 16 08:38:47 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [0.74256 0.      0.     ] total reward: 0.74256\n",
      "UEHitrate: 0.05807  edgeHitrate 0.3355 sumHitrate 0.39357  privacy: 1.2685\n",
      "\n",
      "--Time: Sat Oct 16 08:39:35 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [0.74991 0.      0.     ] total reward: 0.74991\n",
      "UEHitrate: 0.05788  edgeHitrate 0.33684 sumHitrate 0.39472  privacy: 1.24917\n",
      "\n",
      "--Time: Sat Oct 16 08:40:22 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [0.7568 0.     0.    ] total reward: 0.7568\n",
      "UEHitrate: 0.05645  edgeHitrate 0.33417 sumHitrate 0.39062  privacy: 1.23189\n",
      "\n",
      "--Time: Sat Oct 16 08:41:11 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [0.76354 0.      0.     ] total reward: 0.76354\n",
      "UEHitrate: 0.05578  edgeHitrate 0.3298 sumHitrate 0.38558  privacy: 1.21924\n",
      "\n",
      "--Time: Sat Oct 16 08:42:03 2021   Index: 139999   Loss: 0.0 --\n",
      "Reward: [0.76976 0.      0.     ] total reward: 0.76976\n",
      "UEHitrate: 0.05463  edgeHitrate 0.32739 sumHitrate 0.38202  privacy: 1.20866\n",
      "\n",
      "--Time: Sat Oct 16 08:42:52 2021   Index: 149999   Loss: 0.0 --\n",
      "Reward: [0.77513 0.      0.     ] total reward: 0.77513\n",
      "UEHitrate: 0.05352  edgeHitrate 0.32484 sumHitrate 0.37836  privacy: 1.19953\n",
      "\n",
      "--Time: Sat Oct 16 08:43:41 2021   Index: 159999   Loss: 0.0 --\n",
      "Reward: [0.78022 0.      0.     ] total reward: 0.78022\n",
      "UEHitrate: 0.05251  edgeHitrate 0.31969 sumHitrate 0.37219  privacy: 1.19177\n",
      "\n",
      "--Time: Sat Oct 16 08:44:29 2021   Index: 169999   Loss: 0.0 --\n",
      "Reward: [0.78467 0.      0.     ] total reward: 0.78467\n",
      "UEHitrate: 0.05147  edgeHitrate 0.31685 sumHitrate 0.36832  privacy: 1.18586\n",
      "\n",
      "--Time: Sat Oct 16 08:45:21 2021   Index: 179999   Loss: 0.0 --\n",
      "Reward: [0.78865 0.      0.     ] total reward: 0.78865\n",
      "UEHitrate: 0.05048  edgeHitrate 0.31484 sumHitrate 0.36532  privacy: 1.17961\n",
      "\n",
      "--Time: Sat Oct 16 08:46:14 2021   Index: 189999   Loss: 0.0 --\n",
      "Reward: [0.7924 0.     0.    ] total reward: 0.7924\n",
      "UEHitrate: 0.04976  edgeHitrate 0.31096 sumHitrate 0.36073  privacy: 1.17489\n",
      "\n",
      "--Time: Sat Oct 16 08:47:07 2021   Index: 199999   Loss: 0.0 --\n",
      "Reward: [0.79589 0.      0.     ] total reward: 0.79589\n",
      "UEHitrate: 0.04912  edgeHitrate 0.30775 sumHitrate 0.35686  privacy: 1.17082\n",
      "\n",
      "--Time: Sat Oct 16 08:47:57 2021   Index: 209999   Loss: 0.0 --\n",
      "Reward: [0.79894 0.      0.     ] total reward: 0.79894\n",
      "UEHitrate: 0.04864  edgeHitrate 0.30656 sumHitrate 0.3552  privacy: 1.16682\n",
      "\n",
      "--Time: Sat Oct 16 08:48:49 2021   Index: 219999   Loss: 0.0 --\n",
      "Reward: [0.80264 0.      0.     ] total reward: 0.80264\n",
      "UEHitrate: 0.0482  edgeHitrate 0.30389 sumHitrate 0.35209  privacy: 1.16208\n",
      "\n",
      "--Time: Sat Oct 16 08:49:38 2021   Index: 229999   Loss: 0.0 --\n",
      "Reward: [0.8059 0.     0.    ] total reward: 0.8059\n",
      "UEHitrate: 0.04752  edgeHitrate 0.30024 sumHitrate 0.34777  privacy: 1.15718\n",
      "\n",
      "--Time: Sat Oct 16 08:50:26 2021   Index: 239999   Loss: 0.0 --\n",
      "Reward: [0.80885 0.      0.     ] total reward: 0.80885\n",
      "UEHitrate: 0.04713  edgeHitrate 0.29758 sumHitrate 0.34471  privacy: 1.15184\n",
      "\n",
      "--Time: Sat Oct 16 08:51:16 2021   Index: 249999   Loss: 0.0 --\n",
      "Reward: [0.81153 0.      0.     ] total reward: 0.81153\n",
      "UEHitrate: 0.04649  edgeHitrate 0.29465 sumHitrate 0.34114  privacy: 1.14816\n",
      "\n",
      "--Time: Sat Oct 16 08:52:06 2021   Index: 259999   Loss: 0.0 --\n",
      "Reward: [0.8144 0.     0.    ] total reward: 0.8144\n",
      "UEHitrate: 0.04611  edgeHitrate 0.29236 sumHitrate 0.33847  privacy: 1.14388\n",
      "\n",
      "--Time: Sat Oct 16 08:52:55 2021   Index: 269999   Loss: 0.0 --\n",
      "Reward: [0.81679 0.      0.     ] total reward: 0.81679\n",
      "UEHitrate: 0.04567  edgeHitrate 0.28957 sumHitrate 0.33524  privacy: 1.14011\n",
      "\n",
      "--Time: Sat Oct 16 08:53:44 2021   Index: 279999   Loss: 0.0 --\n",
      "Reward: [0.81921 0.      0.     ] total reward: 0.81921\n",
      "UEHitrate: 0.04533  edgeHitrate 0.28766 sumHitrate 0.33299  privacy: 1.13704\n",
      "\n",
      "--Time: Sat Oct 16 08:54:34 2021   Index: 289999   Loss: 0.0 --\n",
      "Reward: [0.82164 0.      0.     ] total reward: 0.82164\n",
      "UEHitrate: 0.04492  edgeHitrate 0.28691 sumHitrate 0.33182  privacy: 1.13291\n",
      "\n",
      "--Time: Sat Oct 16 08:55:25 2021   Index: 299999   Loss: 0.0 --\n",
      "Reward: [0.82428 0.      0.     ] total reward: 0.82428\n",
      "UEHitrate: 0.04425  edgeHitrate 0.28589 sumHitrate 0.33014  privacy: 1.13055\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sat Oct 16 08:55:30 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [0.82453 0.      0.     ] total reward: 0.82453\n",
      "UEHitrate: 0.04416  edgeHitrate 0.28574 sumHitrate 0.3299  privacy: 1.13026\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.3282 , 0.374  , 0.3756 , 0.38302, 0.39494, 0.39838, 0.39584,\n",
       "        0.39804, 0.39486, 0.39357, 0.39472, 0.39062, 0.38558, 0.38202,\n",
       "        0.37836, 0.37219, 0.36832, 0.36532, 0.36073, 0.35686, 0.3552 ,\n",
       "        0.35209, 0.34777, 0.34471, 0.34114, 0.33847, 0.33524, 0.33299,\n",
       "        0.33182, 0.33014, 0.3299 ]),\n",
       " array([0.0262 , 0.0408 , 0.0518 , 0.0551 , 0.05892, 0.05963, 0.05953,\n",
       "        0.05982, 0.05876, 0.05807, 0.05788, 0.05645, 0.05578, 0.05463,\n",
       "        0.05352, 0.05251, 0.05147, 0.05048, 0.04976, 0.04912, 0.04864,\n",
       "        0.0482 , 0.04752, 0.04713, 0.04649, 0.04611, 0.04567, 0.04533,\n",
       "        0.04492, 0.04425, 0.04416]),\n",
       " array([0.302  , 0.3332 , 0.3238 , 0.32793, 0.33602, 0.33875, 0.33631,\n",
       "        0.33821, 0.3361 , 0.3355 , 0.33684, 0.33417, 0.3298 , 0.32739,\n",
       "        0.32484, 0.31969, 0.31685, 0.31484, 0.31096, 0.30775, 0.30656,\n",
       "        0.30389, 0.30024, 0.29758, 0.29465, 0.29236, 0.28957, 0.28766,\n",
       "        0.28691, 0.28589, 0.28574]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.05546, 1.80329, 1.65748, 1.55182, 1.47151, 1.40802, 1.36122,\n",
       "       1.32698, 1.29634, 1.2685 , 1.24917, 1.23189, 1.21924, 1.20866,\n",
       "       1.19953, 1.19177, 1.18586, 1.17961, 1.17489, 1.17082, 1.16682,\n",
       "       1.16208, 1.15718, 1.15184, 1.14816, 1.14388, 1.14011, 1.13704,\n",
       "       1.13291, 1.13055, 1.13026])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep2_1016-03-16-18'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a71a010a29be3df77e49d8f42909021441fd02d28ccbb313c9d0d3f90d3253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
