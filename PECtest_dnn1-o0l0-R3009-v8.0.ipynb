{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2 增加神经网络输出\n",
    "#v3 修改reward为直接变化量\n",
    "#v4 删除e,S特征 \n",
    "#v7 增加邻居节点特征\n",
    "#V8 修改时序上bug\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_o0l0_v8.0_'\n",
    "\n",
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=np.int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0,\"betal\":0}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    '''\n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    '''\n",
    "    #next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    #expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "\n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        statusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        if train and len(self.W)>1: #d\n",
    "            lastAction = np.row_stack((1-self.lastAction,self.lastAction)).T\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature.to(device), \n",
    "                        torch.from_numpy(lastAction).to(device), \n",
    "                        statusFeature.to(device),\n",
    "                        torch.tensor([self.reward]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "        self.lastStatusFeature = statusFeature\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            #eps_threshold = 0\n",
    "            if  not train or (train and sample >= eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                #with torch.no_grad():\n",
    "                Q_value = QNetwork(statusFeature).detach()\n",
    "                QNetwork.train()\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                \n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "            \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 01:32:47 2021 Episode: 0   Index: 198174   Loss: 1.49656 --\n",
      "Reward: [-15.74712   0.        0.     ] total reward: -15.74712\n",
      "UEHitrate: 0.01292  edgeHitrate 0.09967 sumHitrate 0.11259  privacy: 1.13279\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 01:57:20 2021 Episode: 0   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-11.44882   0.        0.     ] total reward: -11.44882\n",
      "UEHitrate: 0.02417  edgeHitrate 0.14806 sumHitrate 0.17224  privacy: 1.41968\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep0_1016-01-57-20\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 02:20:01 2021 Episode: 1   Index: 198174   Loss: 1.39442 --\n",
      "Reward: [-15.21397   0.        0.     ] total reward: -15.21397\n",
      "UEHitrate: 0.00687  edgeHitrate 0.07211 sumHitrate 0.07898  privacy: 1.39759\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 02:39:49 2021 Episode: 1   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.94596  0.       0.     ] total reward: -1.94596\n",
      "UEHitrate: 0.00592  edgeHitrate 0.06381 sumHitrate 0.06973  privacy: 1.10614\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep1_1016-02-39-49\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 03:05:24 2021 Episode: 2   Index: 198174   Loss: 1.47797 --\n",
      "Reward: [-14.09081   0.        0.     ] total reward: -14.09081\n",
      "UEHitrate: 0.00619  edgeHitrate 0.05487 sumHitrate 0.06106  privacy: 1.68302\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 03:28:15 2021 Episode: 2   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.94482  0.       0.     ] total reward: -1.94482\n",
      "UEHitrate: 0.00595  edgeHitrate 0.06399 sumHitrate 0.06994  privacy: 1.10568\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep2_1016-03-28-15\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 03:50:52 2021 Episode: 3   Index: 198174   Loss: 1.41452 --\n",
      "Reward: [-12.58552   0.        0.     ] total reward: -12.58552\n",
      "UEHitrate: 0.00608  edgeHitrate 0.07117 sumHitrate 0.07725  privacy: 1.90576\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 04:16:10 2021 Episode: 3   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.92548  0.       0.     ] total reward: -1.92548\n",
      "UEHitrate: 0.00872  edgeHitrate 0.09749 sumHitrate 0.10621  privacy: 1.095\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep3_1016-04-16-10\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 04:43:47 2021 Episode: 4   Index: 198174   Loss: 0.88031 --\n",
      "Reward: [-10.96454   0.        0.     ] total reward: -10.96454\n",
      "UEHitrate: 0.00612  edgeHitrate 0.06894 sumHitrate 0.07506  privacy: 2.07328\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 05:04:30 2021 Episode: 4   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.92087  0.       0.     ] total reward: -1.92087\n",
      "UEHitrate: 0.0063  edgeHitrate 0.10197 sumHitrate 0.10827  privacy: 1.09246\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep4_1016-05-04-30\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 05:33:20 2021 Episode: 5   Index: 198174   Loss: 0.8197 --\n",
      "Reward: [-9.33081  0.       0.     ] total reward: -9.33081\n",
      "UEHitrate: 0.00723  edgeHitrate 0.06927 sumHitrate 0.07649  privacy: 2.06143\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 05:57:56 2021 Episode: 5   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.72748  0.       0.     ] total reward: -1.72748\n",
      "UEHitrate: 0.01193  edgeHitrate 0.19152 sumHitrate 0.20345  privacy: 0.79722\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep5_1016-05-57-56\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 06:25:26 2021 Episode: 6   Index: 198174   Loss: 0.77282 --\n",
      "Reward: [-7.98051  0.       0.     ] total reward: -7.98051\n",
      "UEHitrate: 0.00835  edgeHitrate 0.08701 sumHitrate 0.09536  privacy: 2.09694\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 06:49:40 2021 Episode: 6   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.70442  0.       0.     ] total reward: -1.70442\n",
      "UEHitrate: 0.01092  edgeHitrate 0.18968 sumHitrate 0.2006  privacy: 0.77906\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep6_1016-06-49-40\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 07:18:55 2021 Episode: 7   Index: 198174   Loss: 0.31405 --\n",
      "Reward: [-6.8414  0.      0.    ] total reward: -6.8414\n",
      "UEHitrate: 0.0091  edgeHitrate 0.11574 sumHitrate 0.12483  privacy: 2.09585\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 07:41:59 2021 Episode: 7   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.79136  0.       0.     ] total reward: -1.79136\n",
      "UEHitrate: 0.01476  edgeHitrate 0.1592 sumHitrate 0.17396  privacy: 0.89746\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 08:09:54 2021 Episode: 8   Index: 198174   Loss: 0.31448 --\n",
      "Reward: [-5.90526  0.       0.     ] total reward: -5.90526\n",
      "UEHitrate: 0.01216  edgeHitrate 0.137 sumHitrate 0.14916  privacy: 1.99853\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 08:35:16 2021 Episode: 8   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.80934  0.       0.     ] total reward: -1.80934\n",
      "UEHitrate: 0.01221  edgeHitrate 0.15333 sumHitrate 0.16554  privacy: 0.93235\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 09:04:12 2021 Episode: 9   Index: 198174   Loss: 0.29688 --\n",
      "Reward: [-5.17838  0.       0.     ] total reward: -5.17838\n",
      "UEHitrate: 0.01256  edgeHitrate 0.11215 sumHitrate 0.12471  privacy: 1.94643\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 09:24:47 2021 Episode: 9   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-1.83377  0.       0.     ] total reward: -1.83377\n",
      "UEHitrate: 0.0174  edgeHitrate 0.16531 sumHitrate 0.18271  privacy: 0.98256\n",
      "------------------------------validation---------------------------#-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(4000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if (index+1) % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        '''\n",
    "        if (index+1) % 1000 == 0 :\n",
    "            psi = 0\n",
    "            for u in UEs:\n",
    "                psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "        '''\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sat Oct 16 09:25:46 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-7.59319  0.       0.     ] total reward: -7.59319\n",
      "UEHitrate: 0.0041  edgeHitrate 0.082 sumHitrate 0.0861  privacy: 1.90619\n",
      "\n",
      "--Time: Sat Oct 16 09:26:46 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-5.6951  0.      0.    ] total reward: -5.6951\n",
      "UEHitrate: 0.0047  edgeHitrate 0.09655 sumHitrate 0.10125  privacy: 1.60584\n",
      "\n",
      "--Time: Sat Oct 16 09:27:43 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-4.88806  0.       0.     ] total reward: -4.88806\n",
      "UEHitrate: 0.0062  edgeHitrate 0.0922 sumHitrate 0.0984  privacy: 1.44793\n",
      "\n",
      "--Time: Sat Oct 16 09:28:44 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-4.35053  0.       0.     ] total reward: -4.35053\n",
      "UEHitrate: 0.00625  edgeHitrate 0.09545 sumHitrate 0.1017  privacy: 1.35225\n",
      "\n",
      "--Time: Sat Oct 16 09:29:43 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-3.94325  0.       0.     ] total reward: -3.94325\n",
      "UEHitrate: 0.00668  edgeHitrate 0.10222 sumHitrate 0.1089  privacy: 1.29552\n",
      "\n",
      "--Time: Sat Oct 16 09:30:43 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-3.6619  0.      0.    ] total reward: -3.6619\n",
      "UEHitrate: 0.0065  edgeHitrate 0.10297 sumHitrate 0.10947  privacy: 1.25178\n",
      "\n",
      "--Time: Sat Oct 16 09:31:41 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-3.43848  0.       0.     ] total reward: -3.43848\n",
      "UEHitrate: 0.00636  edgeHitrate 0.10386 sumHitrate 0.11021  privacy: 1.2225\n",
      "\n",
      "--Time: Sat Oct 16 09:32:39 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-3.22195  0.       0.     ] total reward: -3.22195\n",
      "UEHitrate: 0.00661  edgeHitrate 0.10802 sumHitrate 0.11464  privacy: 1.20125\n",
      "\n",
      "--Time: Sat Oct 16 09:33:38 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-3.05611  0.       0.     ] total reward: -3.05611\n",
      "UEHitrate: 0.00654  edgeHitrate 0.11191 sumHitrate 0.11846  privacy: 1.1817\n",
      "\n",
      "--Time: Sat Oct 16 09:34:37 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-2.92495  0.       0.     ] total reward: -2.92495\n",
      "UEHitrate: 0.00707  edgeHitrate 0.11331 sumHitrate 0.12038  privacy: 1.1604\n",
      "\n",
      "--Time: Sat Oct 16 09:35:35 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-2.79582  0.       0.     ] total reward: -2.79582\n",
      "UEHitrate: 0.00746  edgeHitrate 0.11831 sumHitrate 0.12577  privacy: 1.13432\n",
      "\n",
      "--Time: Sat Oct 16 09:36:34 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-2.68743  0.       0.     ] total reward: -2.68743\n",
      "UEHitrate: 0.00752  edgeHitrate 0.12176 sumHitrate 0.12928  privacy: 1.10634\n",
      "\n",
      "--Time: Sat Oct 16 09:37:35 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-2.57587  0.       0.     ] total reward: -2.57587\n",
      "UEHitrate: 0.00793  edgeHitrate 0.12561 sumHitrate 0.13354  privacy: 1.07433\n",
      "\n",
      "--Time: Sat Oct 16 09:38:33 2021   Index: 139999   Loss: 0.0 --\n",
      "Reward: [-2.48055  0.       0.     ] total reward: -2.48055\n",
      "UEHitrate: 0.0085  edgeHitrate 0.13085 sumHitrate 0.13935  privacy: 1.04577\n",
      "\n",
      "--Time: Sat Oct 16 09:39:32 2021   Index: 149999   Loss: 0.0 --\n",
      "Reward: [-2.38276  0.       0.     ] total reward: -2.38276\n",
      "UEHitrate: 0.00878  edgeHitrate 0.13697 sumHitrate 0.14575  privacy: 1.02028\n",
      "\n",
      "--Time: Sat Oct 16 09:40:32 2021   Index: 159999   Loss: 0.0 --\n",
      "Reward: [-2.29911  0.       0.     ] total reward: -2.29911\n",
      "UEHitrate: 0.00929  edgeHitrate 0.14077 sumHitrate 0.15006  privacy: 0.99295\n",
      "\n",
      "--Time: Sat Oct 16 09:41:32 2021   Index: 169999   Loss: 0.0 --\n",
      "Reward: [-2.21895  0.       0.     ] total reward: -2.21895\n",
      "UEHitrate: 0.00927  edgeHitrate 0.14739 sumHitrate 0.15666  privacy: 0.9696\n",
      "\n",
      "--Time: Sat Oct 16 09:42:29 2021   Index: 179999   Loss: 0.0 --\n",
      "Reward: [-2.13258  0.       0.     ] total reward: -2.13258\n",
      "UEHitrate: 0.01009  edgeHitrate 0.15441 sumHitrate 0.16451  privacy: 0.93792\n",
      "\n",
      "--Time: Sat Oct 16 09:43:30 2021   Index: 189999   Loss: 0.0 --\n",
      "Reward: [-2.04923  0.       0.     ] total reward: -2.04923\n",
      "UEHitrate: 0.01031  edgeHitrate 0.16136 sumHitrate 0.17167  privacy: 0.91213\n",
      "\n",
      "--Time: Sat Oct 16 09:44:28 2021   Index: 199999   Loss: 0.0 --\n",
      "Reward: [-1.97963  0.       0.     ] total reward: -1.97963\n",
      "UEHitrate: 0.01042  edgeHitrate 0.16668 sumHitrate 0.1771  privacy: 0.88533\n",
      "\n",
      "--Time: Sat Oct 16 09:45:29 2021   Index: 209999   Loss: 0.0 --\n",
      "Reward: [-1.91594  0.       0.     ] total reward: -1.91594\n",
      "UEHitrate: 0.01044  edgeHitrate 0.1719 sumHitrate 0.18234  privacy: 0.84854\n",
      "\n",
      "--Time: Sat Oct 16 09:46:28 2021   Index: 219999   Loss: 0.0 --\n",
      "Reward: [-1.85716  0.       0.     ] total reward: -1.85716\n",
      "UEHitrate: 0.0104  edgeHitrate 0.17506 sumHitrate 0.18546  privacy: 0.82196\n",
      "\n",
      "--Time: Sat Oct 16 09:47:27 2021   Index: 229999   Loss: 0.0 --\n",
      "Reward: [-1.8056  0.      0.    ] total reward: -1.8056\n",
      "UEHitrate: 0.01056  edgeHitrate 0.1793 sumHitrate 0.18986  privacy: 0.80637\n",
      "\n",
      "--Time: Sat Oct 16 09:48:25 2021   Index: 239999   Loss: 0.0 --\n",
      "Reward: [-1.76122  0.       0.     ] total reward: -1.76122\n",
      "UEHitrate: 0.01082  edgeHitrate 0.18298 sumHitrate 0.19381  privacy: 0.78873\n",
      "\n",
      "--Time: Sat Oct 16 09:49:24 2021   Index: 249999   Loss: 0.0 --\n",
      "Reward: [-1.71906  0.       0.     ] total reward: -1.71906\n",
      "UEHitrate: 0.01098  edgeHitrate 0.18893 sumHitrate 0.19992  privacy: 0.78403\n",
      "\n",
      "--Time: Sat Oct 16 09:50:23 2021   Index: 259999   Loss: 0.0 --\n",
      "Reward: [-1.68209  0.       0.     ] total reward: -1.68209\n",
      "UEHitrate: 0.01089  edgeHitrate 0.1915 sumHitrate 0.2024  privacy: 0.77867\n",
      "\n",
      "--Time: Sat Oct 16 09:51:21 2021   Index: 269999   Loss: 0.0 --\n",
      "Reward: [-1.65226  0.       0.     ] total reward: -1.65226\n",
      "UEHitrate: 0.01095  edgeHitrate 0.19479 sumHitrate 0.20574  privacy: 0.7769\n",
      "\n",
      "--Time: Sat Oct 16 09:52:21 2021   Index: 279999   Loss: 0.0 --\n",
      "Reward: [-1.62119  0.       0.     ] total reward: -1.62119\n",
      "UEHitrate: 0.01092  edgeHitrate 0.19824 sumHitrate 0.20916  privacy: 0.77434\n",
      "\n",
      "--Time: Sat Oct 16 09:53:20 2021   Index: 289999   Loss: 0.0 --\n",
      "Reward: [-1.59082  0.       0.     ] total reward: -1.59082\n",
      "UEHitrate: 0.01087  edgeHitrate 0.20216 sumHitrate 0.21303  privacy: 0.77765\n",
      "\n",
      "--Time: Sat Oct 16 09:54:20 2021   Index: 299999   Loss: 0.0 --\n",
      "Reward: [-1.55797  0.       0.     ] total reward: -1.55797\n",
      "UEHitrate: 0.01091  edgeHitrate 0.20925 sumHitrate 0.22016  privacy: 0.78144\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sat Oct 16 09:54:26 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-1.55483  0.       0.     ] total reward: -1.55483\n",
      "UEHitrate: 0.0109  edgeHitrate 0.20985 sumHitrate 0.22075  privacy: 0.78181\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0861 , 0.10125, 0.0984 , 0.1017 , 0.1089 , 0.10947, 0.11021,\n",
       "        0.11464, 0.11846, 0.12038, 0.12577, 0.12928, 0.13354, 0.13935,\n",
       "        0.14575, 0.15006, 0.15666, 0.16451, 0.17167, 0.1771 , 0.18234,\n",
       "        0.18546, 0.18986, 0.19381, 0.19992, 0.2024 , 0.20574, 0.20916,\n",
       "        0.21303, 0.22016, 0.22075]),\n",
       " array([0.0041 , 0.0047 , 0.0062 , 0.00625, 0.00668, 0.0065 , 0.00636,\n",
       "        0.00661, 0.00654, 0.00707, 0.00746, 0.00752, 0.00793, 0.0085 ,\n",
       "        0.00878, 0.00929, 0.00927, 0.01009, 0.01031, 0.01042, 0.01044,\n",
       "        0.0104 , 0.01056, 0.01082, 0.01098, 0.01089, 0.01095, 0.01092,\n",
       "        0.01087, 0.01091, 0.0109 ]),\n",
       " array([0.082  , 0.09655, 0.0922 , 0.09545, 0.10222, 0.10297, 0.10386,\n",
       "        0.10802, 0.11191, 0.11331, 0.11831, 0.12176, 0.12561, 0.13085,\n",
       "        0.13697, 0.14077, 0.14739, 0.15441, 0.16136, 0.16668, 0.1719 ,\n",
       "        0.17506, 0.1793 , 0.18298, 0.18893, 0.1915 , 0.19479, 0.19824,\n",
       "        0.20216, 0.20925, 0.20985]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.90619, 1.60584, 1.44793, 1.35225, 1.29552, 1.25178, 1.2225 ,\n",
       "       1.20125, 1.1817 , 1.1604 , 1.13432, 1.10634, 1.07433, 1.04577,\n",
       "       1.02028, 0.99295, 0.9696 , 0.93792, 0.91213, 0.88533, 0.84854,\n",
       "       0.82196, 0.80637, 0.78873, 0.78403, 0.77867, 0.7769 , 0.77434,\n",
       "       0.77765, 0.78144, 0.78181])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_o0l0_v8.0_ep6_1016-06-49-40'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a71a010a29be3df77e49d8f42909021441fd02d28ccbb313c9d0d3f90d3253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
