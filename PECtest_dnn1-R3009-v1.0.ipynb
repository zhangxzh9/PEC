{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1 修改时序上bug\n",
    "#v2 增加边缘特征\n",
    "#v3 考虑长期收益\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "import os\n",
    "pathName = '/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/'\n",
    "if not os.path.exists(pathName):\n",
    "    os.makedirs(pathName)\n",
    "MODELPATH = pathName + 'dnn_v1.0_'\n",
    "\n",
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=np.int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":0.01,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','content'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self,inputs,outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conn1 = nn.Linear(inputs,32)\n",
    "        self.conn2 = nn.Linear(32,128)\n",
    "        self.conn3 = nn.Linear(128,16)\n",
    "        self.conn4 = nn.Linear(16, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = x.float().to(device)\n",
    "        x = F.relu(self.conn1(x))\n",
    "        x = F.relu(self.conn2(x))\n",
    "        x = F.relu(self.conn3(x))\n",
    "        return self.conn4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        #print(len(memory))\n",
    "        return 0\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    #non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    #non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    content_batch = torch.cat(batch.content)\n",
    "    next_state_batch = torch.cat(batch.next_state)\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_mask_bacth = action_batch.ge(0.5)\n",
    "    state_action_values = torch.stack(torch.masked_select(policy_net(state_batch),state_action_mask_bacth).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "    \n",
    "    #state_action_values = torch.stack((policy_net(state_batch).squeeze(1) * action_batch).chunk(BATCH_SIZE,dim=0)).sum(dim=1)\n",
    "\n",
    "    #print(state_action_values,state_action_values.dtype)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    #next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    #next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    \n",
    "    def getNextStatusQ(s_batch,c_batch):\n",
    "        \n",
    "        Q_value= torch.stack(target_net(s_batch).detach().chunk(BATCH_SIZE,dim=0))\n",
    "        c = c_batch.chunk(BATCH_SIZE,dim=0)\n",
    "        action = torch.zeros(size=(BATCH_SIZE,contentNum,2),dtype=int).to(device)\n",
    "        for b in range(BATCH_SIZE):\n",
    "            Q_value_sortindex = list((Q_value[b,:,1]-Q_value[b,:,0]).argsort(descending=True)[0:Bu])\n",
    "            i = c[b].squeeze()\n",
    "            if i not in Q_value_sortindex:\n",
    "                Q_value_sortindex.pop()\n",
    "            action[b,i,1] = 1\n",
    "            for index in Q_value_sortindex:\n",
    "                action[b,index,1] = 1\n",
    "        action[:,:,0]=1-action[:,:,1]\n",
    "        action_mask = action.ge(0.5).to(device)\n",
    "        next_state_values = torch.stack(torch.masked_select(Q_value,action_mask).chunk(BATCH_SIZE,dim=0)).sum(dim=1).float()\n",
    "\n",
    "        return next_state_values\n",
    "    \n",
    "    next_state_values =  getNextStatusQ(next_state_batch,content_batch)\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    #expected_state_action_values =  reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values.unsqueeze(1), expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,QNetwork,train,memory):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        statusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        \n",
    "        if train and len(self.W)>1: #d\n",
    "            lastAction = np.row_stack((1-self.lastAction,self.lastAction)).T\n",
    "            #lastAction = self.lastAction\n",
    "            memory.push(self.lastStatusFeature.to(device), \n",
    "                        torch.from_numpy(lastAction).to(device), \n",
    "                        statusFeature.to(device),\n",
    "                        torch.tensor([self.reward]).to(device),\n",
    "                        torch.tensor([self.W[-1]]).to(device))\n",
    "        self.lastStatusFeature = statusFeature\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            sample = random.random()\n",
    "            eps_threshold = EPS_END + (EPS_START - EPS_END) *  np.exp(-1. * agentStep / EPS_DECAY)\n",
    "            #eps_threshold = 0\n",
    "            if  not train or (train and sample >= eps_threshold): #var greedy\n",
    "                QNetwork.eval()\n",
    "                #with torch.no_grad():\n",
    "                Q_value = QNetwork(statusFeature).detach()\n",
    "                QNetwork.train()\n",
    "                actionIndex = list((Q_value[:,1]-Q_value[:,0]).argsort(descending=True)[0:self.Bu])\n",
    "                \n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "            \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 14:12:02 2021 Episode: 0   Index: 198174   Loss: 0.00889 --\n",
      "Reward: [-0.15781  0.00759  0.09895] total reward: -0.05127\n",
      "UEHitrate: 0.0106  edgeHitrate 0.12368 sumHitrate 0.13428  privacy: 1.10748\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 14:25:35 2021 Episode: 0   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-0.12136  0.00575  0.1805 ] total reward: 0.06488\n",
      "UEHitrate: 0.008  edgeHitrate 0.22562 sumHitrate 0.23362  privacy: 1.03119\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_v1.0_ep0_1016-14-25-35\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 15:48:06 2021 Episode: 1   Index: 198174   Loss: 0.00751 --\n",
      "Reward: [-0.15466  0.00594  0.08336] total reward: -0.06536\n",
      "UEHitrate: 0.00728  edgeHitrate 0.10421 sumHitrate 0.11148  privacy: 1.23626\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 16:00:14 2021 Episode: 1   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-0.05576  0.03445  0.14242] total reward: 0.12111\n",
      "UEHitrate: 0.05564  edgeHitrate 0.17803 sumHitrate 0.23367  privacy: 2.1337\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_v1.0_ep1_1016-16-00-14\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 17:18:50 2021 Episode: 2   Index: 198174   Loss: 0.01446 --\n",
      "Reward: [-0.14173  0.02279  0.1508 ] total reward: 0.03187\n",
      "UEHitrate: 0.05346  edgeHitrate 0.18851 sumHitrate 0.24197  privacy: 1.61621\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 17:32:12 2021 Episode: 2   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-0.02037  0.01726  0.29507] total reward: 0.29196\n",
      "UEHitrate: 0.06701  edgeHitrate 0.36884 sumHitrate 0.43585  privacy: 0.87859\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_v1.0_ep2_1016-17-32-12\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 18:57:28 2021 Episode: 3   Index: 198174   Loss: 0.02013 --\n",
      "Reward: [-0.12617  0.02443  0.1836 ] total reward: 0.08187\n",
      "UEHitrate: 0.06312  edgeHitrate 0.2295 sumHitrate 0.29263  privacy: 1.8473\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 19:11:07 2021 Episode: 3   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-0.01699  0.01642  0.33015] total reward: 0.32958\n",
      "UEHitrate: 0.06701  edgeHitrate 0.41269 sumHitrate 0.47969  privacy: 0.8703\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "\n",
      "/home/ubuntu/data/PEC/model_dict/R3009_U5_V100/dnn_v1.0_ep3_1016-19-11-07\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 20:31:47 2021 Episode: 4   Index: 198174   Loss: 0.03416 --\n",
      "Reward: [-0.10768  0.01804  0.14434] total reward: 0.0547\n",
      "UEHitrate: 0.04651  edgeHitrate 0.18043 sumHitrate 0.22694  privacy: 1.91978\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 20:43:45 2021 Episode: 4   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-0.01828  0.01232  0.2781 ] total reward: 0.27213\n",
      "UEHitrate: 0.04656  edgeHitrate 0.34762 sumHitrate 0.39418  privacy: 0.98936\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 21:59:15 2021 Episode: 5   Index: 198174   Loss: 0.02233 --\n",
      "Reward: [-0.09884  0.01942  0.17945] total reward: 0.10003\n",
      "UEHitrate: 0.06187  edgeHitrate 0.22432 sumHitrate 0.28619  privacy: 2.08482\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 22:10:51 2021 Episode: 5   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-0.02313  0.01347  0.1994 ] total reward: 0.18974\n",
      "UEHitrate: 0.04887  edgeHitrate 0.24924 sumHitrate 0.29811  privacy: 1.23365\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sat Oct 16 23:26:01 2021 Episode: 6   Index: 198174   Loss: 0.02721 --\n",
      "Reward: [-0.08984  0.02024  0.18778] total reward: 0.11818\n",
      "UEHitrate: 0.0637  edgeHitrate 0.23472 sumHitrate 0.29842  privacy: 2.08567\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sat Oct 16 23:38:16 2021 Episode: 6   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-0.02018  0.00432  0.29754] total reward: 0.28167\n",
      "UEHitrate: 0.00712  edgeHitrate 0.37192 sumHitrate 0.37904  privacy: 0.94674\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 17 01:27:54 2021 Episode: 7   Index: 198174   Loss: 0.03274 --\n",
      "Reward: [-0.06899  0.01795  0.20462] total reward: 0.15358\n",
      "UEHitrate: 0.05982  edgeHitrate 0.25577 sumHitrate 0.31559  privacy: 1.25813\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sun Oct 17 01:43:35 2021 Episode: 7   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-0.02377  0.0055   0.20059] total reward: 0.18233\n",
      "UEHitrate: 0.00772  edgeHitrate 0.25074 sumHitrate 0.25846  privacy: 1.17763\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 17 03:22:17 2021 Episode: 8   Index: 198174   Loss: 0.03563 --\n",
      "Reward: [-0.06938  0.02211  0.21421] total reward: 0.16694\n",
      "UEHitrate: 0.07299  edgeHitrate 0.26777 sumHitrate 0.34075  privacy: 1.84658\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sun Oct 17 03:38:22 2021 Episode: 8   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-0.08792  0.02131  0.23396] total reward: 0.16735\n",
      "UEHitrate: 0.07724  edgeHitrate 0.29245 sumHitrate 0.36969  privacy: 1.85533\n",
      "------------------------------validation---------------------------#-------\n",
      "\n",
      "------------------------------train----------------------------------\n",
      "--Time: Sun Oct 17 05:19:34 2021 Episode: 9   Index: 198174   Loss: 0.04688 --\n",
      "Reward: [-0.06229  0.022    0.19949] total reward: 0.1592\n",
      "UEHitrate: 0.07649  edgeHitrate 0.24937 sumHitrate 0.32586  privacy: 1.73883\n",
      "------------------------------train----------------------------------\n",
      "\n",
      "------------------------------validation---------------------------#-------\n",
      "--Time: Sun Oct 17 05:31:28 2021 Episode: 9   Index: 253629   Loss: 0.0 --\n",
      "Reward: [-0.04158  0.02164  0.19671] total reward: 0.17677\n",
      "UEHitrate: 0.08156  edgeHitrate 0.24589 sumHitrate 0.32745  privacy: 1.29095\n",
      "------------------------------validation---------------------------#-------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get screen size so that we can initialize layers correctly based on shape\n",
    "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
    "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
    "#init_screen = get_screen()\n",
    "#_, _, screen_height, screen_width = init_screen.shape\n",
    "# Get number of actions from gym action space\n",
    "#n_actions = env.action_space.n\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters())\n",
    "memory = ReplayMemory(4000)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,1,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)\n",
    "        agentStep += 1\n",
    "\n",
    "        # train the DQN network\n",
    "        if (index+1) % 10 == 0:\n",
    "            # Perform one step of the optimization (on the policy network)\n",
    "            loss += float(optimize_model())\n",
    "            # Update the target network, copying all weights and biases in DQN\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        '''\n",
    "        if (index+1) % 1000 == 0 :\n",
    "            psi = 0\n",
    "            for u in UEs:\n",
    "                psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "        '''\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------train----------------------------------\")\n",
    "    print()\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    #valid the hyperparameters\n",
    "    env = ENV(userNum,contentNum,latency,Bu)\n",
    "    UEs = {}\n",
    "    sumReward = np.zeros(3)\n",
    "    loss = 0\n",
    "    UEHit = np.zeros(userNum)\n",
    "    edgeHit = 0\n",
    "    for index,trace in validUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "        ue = UEs[uit[0]]\n",
    "\n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit,policy_net,0,memory)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Ro)\n",
    "        sumReward[2] += float(ue.Rl)       \n",
    "\n",
    "    psi = 0\n",
    "    for u in UEs:\n",
    "        psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"------------------------------validation---------------------------#-------\")\n",
    "    print()\n",
    "    modelSavePath = MODELPATH+'ep{}_'.format(i_episode)+time.strftime(\"%m%d-%H-%M-%S\",time.localtime(time.time()))\n",
    "    torch.save(policy_net.state_dict(),modelSavePath)\n",
    "    if sumReward.sum() > bestReward:\n",
    "        bestReward = sumReward.sum()\n",
    "        bestEpisode = i_episode\n",
    "        bestPath = modelSavePath\n",
    "        print()\n",
    "        print(bestPath)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sun Oct 17 05:31:57 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-0.0433   0.002    0.29736] total reward: 0.25606\n",
      "UEHitrate: 0.0032  edgeHitrate 0.3717 sumHitrate 0.3749  privacy: 0.88866\n",
      "\n",
      "--Time: Sun Oct 17 05:32:26 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-0.03878  0.0052   0.34232] total reward: 0.30874\n",
      "UEHitrate: 0.0168  edgeHitrate 0.4279 sumHitrate 0.4447  privacy: 0.92239\n",
      "\n",
      "--Time: Sun Oct 17 05:32:54 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-0.03781  0.00725  0.33515] total reward: 0.30459\n",
      "UEHitrate: 0.0257  edgeHitrate 0.41893 sumHitrate 0.44463  privacy: 0.8483\n",
      "\n",
      "--Time: Sun Oct 17 05:33:22 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-0.03436  0.00853  0.32944] total reward: 0.30361\n",
      "UEHitrate: 0.03182  edgeHitrate 0.4118 sumHitrate 0.44362  privacy: 0.78819\n",
      "\n",
      "--Time: Sun Oct 17 05:33:50 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-0.03206  0.01049  0.33174] total reward: 0.31017\n",
      "UEHitrate: 0.04028  edgeHitrate 0.41468 sumHitrate 0.45496  privacy: 0.77447\n",
      "\n",
      "--Time: Sun Oct 17 05:34:17 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-0.03041  0.01165  0.32976] total reward: 0.311\n",
      "UEHitrate: 0.04578  edgeHitrate 0.4122 sumHitrate 0.45798  privacy: 0.77967\n",
      "\n",
      "--Time: Sun Oct 17 05:34:45 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-0.02917  0.0125   0.32504] total reward: 0.30838\n",
      "UEHitrate: 0.05046  edgeHitrate 0.4063 sumHitrate 0.45676  privacy: 0.78902\n",
      "\n",
      "--Time: Sun Oct 17 05:35:13 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-0.02756  0.01361  0.32658] total reward: 0.31262\n",
      "UEHitrate: 0.05588  edgeHitrate 0.40823 sumHitrate 0.4641  privacy: 0.79672\n",
      "\n",
      "--Time: Sun Oct 17 05:35:40 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-0.02631  0.01407  0.32624] total reward: 0.314\n",
      "UEHitrate: 0.05804  edgeHitrate 0.4078 sumHitrate 0.46584  privacy: 0.80597\n",
      "\n",
      "--Time: Sun Oct 17 05:36:08 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.02528  0.01456  0.32699] total reward: 0.31627\n",
      "UEHitrate: 0.06063  edgeHitrate 0.40874 sumHitrate 0.46937  privacy: 0.81528\n",
      "\n",
      "--Time: Sun Oct 17 05:36:36 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-0.0243   0.0151   0.32724] total reward: 0.31805\n",
      "UEHitrate: 0.06329  edgeHitrate 0.40905 sumHitrate 0.47235  privacy: 0.82163\n",
      "\n",
      "--Time: Sun Oct 17 05:37:04 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-0.02352  0.01542  0.32648] total reward: 0.31838\n",
      "UEHitrate: 0.0645  edgeHitrate 0.4081 sumHitrate 0.4726  privacy: 0.82698\n",
      "\n",
      "--Time: Sun Oct 17 05:37:31 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-0.02272  0.01566  0.32586] total reward: 0.31881\n",
      "UEHitrate: 0.0651  edgeHitrate 0.40733 sumHitrate 0.47243  privacy: 0.83159\n",
      "\n",
      "--Time: Sun Oct 17 05:37:59 2021   Index: 139999   Loss: 0.0 --\n",
      "Reward: [-0.02202  0.01588  0.32801] total reward: 0.32187\n",
      "UEHitrate: 0.06596  edgeHitrate 0.41001 sumHitrate 0.47597  privacy: 0.83352\n",
      "\n",
      "--Time: Sun Oct 17 05:38:26 2021   Index: 149999   Loss: 0.0 --\n",
      "Reward: [-0.02137  0.01602  0.33037] total reward: 0.32502\n",
      "UEHitrate: 0.06664  edgeHitrate 0.41297 sumHitrate 0.47961  privacy: 0.83749\n",
      "\n",
      "--Time: Sun Oct 17 05:38:54 2021   Index: 159999   Loss: 0.0 --\n",
      "Reward: [-0.02085  0.01609  0.33021] total reward: 0.32545\n",
      "UEHitrate: 0.06644  edgeHitrate 0.41276 sumHitrate 0.4792  privacy: 0.84154\n",
      "\n",
      "--Time: Sun Oct 17 05:39:22 2021   Index: 169999   Loss: 0.0 --\n",
      "Reward: [-0.02029  0.01622  0.3326 ] total reward: 0.32854\n",
      "UEHitrate: 0.06703  edgeHitrate 0.41575 sumHitrate 0.48278  privacy: 0.84653\n",
      "\n",
      "--Time: Sun Oct 17 05:39:50 2021   Index: 179999   Loss: 0.0 --\n",
      "Reward: [-0.01971  0.0163   0.33442] total reward: 0.33102\n",
      "UEHitrate: 0.06721  edgeHitrate 0.41803 sumHitrate 0.48523  privacy: 0.84831\n",
      "\n",
      "--Time: Sun Oct 17 05:40:18 2021   Index: 189999   Loss: 0.0 --\n",
      "Reward: [-0.01921  0.01633  0.33584] total reward: 0.33295\n",
      "UEHitrate: 0.06725  edgeHitrate 0.41979 sumHitrate 0.48704  privacy: 0.85106\n",
      "\n",
      "--Time: Sun Oct 17 05:40:46 2021   Index: 199999   Loss: 0.0 --\n",
      "Reward: [-0.01877  0.01639  0.33584] total reward: 0.33346\n",
      "UEHitrate: 0.06759  edgeHitrate 0.4198 sumHitrate 0.48739  privacy: 0.85429\n",
      "\n",
      "--Time: Sun Oct 17 05:41:13 2021   Index: 209999   Loss: 0.0 --\n",
      "Reward: [-0.0184   0.01644  0.3359 ] total reward: 0.33394\n",
      "UEHitrate: 0.06769  edgeHitrate 0.41988 sumHitrate 0.48757  privacy: 0.85917\n",
      "\n",
      "--Time: Sun Oct 17 05:41:41 2021   Index: 219999   Loss: 0.0 --\n",
      "Reward: [-0.01804  0.01645  0.33449] total reward: 0.3329\n",
      "UEHitrate: 0.06755  edgeHitrate 0.41812 sumHitrate 0.48566  privacy: 0.86258\n",
      "\n",
      "--Time: Sun Oct 17 05:42:09 2021   Index: 229999   Loss: 0.0 --\n",
      "Reward: [-0.01775  0.01637  0.33299] total reward: 0.33161\n",
      "UEHitrate: 0.06701  edgeHitrate 0.41624 sumHitrate 0.48325  privacy: 0.86518\n",
      "\n",
      "--Time: Sun Oct 17 05:42:37 2021   Index: 239999   Loss: 0.0 --\n",
      "Reward: [-0.01741  0.01641  0.33152] total reward: 0.33052\n",
      "UEHitrate: 0.067  edgeHitrate 0.4144 sumHitrate 0.4814  privacy: 0.86651\n",
      "\n",
      "--Time: Sun Oct 17 05:43:05 2021   Index: 249999   Loss: 0.0 --\n",
      "Reward: [-0.0171   0.01643  0.33081] total reward: 0.33014\n",
      "UEHitrate: 0.06704  edgeHitrate 0.41351 sumHitrate 0.48055  privacy: 0.8697\n",
      "\n",
      "--Time: Sun Oct 17 05:43:33 2021   Index: 259999   Loss: 0.0 --\n",
      "Reward: [-0.0168   0.01647  0.32888] total reward: 0.32855\n",
      "UEHitrate: 0.0672  edgeHitrate 0.4111 sumHitrate 0.47831  privacy: 0.87188\n",
      "\n",
      "--Time: Sun Oct 17 05:44:01 2021   Index: 269999   Loss: 0.0 --\n",
      "Reward: [-0.01656  0.01656  0.32689] total reward: 0.32688\n",
      "UEHitrate: 0.06747  edgeHitrate 0.40861 sumHitrate 0.47608  privacy: 0.87435\n",
      "\n",
      "--Time: Sun Oct 17 05:44:28 2021   Index: 279999   Loss: 0.0 --\n",
      "Reward: [-0.01629  0.0166   0.32607] total reward: 0.32637\n",
      "UEHitrate: 0.06752  edgeHitrate 0.40759 sumHitrate 0.4751  privacy: 0.87604\n",
      "\n",
      "--Time: Sun Oct 17 05:44:55 2021   Index: 289999   Loss: 0.0 --\n",
      "Reward: [-0.01598  0.0167   0.32685] total reward: 0.32756\n",
      "UEHitrate: 0.06801  edgeHitrate 0.40856 sumHitrate 0.47658  privacy: 0.87879\n",
      "\n",
      "--Time: Sun Oct 17 05:45:24 2021   Index: 299999   Loss: 0.0 --\n",
      "Reward: [-0.01565  0.01668  0.32843] total reward: 0.32946\n",
      "UEHitrate: 0.06801  edgeHitrate 0.41054 sumHitrate 0.47855  privacy: 0.88124\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 17 05:45:27 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-0.01562  0.01666  0.32859] total reward: 0.32963\n",
      "UEHitrate: 0.06794  edgeHitrate 0.41074 sumHitrate 0.47868  privacy: 0.88132\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "policy_net = DQN(5, 2).to(device)\n",
    "target_net = DQN(5, 2).to(device)\n",
    "policy_net.load_state_dict(torch.load(bestPath))\n",
    "policy_net.eval()\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit,policy_net,0,memory)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.3749 , 0.4447 , 0.44463, 0.44362, 0.45496, 0.45798, 0.45676,\n",
       "        0.4641 , 0.46584, 0.46937, 0.47235, 0.4726 , 0.47243, 0.47597,\n",
       "        0.47961, 0.4792 , 0.48278, 0.48523, 0.48704, 0.48739, 0.48757,\n",
       "        0.48566, 0.48325, 0.4814 , 0.48055, 0.47831, 0.47608, 0.4751 ,\n",
       "        0.47658, 0.47855, 0.47868]),\n",
       " array([0.0032 , 0.0168 , 0.0257 , 0.03182, 0.04028, 0.04578, 0.05046,\n",
       "        0.05588, 0.05804, 0.06063, 0.06329, 0.0645 , 0.0651 , 0.06596,\n",
       "        0.06664, 0.06644, 0.06703, 0.06721, 0.06725, 0.06759, 0.06769,\n",
       "        0.06755, 0.06701, 0.067  , 0.06704, 0.0672 , 0.06747, 0.06752,\n",
       "        0.06801, 0.06801, 0.06794]),\n",
       " array([0.3717 , 0.4279 , 0.41893, 0.4118 , 0.41468, 0.4122 , 0.4063 ,\n",
       "        0.40823, 0.4078 , 0.40874, 0.40905, 0.4081 , 0.40733, 0.41001,\n",
       "        0.41297, 0.41276, 0.41575, 0.41803, 0.41979, 0.4198 , 0.41988,\n",
       "        0.41812, 0.41624, 0.4144 , 0.41351, 0.4111 , 0.40861, 0.40759,\n",
       "        0.40856, 0.41054, 0.41074]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.88866, 0.92239, 0.8483 , 0.78819, 0.77447, 0.77967, 0.78902,\n",
       "       0.79672, 0.80597, 0.81528, 0.82163, 0.82698, 0.83159, 0.83352,\n",
       "       0.83749, 0.84154, 0.84653, 0.84831, 0.85106, 0.85429, 0.85917,\n",
       "       0.86258, 0.86518, 0.86651, 0.8697 , 0.87188, 0.87435, 0.87604,\n",
       "       0.87879, 0.88124, 0.88132])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86a71a010a29be3df77e49d8f42909021441fd02d28ccbb313c9d0d3f90d3253"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('py38': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
