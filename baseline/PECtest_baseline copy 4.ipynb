{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>1030</td>\n",
       "      <td>101001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11807</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15068</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>1035</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5375</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1030</td>\n",
       "      <td>10202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5992</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3468</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148        1030        101001     0   \n",
       "1       203  5779    0        0         7        1030         10203     0   \n",
       "2       208  4675    0        0        92        1035         10203     0   \n",
       "3       159   332    0        0        56        1030         10202     0   \n",
       "4        50   674    0        0       439        1030         10203     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831   29  2591880        34        1030         10203     0   \n",
       "300979  158  8448   29  2591880        34        1030         10203     0   \n",
       "300980  483  6463   29  2591940        35        1030         10203     0   \n",
       "300981  158  4715   29  2591940        34        1030         10203     0   \n",
       "300982  483  2021   29  2591940        34        1030         10203     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0      11807          1            2  \n",
       "1              0      15068          1            2  \n",
       "2              0       5375          1            2  \n",
       "3              0       5992          1            2  \n",
       "4              0       3468          1            2  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0      10010          1            2  \n",
       "300979         0      23340          1            2  \n",
       "300980         0      10010          1            2  \n",
       "300981         0      23340          1            2  \n",
       "300982         0      10010          1            2  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000,\n",
       " 500,\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148        1030        101001     0   \n",
       " 1       203  5779    0        0         7        1030         10203     0   \n",
       " 2       208  4675    0        0        92        1035         10203     0   \n",
       " 3       159   332    0        0        56        1030         10202     0   \n",
       " 4        50   674    0        0       439        1030         10203     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 198170  264  7442   17  1555140        90        1035         10203     0   \n",
       " 198171   19  9362   17  1555140       424        1035         10203     0   \n",
       " 198172   82  9223   17  1555140        94        1037         10203     0   \n",
       " 198173   35  4164   17  1555140        22        1030         10203     0   \n",
       " 198174  239  5062   17  1555140        89        1035         10203     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0      11807          1            2  \n",
       " 1              0      15068          1            2  \n",
       " 2              0       5375          1            2  \n",
       " 3              0       5992          1            2  \n",
       " 4              0       3468          1            2  \n",
       " ...          ...        ...        ...          ...  \n",
       " 198170         0       7592          1            2  \n",
       " 198171         0       5938          1            2  \n",
       " 198172         0      11393          1            2  \n",
       " 198173         0       5866          1            2  \n",
       " 198174         0      23746          1            2  \n",
       " \n",
       " [198175 rows x 12 columns])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=25,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p) , \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                self.l_edge,\n",
    "                self.l_cp)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class UE_random(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e):\n",
    "\n",
    "        self.Rh = - self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)) - torch.log(lastru * lastp + (1-lastru) * (1-lastp))).sum()\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        self.Rl =   self.BETAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        return  self.Rh+self.Ro+self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit):\n",
    "\n",
    "\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.Bu,self.l_edge,self.l_cp,self.e)\n",
    "        \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "\n",
    "        actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "        \n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.5,\"betal\":0.5}\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "UEHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "edgeHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "privacyReduction = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in UIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE_random(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Rl)\n",
    "        sumReward[2] += float(ue.Ro)\n",
    "\n",
    "        if index % 10000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "            sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "            UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "            edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "            privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "    UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "    edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "    privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sun Sep 19 13:48:56 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 6.17511\n",
      "\n",
      "--Time: Sun Sep 19 13:49:11 2021 Episode: 0   Index: 10000   Loss: 0.0 --\n",
      "Reward: [-8.88804e+00  3.22600e-02  1.98000e-03] total reward: -8.8538\n",
      "UEHitrate: 0.0031  edgeHitrate 0.07219 sumHitrate 0.07529  privacy: 4.0237\n",
      "\n",
      "--Time: Sun Sep 19 13:49:25 2021 Episode: 0   Index: 20000   Loss: 0.0 --\n",
      "Reward: [-7.62881e+00  3.18600e-02  2.27000e-03] total reward: -7.59468\n",
      "UEHitrate: 0.0034  edgeHitrate 0.07145 sumHitrate 0.07485  privacy: 3.26835\n",
      "\n",
      "--Time: Sun Sep 19 13:49:40 2021 Episode: 0   Index: 30000   Loss: 0.0 --\n",
      "Reward: [-6.53934e+00  3.19000e-02  2.48000e-03] total reward: -6.50496\n",
      "UEHitrate: 0.00387  edgeHitrate 0.07133 sumHitrate 0.0752  privacy: 2.80276\n",
      "\n",
      "--Time: Sun Sep 19 13:49:54 2021 Episode: 0   Index: 40000   Loss: 0.0 --\n",
      "Reward: [-5.89595e+00  3.26200e-02  2.62000e-03] total reward: -5.86071\n",
      "UEHitrate: 0.00422  edgeHitrate 0.07302 sumHitrate 0.07725  privacy: 2.47985\n",
      "\n",
      "--Time: Sun Sep 19 13:50:09 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [-5.28836e+00  3.20400e-02  2.75000e-03] total reward: -5.25357\n",
      "UEHitrate: 0.00468  edgeHitrate 0.07192 sumHitrate 0.0766  privacy: 2.22832\n",
      "\n",
      "--Time: Sun Sep 19 13:50:24 2021 Episode: 0   Index: 60000   Loss: 0.0 --\n",
      "Reward: [-4.83211e+00  3.13800e-02  2.78000e-03] total reward: -4.79795\n",
      "UEHitrate: 0.00453  edgeHitrate 0.07035 sumHitrate 0.07488  privacy: 2.03581\n",
      "\n",
      "--Time: Sun Sep 19 13:50:39 2021 Episode: 0   Index: 70000   Loss: 0.0 --\n",
      "Reward: [-4.40712e+00  3.05700e-02  2.73000e-03] total reward: -4.37382\n",
      "UEHitrate: 0.00446  edgeHitrate 0.06858 sumHitrate 0.07304  privacy: 1.8798\n",
      "\n",
      "--Time: Sun Sep 19 13:50:53 2021 Episode: 0   Index: 80000   Loss: 0.0 --\n",
      "Reward: [-4.14417e+00  3.03900e-02  2.78000e-03] total reward: -4.111\n",
      "UEHitrate: 0.00445  edgeHitrate 0.06829 sumHitrate 0.07274  privacy: 1.73776\n",
      "\n",
      "--Time: Sun Sep 19 13:51:10 2021 Episode: 0   Index: 90000   Loss: 0.0 --\n",
      "Reward: [-3.84772e+00  3.02600e-02  2.83000e-03] total reward: -3.81463\n",
      "UEHitrate: 0.00458  edgeHitrate 0.06791 sumHitrate 0.07249  privacy: 1.61075\n",
      "\n",
      "--Time: Sun Sep 19 13:51:28 2021 Episode: 0   Index: 100000   Loss: 0.0 --\n",
      "Reward: [-3.55751e+00  2.99700e-02  2.81000e-03] total reward: -3.52474\n",
      "UEHitrate: 0.00461  edgeHitrate 0.06722 sumHitrate 0.07183  privacy: 1.48938\n",
      "\n",
      "--Time: Sun Sep 19 13:51:46 2021 Episode: 0   Index: 110000   Loss: 0.0 --\n",
      "Reward: [-3.28406e+00  2.97900e-02  2.86000e-03] total reward: -3.2514\n",
      "UEHitrate: 0.00467  edgeHitrate 0.06681 sumHitrate 0.07148  privacy: 1.3818\n",
      "\n",
      "--Time: Sun Sep 19 13:52:04 2021 Episode: 0   Index: 120000   Loss: 0.0 --\n",
      "Reward: [-2.99485e+00  2.97200e-02  2.89000e-03] total reward: -2.96224\n",
      "UEHitrate: 0.00471  edgeHitrate 0.06672 sumHitrate 0.07142  privacy: 1.28982\n",
      "\n",
      "--Time: Sun Sep 19 13:52:22 2021 Episode: 0   Index: 130000   Loss: 0.0 --\n",
      "Reward: [-2.81988  0.02953  0.00298] total reward: -2.78737\n",
      "UEHitrate: 0.00482  edgeHitrate 0.06638 sumHitrate 0.07119  privacy: 1.2066\n",
      "\n",
      "--Time: Sun Sep 19 13:52:40 2021 Episode: 0   Index: 140000   Loss: 0.0 --\n",
      "Reward: [-2.55277  0.02945  0.00305] total reward: -2.52027\n",
      "UEHitrate: 0.00491  edgeHitrate 0.06624 sumHitrate 0.07115  privacy: 1.12836\n",
      "\n",
      "--Time: Sun Sep 19 13:52:58 2021 Episode: 0   Index: 150000   Loss: 0.0 --\n",
      "Reward: [-2.42293  0.02934  0.00309] total reward: -2.3905\n",
      "UEHitrate: 0.00498  edgeHitrate 0.066 sumHitrate 0.07098  privacy: 1.05847\n",
      "\n",
      "--Time: Sun Sep 19 13:53:15 2021 Episode: 0   Index: 160000   Loss: 0.0 --\n",
      "Reward: [-2.26202  0.02922  0.00318] total reward: -2.22962\n",
      "UEHitrate: 0.00499  edgeHitrate 0.06576 sumHitrate 0.07075  privacy: 0.99188\n",
      "\n",
      "--Time: Sun Sep 19 13:53:30 2021 Episode: 0   Index: 170000   Loss: 0.0 --\n",
      "Reward: [-2.13604  0.02916  0.00319] total reward: -2.1037\n",
      "UEHitrate: 0.00509  edgeHitrate 0.06561 sumHitrate 0.07071  privacy: 0.93333\n",
      "\n",
      "--Time: Sun Sep 19 13:53:44 2021 Episode: 0   Index: 180000   Loss: 0.0 --\n",
      "Reward: [-1.97849  0.02911  0.00322] total reward: -1.94616\n",
      "UEHitrate: 0.00508  edgeHitrate 0.0655 sumHitrate 0.07058  privacy: 0.87561\n",
      "\n",
      "--Time: Sun Sep 19 13:53:59 2021 Episode: 0   Index: 190000   Loss: 0.0 --\n",
      "Reward: [-1.88835  0.02898  0.00325] total reward: -1.85612\n",
      "UEHitrate: 0.00515  edgeHitrate 0.06522 sumHitrate 0.07037  privacy: 0.82147\n",
      "\n",
      "--Time: Sun Sep 19 13:54:14 2021 Episode: 0   Index: 200000   Loss: 0.0 --\n",
      "Reward: [-1.79439  0.02904  0.00327] total reward: -1.76208\n",
      "UEHitrate: 0.00516  edgeHitrate 0.06534 sumHitrate 0.0705  privacy: 0.77476\n",
      "\n",
      "--Time: Sun Sep 19 13:54:28 2021 Episode: 0   Index: 210000   Loss: 0.0 --\n",
      "Reward: [-1.67653  0.02919  0.00331] total reward: -1.64402\n",
      "UEHitrate: 0.00524  edgeHitrate 0.06566 sumHitrate 0.0709  privacy: 0.72922\n",
      "\n",
      "--Time: Sun Sep 19 13:54:43 2021 Episode: 0   Index: 220000   Loss: 0.0 --\n",
      "Reward: [-1.58325  0.02909  0.00334] total reward: -1.55082\n",
      "UEHitrate: 0.00526  edgeHitrate 0.06545 sumHitrate 0.07072  privacy: 0.68833\n",
      "\n",
      "--Time: Sun Sep 19 13:54:58 2021 Episode: 0   Index: 230000   Loss: 0.0 --\n",
      "Reward: [-1.51317  0.02898  0.00335] total reward: -1.48084\n",
      "UEHitrate: 0.00533  edgeHitrate 0.06524 sumHitrate 0.07056  privacy: 0.64641\n",
      "\n",
      "--Time: Sun Sep 19 13:55:13 2021 Episode: 0   Index: 240000   Loss: 0.0 --\n",
      "Reward: [-1.4168   0.0289   0.00337] total reward: -1.38452\n",
      "UEHitrate: 0.00535  edgeHitrate 0.06505 sumHitrate 0.0704  privacy: 0.60719\n",
      "\n",
      "--Time: Sun Sep 19 13:55:27 2021 Episode: 0   Index: 250000   Loss: 0.0 --\n",
      "Reward: [-1.34967  0.029    0.00339] total reward: -1.31729\n",
      "UEHitrate: 0.00541  edgeHitrate 0.06524 sumHitrate 0.07066  privacy: 0.57118\n",
      "\n",
      "--Time: Sun Sep 19 13:55:42 2021 Episode: 0   Index: 260000   Loss: 0.0 --\n",
      "Reward: [-1.29565  0.02897  0.0034 ] total reward: -1.26328\n",
      "UEHitrate: 0.00545  edgeHitrate 0.0652 sumHitrate 0.07066  privacy: 0.53914\n",
      "\n",
      "--Time: Sun Sep 19 13:55:57 2021 Episode: 0   Index: 270000   Loss: 0.0 --\n",
      "Reward: [-1.23819  0.02896  0.00342] total reward: -1.20581\n",
      "UEHitrate: 0.00552  edgeHitrate 0.06515 sumHitrate 0.07067  privacy: 0.50943\n",
      "\n",
      "--Time: Sun Sep 19 13:56:12 2021 Episode: 0   Index: 280000   Loss: 0.0 --\n",
      "Reward: [-1.19975  0.02903  0.00344] total reward: -1.16728\n",
      "UEHitrate: 0.00553  edgeHitrate 0.06535 sumHitrate 0.07089  privacy: 0.48071\n",
      "\n",
      "--Time: Sun Sep 19 13:56:26 2021 Episode: 0   Index: 290000   Loss: 0.0 --\n",
      "Reward: [-1.15578  0.02909  0.00344] total reward: -1.12325\n",
      "UEHitrate: 0.00553  edgeHitrate 0.06549 sumHitrate 0.07102  privacy: 0.45233\n",
      "\n",
      "--Time: Sun Sep 19 13:56:41 2021 Episode: 0   Index: 300000   Loss: 0.0 --\n",
      "Reward: [-1.11124  0.02902  0.00342] total reward: -1.0788\n",
      "UEHitrate: 0.00554  edgeHitrate 0.06532 sumHitrate 0.07086  privacy: 0.42817\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Sep 19 13:56:42 2021 Episode: 0   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-1.10736  0.02904  0.00341] total reward: -1.07491\n",
      "UEHitrate: 0.00555  edgeHitrate 0.06535 sumHitrate 0.0709  privacy: 0.42597\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.07529, 0.07485, 0.0752 , 0.07725, 0.0766 , 0.07488,\n",
       "        0.07304, 0.07274, 0.07249, 0.07183, 0.07148, 0.07142, 0.07119,\n",
       "        0.07115, 0.07098, 0.07075, 0.07071, 0.07058, 0.07037, 0.0705 ,\n",
       "        0.0709 , 0.07072, 0.07056, 0.0704 , 0.07066, 0.07066, 0.07067,\n",
       "        0.07089, 0.07102, 0.0709 , 0.     ]),\n",
       " array([0.     , 0.0031 , 0.0034 , 0.00387, 0.00422, 0.00468, 0.00453,\n",
       "        0.00446, 0.00445, 0.00458, 0.00461, 0.00467, 0.00471, 0.00482,\n",
       "        0.00491, 0.00498, 0.00499, 0.00509, 0.00508, 0.00515, 0.00516,\n",
       "        0.00524, 0.00526, 0.00533, 0.00535, 0.00541, 0.00545, 0.00552,\n",
       "        0.00553, 0.00553, 0.00555, 0.     ]),\n",
       " array([0.     , 0.07219, 0.07145, 0.07133, 0.07302, 0.07192, 0.07035,\n",
       "        0.06858, 0.06829, 0.06791, 0.06722, 0.06681, 0.06672, 0.06638,\n",
       "        0.06624, 0.066  , 0.06576, 0.06561, 0.0655 , 0.06522, 0.06534,\n",
       "        0.06566, 0.06545, 0.06524, 0.06505, 0.06524, 0.0652 , 0.06515,\n",
       "        0.06535, 0.06549, 0.06535, 0.     ]))"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([6.17511, 4.0237 , 3.26835, 2.80276, 2.47985, 2.22832, 2.03581,\n",
       "       1.8798 , 1.73776, 1.61075, 1.48938, 1.3818 , 1.28982, 1.2066 ,\n",
       "       1.12836, 1.05847, 0.99188, 0.93333, 0.87561, 0.82147, 0.77476,\n",
       "       0.72922, 0.68833, 0.64641, 0.60719, 0.57118, 0.53914, 0.50943,\n",
       "       0.48071, 0.45233, 0.42597, 0.     ])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class UE_None(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e):\n",
    "\n",
    "        self.Rh = - self.ALPHAh * (torch.log(lastru * lastp + (1-lastru) * (1-lastp)).sum() - torch.log(ru * p + (1-ru) * (1-p)).sum())\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        self.Rl =   self.BETAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        return  self.Rh+self.Ro+self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit):\n",
    "\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.Bu,self.l_edge,self.l_cp,self.e)\n",
    "        \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.5,\"betal\":0.5}\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE_None(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Rl)\n",
    "        sumReward[2] += float(ue.Ro)\n",
    "\n",
    "        if index % 10000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "            sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "            UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "            edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Wed Sep 15 22:04:35 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:04:45 2021 Episode: 0   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.10938 0.1794  0.00074] total reward: 0.28952\n",
      "UEHitrate: 0.0018  edgeHitrate 0.39936 sumHitrate 0.40116  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:04:55 2021 Episode: 0   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.03867 0.18685 0.00087] total reward: 0.22639\n",
      "UEHitrate: 0.00195  edgeHitrate 0.41593 sumHitrate 0.41788  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:05:05 2021 Episode: 0   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.01672 0.19601 0.00083] total reward: 0.21356\n",
      "UEHitrate: 0.0024  edgeHitrate 0.43615 sumHitrate 0.43855  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:05:15 2021 Episode: 0   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.01123 0.19757 0.00101] total reward: 0.20981\n",
      "UEHitrate: 0.00247  edgeHitrate 0.43974 sumHitrate 0.44221  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:05:25 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.0048  0.19843 0.00096] total reward: 0.20419\n",
      "UEHitrate: 0.00246  edgeHitrate 0.44157 sumHitrate 0.44403  privacy: 1.0\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 15 22:05:32 2021 Episode: 0   Index: 56743   Loss: 0.0 --\n",
      "Reward: [-0.00056  0.19738  0.00101] total reward: 0.19783\n",
      "UEHitrate: 0.00252  edgeHitrate 0.43924 sumHitrate 0.44176  privacy: 1.0\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.5,\"betal\":0.5}\n",
    "\n",
    "sumHitrate = np.zeros(10)\n",
    "UEHitrate = np.zeros(10)\n",
    "edgeHitrate = np.zeros(10)\n",
    "privacyReduction = np.zeros(10)\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in UIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE_None(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Rl)\n",
    "        sumReward[2] += float(ue.Ro)\n",
    "\n",
    "        if index % 10000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "            sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "            UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "            edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "            privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "    UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "    edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "    privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Wed Sep 15 22:59:36 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:59:47 2021 Episode: 0   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.10938 0.1794  0.00074] total reward: 0.28952\n",
      "UEHitrate: 0.0018  edgeHitrate 0.39936 sumHitrate 0.40116  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:59:58 2021 Episode: 0   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.03867 0.18685 0.00087] total reward: 0.22639\n",
      "UEHitrate: 0.00195  edgeHitrate 0.41593 sumHitrate 0.41788  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:00:09 2021 Episode: 0   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.01672 0.19601 0.00083] total reward: 0.21356\n",
      "UEHitrate: 0.0024  edgeHitrate 0.43615 sumHitrate 0.43855  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:00:20 2021 Episode: 0   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.01123 0.19757 0.00101] total reward: 0.20981\n",
      "UEHitrate: 0.00247  edgeHitrate 0.43974 sumHitrate 0.44221  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:00:31 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.0048  0.19843 0.00096] total reward: 0.20419\n",
      "UEHitrate: 0.00246  edgeHitrate 0.44157 sumHitrate 0.44403  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:00:42 2021 Episode: 0   Index: 60000   Loss: 0.0 --\n",
      "Reward: [-0.00059  0.197    0.00102] total reward: 0.19742\n",
      "UEHitrate: 0.00258  edgeHitrate 0.43834 sumHitrate 0.44093  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:00:53 2021 Episode: 0   Index: 70000   Loss: 0.0 --\n",
      "Reward: [-0.00434  0.19787  0.00103] total reward: 0.19456\n",
      "UEHitrate: 0.0028  edgeHitrate 0.44025 sumHitrate 0.44305  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:01:03 2021 Episode: 0   Index: 80000   Loss: 0.0 --\n",
      "Reward: [-0.0076   0.19829  0.001  ] total reward: 0.1917\n",
      "UEHitrate: 0.00297  edgeHitrate 0.44133 sumHitrate 0.44431  privacy: 1.0\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 15 23:01:14 2021 Episode: 0   Index: 89292   Loss: 0.0 --\n",
      "Reward: [-0.00976  0.20089  0.00102] total reward: 0.19215\n",
      "UEHitrate: 0.00298  edgeHitrate 0.44711 sumHitrate 0.45009  privacy: 1.0\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.40116, 0.41788, 0.43855, 0.44221, 0.44403, 0.44093,\n",
       "        0.44305, 0.44431, 0.45009]),\n",
       " array([0.     , 0.0018 , 0.00195, 0.0024 , 0.00247, 0.00246, 0.00258,\n",
       "        0.0028 , 0.00297, 0.00298]),\n",
       " array([0.     , 0.39936, 0.41593, 0.43615, 0.43974, 0.44157, 0.43834,\n",
       "        0.44025, 0.44133, 0.44711]))"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}