{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300983, 12), (198175, 12), (253630, 12))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "#import os\n",
    "#pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "#if not os.path.exists(pathName):\n",
    "#     os.makedirs(pathName)\n",
    "#MODELPATH = pathName + 'dnn_v1.0_'\n",
    "\n",
    "data_path = '/home/zhangxz/workspace/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":0.01,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "UIT.shape,trainUIT.shape,validUIT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE_random(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "\n",
    "            actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "        \n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                action[index] = 1           \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sun Oct 17 00:16:16 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-0.66793  0.00378  0.06152] total reward: -0.60263\n",
      "UEHitrate: 0.0053  edgeHitrate 0.0769 sumHitrate 0.0822  privacy: 3.98917\n",
      "\n",
      "--Time: Sun Oct 17 00:16:42 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-0.55522  0.00441  0.0616 ] total reward: -0.48921\n",
      "UEHitrate: 0.00585  edgeHitrate 0.077 sumHitrate 0.08285  privacy: 3.35539\n",
      "\n",
      "--Time: Sun Oct 17 00:17:21 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-0.48657  0.00482  0.0596 ] total reward: -0.42215\n",
      "UEHitrate: 0.0061  edgeHitrate 0.0745 sumHitrate 0.0806  privacy: 2.94\n",
      "\n",
      "--Time: Sun Oct 17 00:18:06 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-0.43571  0.00474  0.06008] total reward: -0.37089\n",
      "UEHitrate: 0.00608  edgeHitrate 0.0751 sumHitrate 0.08118  privacy: 2.64643\n",
      "\n",
      "--Time: Sun Oct 17 00:18:49 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-0.39415  0.00502  0.05973] total reward: -0.32941\n",
      "UEHitrate: 0.00644  edgeHitrate 0.07466 sumHitrate 0.0811  privacy: 2.40865\n",
      "\n",
      "--Time: Sun Oct 17 00:19:28 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-0.36099  0.00511  0.05924] total reward: -0.29664\n",
      "UEHitrate: 0.00642  edgeHitrate 0.07405 sumHitrate 0.08047  privacy: 2.22707\n",
      "\n",
      "--Time: Sun Oct 17 00:20:04 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-0.33291  0.00495  0.05749] total reward: -0.27048\n",
      "UEHitrate: 0.00621  edgeHitrate 0.07186 sumHitrate 0.07807  privacy: 2.07864\n",
      "\n",
      "--Time: Sun Oct 17 00:20:40 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-0.30807  0.00495  0.05718] total reward: -0.24595\n",
      "UEHitrate: 0.00618  edgeHitrate 0.07147 sumHitrate 0.07765  privacy: 1.94161\n",
      "\n",
      "--Time: Sun Oct 17 00:21:24 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-0.28728  0.00507  0.05702] total reward: -0.22519\n",
      "UEHitrate: 0.00628  edgeHitrate 0.07128 sumHitrate 0.07756  privacy: 1.81692\n",
      "\n",
      "--Time: Sun Oct 17 00:22:05 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.26955  0.00499  0.05641] total reward: -0.20815\n",
      "UEHitrate: 0.0062  edgeHitrate 0.07051 sumHitrate 0.07671  privacy: 1.69633\n",
      "\n",
      "--Time: Sun Oct 17 00:22:46 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-0.25391  0.00507  0.05585] total reward: -0.19299\n",
      "UEHitrate: 0.00627  edgeHitrate 0.06982 sumHitrate 0.07609  privacy: 1.58983\n",
      "\n",
      "--Time: Sun Oct 17 00:23:28 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-0.23984  0.0051   0.05587] total reward: -0.17887\n",
      "UEHitrate: 0.00631  edgeHitrate 0.06984 sumHitrate 0.07615  privacy: 1.49978\n",
      "\n",
      "--Time: Sun Oct 17 00:24:09 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-0.2262   0.00527  0.05535] total reward: -0.16557\n",
      "UEHitrate: 0.00652  edgeHitrate 0.06919 sumHitrate 0.07572  privacy: 1.41597\n",
      "\n",
      "--Time: Sun Oct 17 00:24:50 2021   Index: 139999   Loss: 0.0 --\n",
      "Reward: [-0.21461  0.0054   0.05529] total reward: -0.15393\n",
      "UEHitrate: 0.00667  edgeHitrate 0.06911 sumHitrate 0.07578  privacy: 1.33826\n",
      "\n",
      "--Time: Sun Oct 17 00:25:30 2021   Index: 149999   Loss: 0.0 --\n",
      "Reward: [-0.20292  0.00545  0.05514] total reward: -0.14233\n",
      "UEHitrate: 0.00671  edgeHitrate 0.06892 sumHitrate 0.07563  privacy: 1.26797\n",
      "\n",
      "--Time: Sun Oct 17 00:26:10 2021   Index: 159999   Loss: 0.0 --\n",
      "Reward: [-0.19247  0.00564  0.0549 ] total reward: -0.13193\n",
      "UEHitrate: 0.00689  edgeHitrate 0.06863 sumHitrate 0.07551  privacy: 1.20028\n",
      "\n",
      "--Time: Sun Oct 17 00:26:50 2021   Index: 169999   Loss: 0.0 --\n",
      "Reward: [-0.18247  0.0057   0.05472] total reward: -0.12206\n",
      "UEHitrate: 0.00695  edgeHitrate 0.0684 sumHitrate 0.07535  privacy: 1.1399\n",
      "\n",
      "--Time: Sun Oct 17 00:27:34 2021   Index: 179999   Loss: 0.0 --\n",
      "Reward: [-0.17354  0.00577  0.05491] total reward: -0.11286\n",
      "UEHitrate: 0.00706  edgeHitrate 0.06864 sumHitrate 0.07569  privacy: 1.08002\n",
      "\n",
      "--Time: Sun Oct 17 00:28:12 2021   Index: 189999   Loss: 0.0 --\n",
      "Reward: [-0.16473  0.00582  0.05475] total reward: -0.10416\n",
      "UEHitrate: 0.00708  edgeHitrate 0.06844 sumHitrate 0.07552  privacy: 1.02384\n",
      "\n",
      "--Time: Sun Oct 17 00:28:49 2021   Index: 199999   Loss: 0.0 --\n",
      "Reward: [-0.15653  0.00587  0.05479] total reward: -0.09587\n",
      "UEHitrate: 0.00712  edgeHitrate 0.06849 sumHitrate 0.07561  privacy: 0.97439\n",
      "\n",
      "--Time: Sun Oct 17 00:29:33 2021   Index: 209999   Loss: 0.0 --\n",
      "Reward: [-0.14906  0.00595  0.05504] total reward: -0.08807\n",
      "UEHitrate: 0.0072  edgeHitrate 0.0688 sumHitrate 0.076  privacy: 0.92599\n",
      "\n",
      "--Time: Sun Oct 17 00:30:12 2021   Index: 219999   Loss: 0.0 --\n",
      "Reward: [-0.14192  0.00602  0.05488] total reward: -0.08102\n",
      "UEHitrate: 0.00726  edgeHitrate 0.0686 sumHitrate 0.07587  privacy: 0.88191\n",
      "\n",
      "--Time: Sun Oct 17 00:30:53 2021   Index: 229999   Loss: 0.0 --\n",
      "Reward: [-0.13498  0.00603  0.0547 ] total reward: -0.07425\n",
      "UEHitrate: 0.00726  edgeHitrate 0.06837 sumHitrate 0.07563  privacy: 0.83733\n",
      "\n",
      "--Time: Sun Oct 17 00:31:33 2021   Index: 239999   Loss: 0.0 --\n",
      "Reward: [-0.12867  0.00604  0.05458] total reward: -0.06805\n",
      "UEHitrate: 0.00727  edgeHitrate 0.06822 sumHitrate 0.0755  privacy: 0.7954\n",
      "\n",
      "--Time: Sun Oct 17 00:32:11 2021   Index: 249999   Loss: 0.0 --\n",
      "Reward: [-0.12251  0.00608  0.05483] total reward: -0.0616\n",
      "UEHitrate: 0.00732  edgeHitrate 0.06854 sumHitrate 0.07586  privacy: 0.75647\n",
      "\n",
      "--Time: Sun Oct 17 00:32:50 2021   Index: 259999   Loss: 0.0 --\n",
      "Reward: [-0.11667  0.0061   0.05482] total reward: -0.05575\n",
      "UEHitrate: 0.00735  edgeHitrate 0.06852 sumHitrate 0.07587  privacy: 0.72131\n",
      "\n",
      "--Time: Sun Oct 17 00:33:30 2021   Index: 269999   Loss: 0.0 --\n",
      "Reward: [-0.11121  0.00616  0.05468] total reward: -0.05038\n",
      "UEHitrate: 0.00744  edgeHitrate 0.06834 sumHitrate 0.07579  privacy: 0.68817\n",
      "\n",
      "--Time: Sun Oct 17 00:34:09 2021   Index: 279999   Loss: 0.0 --\n",
      "Reward: [-0.10586  0.00619  0.05485] total reward: -0.04482\n",
      "UEHitrate: 0.0075  edgeHitrate 0.06856 sumHitrate 0.07606  privacy: 0.6563\n",
      "\n",
      "--Time: Sun Oct 17 00:34:52 2021   Index: 289999   Loss: 0.0 --\n",
      "Reward: [-0.10081  0.0062   0.05502] total reward: -0.03959\n",
      "UEHitrate: 0.00751  edgeHitrate 0.06877 sumHitrate 0.07628  privacy: 0.62479\n",
      "\n",
      "--Time: Sun Oct 17 00:35:28 2021   Index: 299999   Loss: 0.0 --\n",
      "Reward: [-0.09616  0.00619  0.05511] total reward: -0.03485\n",
      "UEHitrate: 0.0075  edgeHitrate 0.06889 sumHitrate 0.07639  privacy: 0.597\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 17 00:35:33 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-0.09571  0.00618  0.05513] total reward: -0.0344\n",
      "UEHitrate: 0.00749  edgeHitrate 0.06891 sumHitrate 0.0764  privacy: 0.59438\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE_random(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0822 , 0.08285, 0.0806 , 0.08118, 0.0811 , 0.08047, 0.07807,\n",
       "        0.07765, 0.07756, 0.07671, 0.07609, 0.07615, 0.07572, 0.07578,\n",
       "        0.07563, 0.07551, 0.07535, 0.07569, 0.07552, 0.07561, 0.076  ,\n",
       "        0.07587, 0.07563, 0.0755 , 0.07586, 0.07587, 0.07579, 0.07606,\n",
       "        0.07628, 0.07639, 0.0764 ]),\n",
       " array([0.0053 , 0.00585, 0.0061 , 0.00608, 0.00644, 0.00642, 0.00621,\n",
       "        0.00618, 0.00628, 0.0062 , 0.00627, 0.00631, 0.00652, 0.00667,\n",
       "        0.00671, 0.00689, 0.00695, 0.00706, 0.00708, 0.00712, 0.0072 ,\n",
       "        0.00726, 0.00726, 0.00727, 0.00732, 0.00735, 0.00744, 0.0075 ,\n",
       "        0.00751, 0.0075 , 0.00749]),\n",
       " array([0.0769 , 0.077  , 0.0745 , 0.0751 , 0.07466, 0.07405, 0.07186,\n",
       "        0.07147, 0.07128, 0.07051, 0.06982, 0.06984, 0.06919, 0.06911,\n",
       "        0.06892, 0.06863, 0.0684 , 0.06864, 0.06844, 0.06849, 0.0688 ,\n",
       "        0.0686 , 0.06837, 0.06822, 0.06854, 0.06852, 0.06834, 0.06856,\n",
       "        0.06877, 0.06889, 0.06891]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.98917, 3.35539, 2.94   , 2.64643, 2.40865, 2.22707, 2.07864,\n",
       "       1.94161, 1.81692, 1.69633, 1.58983, 1.49978, 1.41597, 1.33826,\n",
       "       1.26797, 1.20028, 1.1399 , 1.08002, 1.02384, 0.97439, 0.92599,\n",
       "       0.88191, 0.83733, 0.7954 , 0.75647, 0.72131, 0.68817, 0.6563 ,\n",
       "       0.62479, 0.597  , 0.59438])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UE_None(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1      \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sun Oct 17 00:35:57 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-0.04246  0.00184  0.30576] total reward: 0.26514\n",
      "UEHitrate: 0.0028  edgeHitrate 0.3822 sumHitrate 0.385  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:36:20 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-0.03706  0.00252  0.33908] total reward: 0.30454\n",
      "UEHitrate: 0.0036  edgeHitrate 0.42385 sumHitrate 0.42745  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:36:44 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-0.03487  0.00289  0.32576] total reward: 0.29378\n",
      "UEHitrate: 0.00403  edgeHitrate 0.4072 sumHitrate 0.41123  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:37:09 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-0.03305  0.00298  0.31552] total reward: 0.28545\n",
      "UEHitrate: 0.00407  edgeHitrate 0.3944 sumHitrate 0.39848  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:37:30 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-0.03114  0.0032   0.3135 ] total reward: 0.28556\n",
      "UEHitrate: 0.0044  edgeHitrate 0.39188 sumHitrate 0.39628  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:37:57 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-0.02985  0.00326  0.31128] total reward: 0.28469\n",
      "UEHitrate: 0.00443  edgeHitrate 0.3891 sumHitrate 0.39353  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:38:24 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-0.02869  0.00311  0.30534] total reward: 0.27976\n",
      "UEHitrate: 0.00424  edgeHitrate 0.38167 sumHitrate 0.38591  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:38:53 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-0.02731  0.00312  0.30831] total reward: 0.28412\n",
      "UEHitrate: 0.00426  edgeHitrate 0.38539 sumHitrate 0.38965  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:39:21 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-0.02625  0.00321  0.30635] total reward: 0.28331\n",
      "UEHitrate: 0.00432  edgeHitrate 0.38293 sumHitrate 0.38726  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:39:41 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.02543  0.00317  0.30606] total reward: 0.2838\n",
      "UEHitrate: 0.00428  edgeHitrate 0.38257 sumHitrate 0.38685  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:39:52 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-0.0246   0.00323  0.30633] total reward: 0.28496\n",
      "UEHitrate: 0.00443  edgeHitrate 0.38291 sumHitrate 0.38734  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:40:16 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-0.02393  0.00329  0.30595] total reward: 0.28531\n",
      "UEHitrate: 0.00449  edgeHitrate 0.38244 sumHitrate 0.38693  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:40:40 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-0.02325  0.00346  0.30274] total reward: 0.28295\n",
      "UEHitrate: 0.0047  edgeHitrate 0.37842 sumHitrate 0.38312  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:41:02 2021   Index: 139999   Loss: 0.0 --\n",
      "Reward: [-0.02265  0.0035   0.30588] total reward: 0.28673\n",
      "UEHitrate: 0.00483  edgeHitrate 0.38235 sumHitrate 0.38718  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:41:26 2021   Index: 149999   Loss: 0.0 --\n",
      "Reward: [-0.02205  0.00354  0.30925] total reward: 0.29073\n",
      "UEHitrate: 0.00489  edgeHitrate 0.38656 sumHitrate 0.39145  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:41:51 2021   Index: 159999   Loss: 0.0 --\n",
      "Reward: [-0.02155  0.00368  0.30703] total reward: 0.28915\n",
      "UEHitrate: 0.00507  edgeHitrate 0.38378 sumHitrate 0.38885  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:42:14 2021   Index: 169999   Loss: 0.0 --\n",
      "Reward: [-0.02103  0.00371  0.30954] total reward: 0.29222\n",
      "UEHitrate: 0.00511  edgeHitrate 0.38692 sumHitrate 0.39204  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:42:36 2021   Index: 179999   Loss: 0.0 --\n",
      "Reward: [-0.02052  0.00377  0.31143] total reward: 0.29468\n",
      "UEHitrate: 0.0052  edgeHitrate 0.38929 sumHitrate 0.39449  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:43:02 2021   Index: 189999   Loss: 0.0 --\n",
      "Reward: [-0.02001  0.0038   0.3128 ] total reward: 0.29659\n",
      "UEHitrate: 0.00523  edgeHitrate 0.391 sumHitrate 0.39623  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:43:27 2021   Index: 199999   Loss: 0.0 --\n",
      "Reward: [-0.01955  0.0038   0.31313] total reward: 0.29738\n",
      "UEHitrate: 0.00526  edgeHitrate 0.39142 sumHitrate 0.39668  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:43:49 2021   Index: 209999   Loss: 0.0 --\n",
      "Reward: [-0.01917  0.00385  0.31442] total reward: 0.2991\n",
      "UEHitrate: 0.00536  edgeHitrate 0.39303 sumHitrate 0.39839  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:44:11 2021   Index: 219999   Loss: 0.0 --\n",
      "Reward: [-0.01879  0.00389  0.31297] total reward: 0.29808\n",
      "UEHitrate: 0.00542  edgeHitrate 0.39122 sumHitrate 0.39664  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:44:36 2021   Index: 229999   Loss: 0.0 --\n",
      "Reward: [-0.01849  0.00392  0.31191] total reward: 0.29735\n",
      "UEHitrate: 0.00545  edgeHitrate 0.38989 sumHitrate 0.39534  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:45:00 2021   Index: 239999   Loss: 0.0 --\n",
      "Reward: [-0.01816  0.00395  0.31089] total reward: 0.29668\n",
      "UEHitrate: 0.00549  edgeHitrate 0.38862 sumHitrate 0.39411  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:45:25 2021   Index: 249999   Loss: 0.0 --\n",
      "Reward: [-0.01783  0.00395  0.31124] total reward: 0.29735\n",
      "UEHitrate: 0.00552  edgeHitrate 0.38904 sumHitrate 0.39457  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:45:52 2021   Index: 259999   Loss: 0.0 --\n",
      "Reward: [-0.01752  0.00394  0.30961] total reward: 0.29603\n",
      "UEHitrate: 0.00553  edgeHitrate 0.38702 sumHitrate 0.39255  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:46:19 2021   Index: 269999   Loss: 0.0 --\n",
      "Reward: [-0.01725  0.004    0.30794] total reward: 0.29468\n",
      "UEHitrate: 0.0056  edgeHitrate 0.38492 sumHitrate 0.39052  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:46:44 2021   Index: 279999   Loss: 0.0 --\n",
      "Reward: [-0.01698  0.00402  0.3079 ] total reward: 0.29493\n",
      "UEHitrate: 0.00564  edgeHitrate 0.38488 sumHitrate 0.39051  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:47:09 2021   Index: 289999   Loss: 0.0 --\n",
      "Reward: [-0.01669  0.00402  0.30972] total reward: 0.29705\n",
      "UEHitrate: 0.00565  edgeHitrate 0.38716 sumHitrate 0.39281  privacy: 1.0\n",
      "\n",
      "--Time: Sun Oct 17 00:47:33 2021   Index: 299999   Loss: 0.0 --\n",
      "Reward: [-0.01636  0.004    0.31242] total reward: 0.30006\n",
      "UEHitrate: 0.00564  edgeHitrate 0.39053 sumHitrate 0.39617  privacy: 1.0\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 17 00:47:36 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-0.01633  0.004    0.31267] total reward: 0.30034\n",
      "UEHitrate: 0.00563  edgeHitrate 0.39084 sumHitrate 0.39647  privacy: 1.0\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE_None(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.385  , 0.42745, 0.41123, 0.39848, 0.39628, 0.39353, 0.38591,\n",
       "        0.38965, 0.38726, 0.38685, 0.38734, 0.38693, 0.38312, 0.38718,\n",
       "        0.39145, 0.38885, 0.39204, 0.39449, 0.39623, 0.39668, 0.39839,\n",
       "        0.39664, 0.39534, 0.39411, 0.39457, 0.39255, 0.39052, 0.39051,\n",
       "        0.39281, 0.39617, 0.39647]),\n",
       " array([0.0028 , 0.0036 , 0.00403, 0.00407, 0.0044 , 0.00443, 0.00424,\n",
       "        0.00426, 0.00432, 0.00428, 0.00443, 0.00449, 0.0047 , 0.00483,\n",
       "        0.00489, 0.00507, 0.00511, 0.0052 , 0.00523, 0.00526, 0.00536,\n",
       "        0.00542, 0.00545, 0.00549, 0.00552, 0.00553, 0.0056 , 0.00564,\n",
       "        0.00565, 0.00564, 0.00563]),\n",
       " array([0.3822 , 0.42385, 0.4072 , 0.3944 , 0.39188, 0.3891 , 0.38167,\n",
       "        0.38539, 0.38293, 0.38257, 0.38291, 0.38244, 0.37842, 0.38235,\n",
       "        0.38656, 0.38378, 0.38692, 0.38929, 0.391  , 0.39142, 0.39303,\n",
       "        0.39122, 0.38989, 0.38862, 0.38904, 0.38702, 0.38492, 0.38488,\n",
       "        0.38716, 0.39053, 0.39084]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
