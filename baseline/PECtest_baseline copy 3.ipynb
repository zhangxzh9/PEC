{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>1030</td>\n",
       "      <td>101001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11807</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15068</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>1035</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5375</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1030</td>\n",
       "      <td>10202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5992</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3468</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148        1030        101001     0   \n",
       "1       203  5779    0        0         7        1030         10203     0   \n",
       "2       208  4675    0        0        92        1035         10203     0   \n",
       "3       159   332    0        0        56        1030         10202     0   \n",
       "4        50   674    0        0       439        1030         10203     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831   29  2591880        34        1030         10203     0   \n",
       "300979  158  8448   29  2591880        34        1030         10203     0   \n",
       "300980  483  6463   29  2591940        35        1030         10203     0   \n",
       "300981  158  4715   29  2591940        34        1030         10203     0   \n",
       "300982  483  2021   29  2591940        34        1030         10203     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0      11807          1            2  \n",
       "1              0      15068          1            2  \n",
       "2              0       5375          1            2  \n",
       "3              0       5992          1            2  \n",
       "4              0       3468          1            2  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0      10010          1            2  \n",
       "300979         0      23340          1            2  \n",
       "300980         0      10010          1            2  \n",
       "300981         0      23340          1            2  \n",
       "300982         0      10010          1            2  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000,\n",
       " 500,\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148        1030        101001     0   \n",
       " 1       203  5779    0        0         7        1030         10203     0   \n",
       " 2       208  4675    0        0        92        1035         10203     0   \n",
       " 3       159   332    0        0        56        1030         10202     0   \n",
       " 4        50   674    0        0       439        1030         10203     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 198170  264  7442   17  1555140        90        1035         10203     0   \n",
       " 198171   19  9362   17  1555140       424        1035         10203     0   \n",
       " 198172   82  9223   17  1555140        94        1037         10203     0   \n",
       " 198173   35  4164   17  1555140        22        1030         10203     0   \n",
       " 198174  239  5062   17  1555140        89        1035         10203     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0      11807          1            2  \n",
       " 1              0      15068          1            2  \n",
       " 2              0       5375          1            2  \n",
       " 3              0       5992          1            2  \n",
       " 4              0       3468          1            2  \n",
       " ...          ...        ...        ...          ...  \n",
       " 198170         0       7592          1            2  \n",
       " 198171         0       5938          1            2  \n",
       " 198172         0      11393          1            2  \n",
       " 198173         0       5866          1            2  \n",
       " 198174         0      23746          1            2  \n",
       " \n",
       " [198175 rows x 12 columns])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=20,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p) , \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                self.l_edge,\n",
    "                self.l_cp)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class UE_random(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e):\n",
    "\n",
    "        self.Rh = - self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)) - torch.log(lastru * lastp + (1-lastru) * (1-lastp))).sum()\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        self.Rl =   self.BETAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        return  self.Rh+self.Ro+self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit):\n",
    "\n",
    "\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.Bu,self.l_edge,self.l_cp,self.e)\n",
    "        \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "\n",
    "        actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "        \n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.5,\"betal\":0.5}\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "UEHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "edgeHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "privacyReduction = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in UIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE_random(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Rl)\n",
    "        sumReward[2] += float(ue.Ro)\n",
    "\n",
    "        if index % 10000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "            sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "            UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "            edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "            privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "    UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "    edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "    privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sun Sep 19 13:48:30 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 5.09857\n",
      "\n",
      "--Time: Sun Sep 19 13:48:44 2021 Episode: 0   Index: 10000   Loss: 0.0 --\n",
      "Reward: [-7.13018e+00  3.19500e-02  1.57000e-03] total reward: -7.09666\n",
      "UEHitrate: 0.0028  edgeHitrate 0.07189 sumHitrate 0.07469  privacy: 3.99156\n",
      "\n",
      "--Time: Sun Sep 19 13:48:58 2021 Episode: 0   Index: 20000   Loss: 0.0 --\n",
      "Reward: [-6.11638e+00  3.33400e-02  2.08000e-03] total reward: -6.08095\n",
      "UEHitrate: 0.00285  edgeHitrate 0.075 sumHitrate 0.07785  privacy: 3.3545\n",
      "\n",
      "--Time: Sun Sep 19 13:49:13 2021 Episode: 0   Index: 30000   Loss: 0.0 --\n",
      "Reward: [-5.23061e+00  3.25800e-02  2.30000e-03] total reward: -5.19573\n",
      "UEHitrate: 0.00353  edgeHitrate 0.0732 sumHitrate 0.07673  privacy: 2.93786\n",
      "\n",
      "--Time: Sun Sep 19 13:49:27 2021 Episode: 0   Index: 40000   Loss: 0.0 --\n",
      "Reward: [-4.71725e+00  3.35700e-02  2.33000e-03] total reward: -4.68135\n",
      "UEHitrate: 0.00377  edgeHitrate 0.07525 sumHitrate 0.07902  privacy: 2.64333\n",
      "\n",
      "--Time: Sun Sep 19 13:49:41 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [-4.24575e+00  3.34300e-02  2.50000e-03] total reward: -4.20982\n",
      "UEHitrate: 0.0042  edgeHitrate 0.07502 sumHitrate 0.07922  privacy: 2.40441\n",
      "\n",
      "--Time: Sun Sep 19 13:49:56 2021 Episode: 0   Index: 60000   Loss: 0.0 --\n",
      "Reward: [-3.88496e+00  3.30300e-02  2.54000e-03] total reward: -3.84939\n",
      "UEHitrate: 0.00423  edgeHitrate 0.07407 sumHitrate 0.0783  privacy: 2.22247\n",
      "\n",
      "--Time: Sun Sep 19 13:50:10 2021 Episode: 0   Index: 70000   Loss: 0.0 --\n",
      "Reward: [-3.55028e+00  3.22300e-02  2.50000e-03] total reward: -3.51555\n",
      "UEHitrate: 0.00416  edgeHitrate 0.07224 sumHitrate 0.0764  privacy: 2.07406\n",
      "\n",
      "--Time: Sun Sep 19 13:50:25 2021 Episode: 0   Index: 80000   Loss: 0.0 --\n",
      "Reward: [-3.34493e+00  3.20000e-02  2.54000e-03] total reward: -3.31038\n",
      "UEHitrate: 0.00404  edgeHitrate 0.07172 sumHitrate 0.07576  privacy: 1.93634\n",
      "\n",
      "--Time: Sun Sep 19 13:50:39 2021 Episode: 0   Index: 90000   Loss: 0.0 --\n",
      "Reward: [-3.10434e+00  3.17000e-02  2.58000e-03] total reward: -3.07006\n",
      "UEHitrate: 0.00423  edgeHitrate 0.07102 sumHitrate 0.07525  privacy: 1.81152\n",
      "\n",
      "--Time: Sun Sep 19 13:50:53 2021 Episode: 0   Index: 100000   Loss: 0.0 --\n",
      "Reward: [-2.86664e+00  3.14900e-02  2.55000e-03] total reward: -2.8326\n",
      "UEHitrate: 0.00427  edgeHitrate 0.07053 sumHitrate 0.0748  privacy: 1.69079\n",
      "\n",
      "--Time: Sun Sep 19 13:51:07 2021 Episode: 0   Index: 110000   Loss: 0.0 --\n",
      "Reward: [-2.64434e+00  3.13300e-02  2.62000e-03] total reward: -2.61038\n",
      "UEHitrate: 0.00432  edgeHitrate 0.07017 sumHitrate 0.07449  privacy: 1.58405\n",
      "\n",
      "--Time: Sun Sep 19 13:51:21 2021 Episode: 0   Index: 120000   Loss: 0.0 --\n",
      "Reward: [-2.41064  0.03112  0.00265] total reward: -2.37687\n",
      "UEHitrate: 0.00437  edgeHitrate 0.0697 sumHitrate 0.07407  privacy: 1.4938\n",
      "\n",
      "--Time: Sun Sep 19 13:51:35 2021 Episode: 0   Index: 130000   Loss: 0.0 --\n",
      "Reward: [-2.2667   0.03087  0.00275] total reward: -2.23308\n",
      "UEHitrate: 0.00462  edgeHitrate 0.06916 sumHitrate 0.07378  privacy: 1.41021\n",
      "\n",
      "--Time: Sun Sep 19 13:51:49 2021 Episode: 0   Index: 140000   Loss: 0.0 --\n",
      "Reward: [-2.04905  0.03084  0.0028 ] total reward: -2.01541\n",
      "UEHitrate: 0.00469  edgeHitrate 0.06912 sumHitrate 0.07381  privacy: 1.33243\n",
      "\n",
      "--Time: Sun Sep 19 13:52:03 2021 Episode: 0   Index: 150000   Loss: 0.0 --\n",
      "Reward: [-1.94307  0.03078  0.00283] total reward: -1.90947\n",
      "UEHitrate: 0.00477  edgeHitrate 0.06895 sumHitrate 0.07373  privacy: 1.26211\n",
      "\n",
      "--Time: Sun Sep 19 13:52:17 2021 Episode: 0   Index: 160000   Loss: 0.0 --\n",
      "Reward: [-1.8113   0.03069  0.00293] total reward: -1.77767\n",
      "UEHitrate: 0.00479  edgeHitrate 0.06877 sumHitrate 0.07356  privacy: 1.19427\n",
      "\n",
      "--Time: Sun Sep 19 13:52:31 2021 Episode: 0   Index: 170000   Loss: 0.0 --\n",
      "Reward: [-1.71042  0.03067  0.00296] total reward: -1.67679\n",
      "UEHitrate: 0.00482  edgeHitrate 0.06869 sumHitrate 0.07351  privacy: 1.13368\n",
      "\n",
      "--Time: Sun Sep 19 13:52:45 2021 Episode: 0   Index: 180000   Loss: 0.0 --\n",
      "Reward: [-1.58222  0.03073  0.003  ] total reward: -1.54848\n",
      "UEHitrate: 0.00481  edgeHitrate 0.06882 sumHitrate 0.07362  privacy: 1.07359\n",
      "\n",
      "--Time: Sun Sep 19 13:53:00 2021 Episode: 0   Index: 190000   Loss: 0.0 --\n",
      "Reward: [-1.50752  0.03055  0.00301] total reward: -1.47396\n",
      "UEHitrate: 0.00489  edgeHitrate 0.0684 sumHitrate 0.07329  privacy: 1.01727\n",
      "\n",
      "--Time: Sun Sep 19 13:53:14 2021 Episode: 0   Index: 200000   Loss: 0.0 --\n",
      "Reward: [-1.43227  0.03073  0.00303] total reward: -1.39851\n",
      "UEHitrate: 0.00488  edgeHitrate 0.0688 sumHitrate 0.07369  privacy: 0.96808\n",
      "\n",
      "--Time: Sun Sep 19 13:53:28 2021 Episode: 0   Index: 210000   Loss: 0.0 --\n",
      "Reward: [-1.33592  0.03091  0.00308] total reward: -1.30193\n",
      "UEHitrate: 0.00491  edgeHitrate 0.06921 sumHitrate 0.07412  privacy: 0.91942\n",
      "\n",
      "--Time: Sun Sep 19 13:53:42 2021 Episode: 0   Index: 220000   Loss: 0.0 --\n",
      "Reward: [-1.25965  0.03085  0.00312] total reward: -1.22567\n",
      "UEHitrate: 0.00493  edgeHitrate 0.06911 sumHitrate 0.07405  privacy: 0.87542\n",
      "\n",
      "--Time: Sun Sep 19 13:53:56 2021 Episode: 0   Index: 230000   Loss: 0.0 --\n",
      "Reward: [-1.20255  0.03071  0.00313] total reward: -1.16872\n",
      "UEHitrate: 0.00497  edgeHitrate 0.06874 sumHitrate 0.07372  privacy: 0.83073\n",
      "\n",
      "--Time: Sun Sep 19 13:54:10 2021 Episode: 0   Index: 240000   Loss: 0.0 --\n",
      "Reward: [-1.12344  0.03064  0.00316] total reward: -1.08964\n",
      "UEHitrate: 0.00499  edgeHitrate 0.06858 sumHitrate 0.07357  privacy: 0.789\n",
      "\n",
      "--Time: Sun Sep 19 13:54:24 2021 Episode: 0   Index: 250000   Loss: 0.0 --\n",
      "Reward: [-1.0689   0.03079  0.00318] total reward: -1.03494\n",
      "UEHitrate: 0.00503  edgeHitrate 0.06893 sumHitrate 0.07396  privacy: 0.75008\n",
      "\n",
      "--Time: Sun Sep 19 13:54:38 2021 Episode: 0   Index: 260000   Loss: 0.0 --\n",
      "Reward: [-1.02673  0.03075  0.00318] total reward: -0.9928\n",
      "UEHitrate: 0.0051  edgeHitrate 0.0688 sumHitrate 0.0739  privacy: 0.71498\n",
      "\n",
      "--Time: Sun Sep 19 13:54:52 2021 Episode: 0   Index: 270000   Loss: 0.0 --\n",
      "Reward: [-0.97942  0.03072  0.0032 ] total reward: -0.9455\n",
      "UEHitrate: 0.00515  edgeHitrate 0.06876 sumHitrate 0.07391  privacy: 0.68174\n",
      "\n",
      "--Time: Sun Sep 19 13:55:06 2021 Episode: 0   Index: 280000   Loss: 0.0 --\n",
      "Reward: [-0.95078  0.03083  0.00321] total reward: -0.91673\n",
      "UEHitrate: 0.00518  edgeHitrate 0.06904 sumHitrate 0.07422  privacy: 0.64993\n",
      "\n",
      "--Time: Sun Sep 19 13:55:20 2021 Episode: 0   Index: 290000   Loss: 0.0 --\n",
      "Reward: [-0.9164   0.03088  0.00322] total reward: -0.8823\n",
      "UEHitrate: 0.00518  edgeHitrate 0.06913 sumHitrate 0.07431  privacy: 0.61848\n",
      "\n",
      "--Time: Sun Sep 19 13:55:34 2021 Episode: 0   Index: 300000   Loss: 0.0 --\n",
      "Reward: [-0.88013  0.03086  0.00321] total reward: -0.84606\n",
      "UEHitrate: 0.00518  edgeHitrate 0.06911 sumHitrate 0.07429  privacy: 0.59078\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Sep 19 13:55:36 2021 Episode: 0   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-0.87701  0.03088  0.00321] total reward: -0.84292\n",
      "UEHitrate: 0.00519  edgeHitrate 0.06915 sumHitrate 0.07433  privacy: 0.58822\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.07469, 0.07785, 0.07673, 0.07902, 0.07922, 0.0783 ,\n",
       "        0.0764 , 0.07576, 0.07525, 0.0748 , 0.07449, 0.07407, 0.07378,\n",
       "        0.07381, 0.07373, 0.07356, 0.07351, 0.07362, 0.07329, 0.07369,\n",
       "        0.07412, 0.07405, 0.07372, 0.07357, 0.07396, 0.0739 , 0.07391,\n",
       "        0.07422, 0.07431, 0.07433, 0.     ]),\n",
       " array([0.     , 0.0028 , 0.00285, 0.00353, 0.00377, 0.0042 , 0.00423,\n",
       "        0.00416, 0.00404, 0.00423, 0.00427, 0.00432, 0.00437, 0.00462,\n",
       "        0.00469, 0.00477, 0.00479, 0.00482, 0.00481, 0.00489, 0.00488,\n",
       "        0.00491, 0.00493, 0.00497, 0.00499, 0.00503, 0.0051 , 0.00515,\n",
       "        0.00518, 0.00518, 0.00519, 0.     ]),\n",
       " array([0.     , 0.07189, 0.075  , 0.0732 , 0.07525, 0.07502, 0.07407,\n",
       "        0.07224, 0.07172, 0.07102, 0.07053, 0.07017, 0.0697 , 0.06916,\n",
       "        0.06912, 0.06895, 0.06877, 0.06869, 0.06882, 0.0684 , 0.0688 ,\n",
       "        0.06921, 0.06911, 0.06874, 0.06858, 0.06893, 0.0688 , 0.06876,\n",
       "        0.06904, 0.06913, 0.06915, 0.     ]))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([5.09857, 3.99156, 3.3545 , 2.93786, 2.64333, 2.40441, 2.22247,\n",
       "       2.07406, 1.93634, 1.81152, 1.69079, 1.58405, 1.4938 , 1.41021,\n",
       "       1.33243, 1.26211, 1.19427, 1.13368, 1.07359, 1.01727, 0.96808,\n",
       "       0.91942, 0.87542, 0.83073, 0.789  , 0.75008, 0.71498, 0.68174,\n",
       "       0.64993, 0.61848, 0.58822, 0.     ])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class UE_None(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e):\n",
    "\n",
    "        self.Rh = - self.ALPHAh * (torch.log(lastru * lastp + (1-lastru) * (1-lastp)).sum() - torch.log(ru * p + (1-ru) * (1-p)).sum())\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        self.Rl =   self.BETAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        return  self.Rh+self.Ro+self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit):\n",
    "\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.Bu,self.l_edge,self.l_cp,self.e)\n",
    "        \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.5,\"betal\":0.5}\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in trainUIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE_None(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Rl)\n",
    "        sumReward[2] += float(ue.Ro)\n",
    "\n",
    "        if index % 10000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "            sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "            UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "            edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Wed Sep 15 22:04:35 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:04:45 2021 Episode: 0   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.10938 0.1794  0.00074] total reward: 0.28952\n",
      "UEHitrate: 0.0018  edgeHitrate 0.39936 sumHitrate 0.40116  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:04:55 2021 Episode: 0   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.03867 0.18685 0.00087] total reward: 0.22639\n",
      "UEHitrate: 0.00195  edgeHitrate 0.41593 sumHitrate 0.41788  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:05:05 2021 Episode: 0   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.01672 0.19601 0.00083] total reward: 0.21356\n",
      "UEHitrate: 0.0024  edgeHitrate 0.43615 sumHitrate 0.43855  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:05:15 2021 Episode: 0   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.01123 0.19757 0.00101] total reward: 0.20981\n",
      "UEHitrate: 0.00247  edgeHitrate 0.43974 sumHitrate 0.44221  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:05:25 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.0048  0.19843 0.00096] total reward: 0.20419\n",
      "UEHitrate: 0.00246  edgeHitrate 0.44157 sumHitrate 0.44403  privacy: 1.0\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 15 22:05:32 2021 Episode: 0   Index: 56743   Loss: 0.0 --\n",
      "Reward: [-0.00056  0.19738  0.00101] total reward: 0.19783\n",
      "UEHitrate: 0.00252  edgeHitrate 0.43924 sumHitrate 0.44176  privacy: 1.0\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.5,\"betal\":0.5}\n",
    "\n",
    "sumHitrate = np.zeros(10)\n",
    "UEHitrate = np.zeros(10)\n",
    "edgeHitrate = np.zeros(10)\n",
    "privacyReduction = np.zeros(10)\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in UIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE_None(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Rl)\n",
    "        sumReward[2] += float(ue.Ro)\n",
    "\n",
    "        if index % 10000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "            sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "            UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "            edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "            privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "    UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "    edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "    privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Wed Sep 15 22:59:36 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:59:47 2021 Episode: 0   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.10938 0.1794  0.00074] total reward: 0.28952\n",
      "UEHitrate: 0.0018  edgeHitrate 0.39936 sumHitrate 0.40116  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 22:59:58 2021 Episode: 0   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.03867 0.18685 0.00087] total reward: 0.22639\n",
      "UEHitrate: 0.00195  edgeHitrate 0.41593 sumHitrate 0.41788  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:00:09 2021 Episode: 0   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.01672 0.19601 0.00083] total reward: 0.21356\n",
      "UEHitrate: 0.0024  edgeHitrate 0.43615 sumHitrate 0.43855  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:00:20 2021 Episode: 0   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.01123 0.19757 0.00101] total reward: 0.20981\n",
      "UEHitrate: 0.00247  edgeHitrate 0.43974 sumHitrate 0.44221  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:00:31 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.0048  0.19843 0.00096] total reward: 0.20419\n",
      "UEHitrate: 0.00246  edgeHitrate 0.44157 sumHitrate 0.44403  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:00:42 2021 Episode: 0   Index: 60000   Loss: 0.0 --\n",
      "Reward: [-0.00059  0.197    0.00102] total reward: 0.19742\n",
      "UEHitrate: 0.00258  edgeHitrate 0.43834 sumHitrate 0.44093  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:00:53 2021 Episode: 0   Index: 70000   Loss: 0.0 --\n",
      "Reward: [-0.00434  0.19787  0.00103] total reward: 0.19456\n",
      "UEHitrate: 0.0028  edgeHitrate 0.44025 sumHitrate 0.44305  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 15 23:01:03 2021 Episode: 0   Index: 80000   Loss: 0.0 --\n",
      "Reward: [-0.0076   0.19829  0.001  ] total reward: 0.1917\n",
      "UEHitrate: 0.00297  edgeHitrate 0.44133 sumHitrate 0.44431  privacy: 1.0\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 15 23:01:14 2021 Episode: 0   Index: 89292   Loss: 0.0 --\n",
      "Reward: [-0.00976  0.20089  0.00102] total reward: 0.19215\n",
      "UEHitrate: 0.00298  edgeHitrate 0.44711 sumHitrate 0.45009  privacy: 1.0\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.40116, 0.41788, 0.43855, 0.44221, 0.44403, 0.44093,\n",
       "        0.44305, 0.44431, 0.45009]),\n",
       " array([0.     , 0.0018 , 0.00195, 0.0024 , 0.00247, 0.00246, 0.00258,\n",
       "        0.0028 , 0.00297, 0.00298]),\n",
       " array([0.     , 0.39936, 0.41593, 0.43615, 0.43974, 0.44157, 0.43834,\n",
       "        0.44025, 0.44133, 0.44711]))"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}