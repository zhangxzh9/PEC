{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>1030</td>\n",
       "      <td>101001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11807</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15068</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>1035</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5375</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1030</td>\n",
       "      <td>10202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5992</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3468</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148        1030        101001     0   \n",
       "1       203  5779    0        0         7        1030         10203     0   \n",
       "2       208  4675    0        0        92        1035         10203     0   \n",
       "3       159   332    0        0        56        1030         10202     0   \n",
       "4        50   674    0        0       439        1030         10203     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831   29  2591880        34        1030         10203     0   \n",
       "300979  158  8448   29  2591880        34        1030         10203     0   \n",
       "300980  483  6463   29  2591940        35        1030         10203     0   \n",
       "300981  158  4715   29  2591940        34        1030         10203     0   \n",
       "300982  483  2021   29  2591940        34        1030         10203     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0      11807          1            2  \n",
       "1              0      15068          1            2  \n",
       "2              0       5375          1            2  \n",
       "3              0       5992          1            2  \n",
       "4              0       3468          1            2  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0      10010          1            2  \n",
       "300979         0      23340          1            2  \n",
       "300980         0      10010          1            2  \n",
       "300981         0      23340          1            2  \n",
       "300982         0      10010          1            2  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "trainUIT = UIT[UIT['day']<18]\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum,trainUIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000,\n",
       " 500,\n",
       "           u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       " 0       365  3391    0        0       148        1030        101001     0   \n",
       " 1       203  5779    0        0         7        1030         10203     0   \n",
       " 2       208  4675    0        0        92        1035         10203     0   \n",
       " 3       159   332    0        0        56        1030         10202     0   \n",
       " 4        50   674    0        0       439        1030         10203     0   \n",
       " ...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       " 198170  264  7442   17  1555140        90        1035         10203     0   \n",
       " 198171   19  9362   17  1555140       424        1035         10203     0   \n",
       " 198172   82  9223   17  1555140        94        1037         10203     0   \n",
       " 198173   35  4164   17  1555140        22        1030         10203     0   \n",
       " 198174  239  5062   17  1555140        89        1035         10203     0   \n",
       " \n",
       "         city_isp  client_ip  conn_type  device_type  \n",
       " 0              0      11807          1            2  \n",
       " 1              0      15068          1            2  \n",
       " 2              0       5375          1            2  \n",
       " 3              0       5992          1            2  \n",
       " 4              0       3468          1            2  \n",
       " ...          ...        ...        ...          ...  \n",
       " 198170         0       7592          1            2  \n",
       " 198171         0       5938          1            2  \n",
       " 198172         0      11393          1            2  \n",
       " 198173         0       5866          1            2  \n",
       " 198174         0      23746          1            2  \n",
       " \n",
       " [198175 rows x 12 columns])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=15,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p) , \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                self.l_edge,\n",
    "                self.l_cp)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class UE_random(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e):\n",
    "\n",
    "        self.Rh = - self.ALPHAh * (torch.log(ru * p + (1-ru) * (1-p)) - torch.log(lastru * lastp + (1-lastru) * (1-lastp))).sum()\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        self.Rl =   self.BETAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        return  self.Rh+self.Ro+self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit):\n",
    "\n",
    "\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.Bu,self.l_edge,self.l_cp,self.e)\n",
    "        \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "\n",
    "        actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "        \n",
    "        if self.W[-1] not in actionIndex:\n",
    "            actionIndex.pop()\n",
    "        for index in actionIndex:\n",
    "            self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":1,\"betal\":1}\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "UEHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "edgeHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "privacyReduction = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in UIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE_random(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction.numpy())\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Rl)\n",
    "        sumReward[2] += float(ue.Ro)\n",
    "\n",
    "        if index % 10000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "            sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "            UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "            edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "            privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "    UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "    edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "    privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Wed Sep 22 14:02:37 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Wed Sep 22 14:02:50 2021 Episode: 0   Index: 10000   Loss: 0.0 --\n",
      "Reward: [-5.35585e+00  7.82900e-02  3.22000e-03] total reward: -5.27434\n",
      "UEHitrate: 0.003  edgeHitrate 0.08759 sumHitrate 0.09059  privacy: 3.87249\n",
      "\n",
      "--Time: Wed Sep 22 14:03:02 2021 Episode: 0   Index: 20000   Loss: 0.0 --\n",
      "Reward: [-4.57106e+00  8.10000e-02  4.40000e-03] total reward: -4.48567\n",
      "UEHitrate: 0.003  edgeHitrate 0.0904 sumHitrate 0.0934  privacy: 3.39743\n",
      "\n",
      "--Time: Wed Sep 22 14:03:15 2021 Episode: 0   Index: 30000   Loss: 0.0 --\n",
      "Reward: [-3.90029  0.0753   0.0048 ] total reward: -3.82019\n",
      "UEHitrate: 0.00327  edgeHitrate 0.08396 sumHitrate 0.08723  privacy: 3.05268\n",
      "\n",
      "--Time: Wed Sep 22 14:03:27 2021 Episode: 0   Index: 40000   Loss: 0.0 --\n",
      "Reward: [-3.51531  0.07603  0.00478] total reward: -3.4345\n",
      "UEHitrate: 0.00352  edgeHitrate 0.08472 sumHitrate 0.08825  privacy: 2.80303\n",
      "\n",
      "--Time: Wed Sep 22 14:03:39 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [-3.17549  0.07565  0.005  ] total reward: -3.09483\n",
      "UEHitrate: 0.00378  edgeHitrate 0.0844 sumHitrate 0.08818  privacy: 2.58978\n",
      "\n",
      "--Time: Wed Sep 22 14:03:52 2021 Episode: 0   Index: 60000   Loss: 0.0 --\n",
      "Reward: [-2.90897  0.07398  0.00505] total reward: -2.82994\n",
      "UEHitrate: 0.00378  edgeHitrate 0.0826 sumHitrate 0.08638  privacy: 2.42809\n",
      "\n",
      "--Time: Wed Sep 22 14:04:04 2021 Episode: 0   Index: 70000   Loss: 0.0 --\n",
      "Reward: [-2.66426  0.07231  0.00488] total reward: -2.58707\n",
      "UEHitrate: 0.0037  edgeHitrate 0.08078 sumHitrate 0.08448  privacy: 2.29468\n",
      "\n",
      "--Time: Wed Sep 22 14:04:17 2021 Episode: 0   Index: 80000   Loss: 0.0 --\n",
      "Reward: [-2.51578  0.07189  0.00485] total reward: -2.43905\n",
      "UEHitrate: 0.00371  edgeHitrate 0.0804 sumHitrate 0.08411  privacy: 2.16831\n",
      "\n",
      "--Time: Wed Sep 22 14:04:29 2021 Episode: 0   Index: 90000   Loss: 0.0 --\n",
      "Reward: [-2.33413  0.0713   0.00491] total reward: -2.25792\n",
      "UEHitrate: 0.00387  edgeHitrate 0.07975 sumHitrate 0.08362  privacy: 2.05126\n",
      "\n",
      "--Time: Wed Sep 22 14:04:41 2021 Episode: 0   Index: 100000   Loss: 0.0 --\n",
      "Reward: [-2.15183  0.07036  0.0049 ] total reward: -2.07657\n",
      "UEHitrate: 0.00383  edgeHitrate 0.07874 sumHitrate 0.08257  privacy: 1.93582\n",
      "\n",
      "--Time: Wed Sep 22 14:04:54 2021 Episode: 0   Index: 110000   Loss: 0.0 --\n",
      "Reward: [-1.98189  0.06981  0.00501] total reward: -1.90707\n",
      "UEHitrate: 0.00393  edgeHitrate 0.07819 sumHitrate 0.08212  privacy: 1.83487\n",
      "\n",
      "--Time: Wed Sep 22 14:05:06 2021 Episode: 0   Index: 120000   Loss: 0.0 --\n",
      "Reward: [-1.80591  0.06936  0.00507] total reward: -1.73148\n",
      "UEHitrate: 0.00398  edgeHitrate 0.07764 sumHitrate 0.08162  privacy: 1.7502\n",
      "\n",
      "--Time: Wed Sep 22 14:05:18 2021 Episode: 0   Index: 130000   Loss: 0.0 --\n",
      "Reward: [-1.69505  0.06852  0.00526] total reward: -1.62127\n",
      "UEHitrate: 0.00419  edgeHitrate 0.07673 sumHitrate 0.08092  privacy: 1.66939\n",
      "\n",
      "--Time: Wed Sep 22 14:05:31 2021 Episode: 0   Index: 140000   Loss: 0.0 --\n",
      "Reward: [-1.52925  0.06866  0.00542] total reward: -1.45517\n",
      "UEHitrate: 0.00428  edgeHitrate 0.07691 sumHitrate 0.08119  privacy: 1.59503\n",
      "\n",
      "--Time: Wed Sep 22 14:05:43 2021 Episode: 0   Index: 150000   Loss: 0.0 --\n",
      "Reward: [-1.44829  0.06837  0.00552] total reward: -1.3744\n",
      "UEHitrate: 0.00436  edgeHitrate 0.07659 sumHitrate 0.08095  privacy: 1.52718\n",
      "\n",
      "--Time: Wed Sep 22 14:05:55 2021 Episode: 0   Index: 160000   Loss: 0.0 --\n",
      "Reward: [-1.34783  0.06817  0.00566] total reward: -1.274\n",
      "UEHitrate: 0.00438  edgeHitrate 0.07641 sumHitrate 0.08079  privacy: 1.46064\n",
      "\n",
      "--Time: Wed Sep 22 14:06:07 2021 Episode: 0   Index: 170000   Loss: 0.0 --\n",
      "Reward: [-1.27279  0.06822  0.00571] total reward: -1.19886\n",
      "UEHitrate: 0.00438  edgeHitrate 0.07643 sumHitrate 0.08082  privacy: 1.40018\n",
      "\n",
      "--Time: Wed Sep 22 14:06:19 2021 Episode: 0   Index: 180000   Loss: 0.0 --\n",
      "Reward: [-1.17523  0.06846  0.00582] total reward: -1.10095\n",
      "UEHitrate: 0.00438  edgeHitrate 0.07668 sumHitrate 0.08106  privacy: 1.33967\n",
      "\n",
      "--Time: Wed Sep 22 14:06:32 2021 Episode: 0   Index: 190000   Loss: 0.0 --\n",
      "Reward: [-1.11772  0.06816  0.00583] total reward: -1.04372\n",
      "UEHitrate: 0.00446  edgeHitrate 0.07633 sumHitrate 0.08078  privacy: 1.28351\n",
      "\n",
      "--Time: Wed Sep 22 14:06:44 2021 Episode: 0   Index: 200000   Loss: 0.0 --\n",
      "Reward: [-1.06168  0.06835  0.00587] total reward: -0.98746\n",
      "UEHitrate: 0.00444  edgeHitrate 0.0765 sumHitrate 0.08094  privacy: 1.23272\n",
      "\n",
      "--Time: Wed Sep 22 14:06:56 2021 Episode: 0   Index: 210000   Loss: 0.0 --\n",
      "Reward: [-0.98853  0.06856  0.00597] total reward: -0.91401\n",
      "UEHitrate: 0.0045  edgeHitrate 0.07675 sumHitrate 0.08124  privacy: 1.18247\n",
      "\n",
      "--Time: Wed Sep 22 14:07:08 2021 Episode: 0   Index: 220000   Loss: 0.0 --\n",
      "Reward: [-0.93044  0.06822  0.00603] total reward: -0.85619\n",
      "UEHitrate: 0.0045  edgeHitrate 0.07639 sumHitrate 0.08088  privacy: 1.13662\n",
      "\n",
      "--Time: Wed Sep 22 14:07:21 2021 Episode: 0   Index: 230000   Loss: 0.0 --\n",
      "Reward: [-0.88623  0.06817  0.00607] total reward: -0.812\n",
      "UEHitrate: 0.00455  edgeHitrate 0.0763 sumHitrate 0.08085  privacy: 1.09053\n",
      "\n",
      "--Time: Wed Sep 22 14:07:33 2021 Episode: 0   Index: 240000   Loss: 0.0 --\n",
      "Reward: [-0.82482  0.06802  0.00609] total reward: -0.7507\n",
      "UEHitrate: 0.00458  edgeHitrate 0.07611 sumHitrate 0.0807  privacy: 1.04733\n",
      "\n",
      "--Time: Wed Sep 22 14:07:45 2021 Episode: 0   Index: 250000   Loss: 0.0 --\n",
      "Reward: [-0.78391  0.06837  0.00611] total reward: -0.70943\n",
      "UEHitrate: 0.00466  edgeHitrate 0.07648 sumHitrate 0.08114  privacy: 1.00647\n",
      "\n",
      "--Time: Wed Sep 22 14:07:57 2021 Episode: 0   Index: 260000   Loss: 0.0 --\n",
      "Reward: [-0.75333  0.06816  0.00612] total reward: -0.67906\n",
      "UEHitrate: 0.0047  edgeHitrate 0.07623 sumHitrate 0.08093  privacy: 0.96889\n",
      "\n",
      "--Time: Wed Sep 22 14:08:09 2021 Episode: 0   Index: 270000   Loss: 0.0 --\n",
      "Reward: [-0.71663  0.0679   0.00616] total reward: -0.64257\n",
      "UEHitrate: 0.00477  edgeHitrate 0.07596 sumHitrate 0.08072  privacy: 0.93318\n",
      "\n",
      "--Time: Wed Sep 22 14:08:22 2021 Episode: 0   Index: 280000   Loss: 0.0 --\n",
      "Reward: [-0.6973   0.06808  0.00619] total reward: -0.62303\n",
      "UEHitrate: 0.00481  edgeHitrate 0.07615 sumHitrate 0.08097  privacy: 0.8988\n",
      "\n",
      "--Time: Wed Sep 22 14:08:34 2021 Episode: 0   Index: 290000   Loss: 0.0 --\n",
      "Reward: [-0.67244  0.06826  0.00618] total reward: -0.598\n",
      "UEHitrate: 0.00481  edgeHitrate 0.07636 sumHitrate 0.08117  privacy: 0.86455\n",
      "\n",
      "--Time: Wed Sep 22 14:08:46 2021 Episode: 0   Index: 300000   Loss: 0.0 --\n",
      "Reward: [-0.64508  0.06823  0.00616] total reward: -0.57069\n",
      "UEHitrate: 0.00479  edgeHitrate 0.07632 sumHitrate 0.08112  privacy: 0.83371\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 22 14:08:47 2021 Episode: 0   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-0.64275  0.06829  0.00615] total reward: -0.56831\n",
      "UEHitrate: 0.00479  edgeHitrate 0.07638 sumHitrate 0.08118  privacy: 0.83082\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.09059, 0.0934 , 0.08723, 0.08825, 0.08818, 0.08638,\n",
       "        0.08448, 0.08411, 0.08362, 0.08257, 0.08212, 0.08162, 0.08092,\n",
       "        0.08119, 0.08095, 0.08079, 0.08082, 0.08106, 0.08078, 0.08094,\n",
       "        0.08124, 0.08088, 0.08085, 0.0807 , 0.08114, 0.08093, 0.08072,\n",
       "        0.08097, 0.08117, 0.08118, 0.     ]),\n",
       " array([0.     , 0.003  , 0.003  , 0.00327, 0.00352, 0.00378, 0.00378,\n",
       "        0.0037 , 0.00371, 0.00387, 0.00383, 0.00393, 0.00398, 0.00419,\n",
       "        0.00428, 0.00436, 0.00438, 0.00438, 0.00438, 0.00446, 0.00444,\n",
       "        0.0045 , 0.0045 , 0.00455, 0.00458, 0.00466, 0.0047 , 0.00477,\n",
       "        0.00481, 0.00481, 0.00479, 0.     ]),\n",
       " array([0.     , 0.08759, 0.0904 , 0.08396, 0.08472, 0.0844 , 0.0826 ,\n",
       "        0.08078, 0.0804 , 0.07975, 0.07874, 0.07819, 0.07764, 0.07673,\n",
       "        0.07691, 0.07659, 0.07641, 0.07643, 0.07668, 0.07633, 0.0765 ,\n",
       "        0.07675, 0.07639, 0.0763 , 0.07611, 0.07648, 0.07623, 0.07596,\n",
       "        0.07615, 0.07636, 0.07638, 0.     ]))"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([4.02118, 3.87249, 3.39743, 3.05268, 2.80303, 2.58978, 2.42809,\n",
       "       2.29468, 2.16831, 2.05126, 1.93582, 1.83487, 1.7502 , 1.66939,\n",
       "       1.59503, 1.52718, 1.46064, 1.40018, 1.33967, 1.28351, 1.23272,\n",
       "       1.18247, 1.13662, 1.09053, 1.04733, 1.00647, 0.96889, 0.93318,\n",
       "       0.8988 , 0.86455, 0.83082, 0.     ])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class UE_None(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e):\n",
    "\n",
    "        self.Rh = - self.ALPHAh * (torch.log(lastru * lastp + (1-lastru) * (1-lastp)).sum() - torch.log(ru * p + (1-ru) * (1-p)).sum())\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        self.Rl =   self.BETAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        return  self.Rh+self.Ro+self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit):\n",
    "\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.Bu,self.l_edge,self.l_cp,self.e)\n",
    "        \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        return self.action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":0.5,\"betal\":0.5}\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "UEHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "edgeHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "privacyReduction = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "\n",
    "    for index,trace in UIT.iterrows():\n",
    "        uit = trace.to_numpy()\n",
    "        if uit[0] not in UEs:\n",
    "            UEs[uit[0]] = UE_None(uit[0],env,rewardPara)\n",
    "\n",
    "        ue = UEs[uit[0]]\n",
    "        \n",
    "        actionIndex = np.argwhere(ue.lastAction)\n",
    "        if uit[1] in actionIndex:\n",
    "            UEHit[uit[0]] += 1\n",
    "        elif uit[1] in env.pipe.keys():\n",
    "            edgeHit += 1\n",
    "\n",
    "        ue.selectAction(env,uit)\n",
    "\n",
    "        sumReward[0] += float(ue.Rh)\n",
    "        sumReward[1] += float(ue.Rl)\n",
    "        sumReward[2] += float(ue.Ro)\n",
    "\n",
    "        if index % 10000 == 0 :\n",
    "            psi = 0\n",
    "            p = torch.from_numpy(env.p)\n",
    "            for u in UEs:\n",
    "                psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "            print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "            print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "            print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "            print()\n",
    "            sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "            UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "            edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "            privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "    psi = 0\n",
    "    p = torch.from_numpy(env.p)\n",
    "    for u in UEs:\n",
    "        psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "    print()\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "    print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "    print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "    print(\"----------------------------------------------------------------\")\n",
    "    print()\n",
    "\n",
    "    sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "    UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "    edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "    privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Wed Sep 22 14:49:10 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:49:21 2021 Episode: 0   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.34533 0.1717  0.00095] total reward: 0.51799\n",
      "UEHitrate: 0.0012  edgeHitrate 0.38216 sumHitrate 0.38336  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:49:32 2021 Episode: 0   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.25903 0.19043 0.00131] total reward: 0.45077\n",
      "UEHitrate: 0.0014  edgeHitrate 0.42373 sumHitrate 0.42513  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:49:43 2021 Episode: 0   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.20849 0.18301 0.00151] total reward: 0.393\n",
      "UEHitrate: 0.00187  edgeHitrate 0.40742 sumHitrate 0.40929  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:49:54 2021 Episode: 0   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.17622 0.17735 0.00155] total reward: 0.35512\n",
      "UEHitrate: 0.0021  edgeHitrate 0.39479 sumHitrate 0.39689  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:50:04 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.14914 0.17622 0.00166] total reward: 0.32702\n",
      "UEHitrate: 0.00242  edgeHitrate 0.39225 sumHitrate 0.39467  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:50:15 2021 Episode: 0   Index: 60000   Loss: 0.0 --\n",
      "Reward: [0.13196 0.17503 0.0017 ] total reward: 0.30869\n",
      "UEHitrate: 0.00232  edgeHitrate 0.38956 sumHitrate 0.39188  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:50:26 2021 Episode: 0   Index: 70000   Loss: 0.0 --\n",
      "Reward: [0.1137  0.17168 0.00163] total reward: 0.287\n",
      "UEHitrate: 0.00227  edgeHitrate 0.38207 sumHitrate 0.38434  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:50:36 2021 Episode: 0   Index: 80000   Loss: 0.0 --\n",
      "Reward: [0.10244 0.17335 0.00163] total reward: 0.27742\n",
      "UEHitrate: 0.00227  edgeHitrate 0.38575 sumHitrate 0.38802  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:50:47 2021 Episode: 0   Index: 90000   Loss: 0.0 --\n",
      "Reward: [0.09235 0.17225 0.00168] total reward: 0.26629\n",
      "UEHitrate: 0.00238  edgeHitrate 0.38325 sumHitrate 0.38563  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:50:58 2021 Episode: 0   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.08198 0.17205 0.00165] total reward: 0.25568\n",
      "UEHitrate: 0.00239  edgeHitrate 0.38282 sumHitrate 0.38521  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:51:09 2021 Episode: 0   Index: 110000   Loss: 0.0 --\n",
      "Reward: [0.07196 0.17218 0.00168] total reward: 0.24582\n",
      "UEHitrate: 0.00242  edgeHitrate 0.3832 sumHitrate 0.38561  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:51:20 2021 Episode: 0   Index: 120000   Loss: 0.0 --\n",
      "Reward: [0.0637  0.17198 0.00171] total reward: 0.2374\n",
      "UEHitrate: 0.00249  edgeHitrate 0.38271 sumHitrate 0.3852  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:51:31 2021 Episode: 0   Index: 130000   Loss: 0.0 --\n",
      "Reward: [0.05731 0.17014 0.0018 ] total reward: 0.22925\n",
      "UEHitrate: 0.00267  edgeHitrate 0.37858 sumHitrate 0.38125  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:51:42 2021 Episode: 0   Index: 140000   Loss: 0.0 --\n",
      "Reward: [0.04989 0.17189 0.00182] total reward: 0.2236\n",
      "UEHitrate: 0.00273  edgeHitrate 0.38255 sumHitrate 0.38528  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:51:53 2021 Episode: 0   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.04756 0.17376 0.00184] total reward: 0.22316\n",
      "UEHitrate: 0.00281  edgeHitrate 0.38669 sumHitrate 0.3895  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:52:04 2021 Episode: 0   Index: 160000   Loss: 0.0 --\n",
      "Reward: [0.04293 0.1725  0.00191] total reward: 0.21734\n",
      "UEHitrate: 0.00282  edgeHitrate 0.38395 sumHitrate 0.38677  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:52:15 2021 Episode: 0   Index: 170000   Loss: 0.0 --\n",
      "Reward: [0.03964 0.17389 0.00193] total reward: 0.21547\n",
      "UEHitrate: 0.00285  edgeHitrate 0.38706 sumHitrate 0.38992  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:52:26 2021 Episode: 0   Index: 180000   Loss: 0.0 --\n",
      "Reward: [0.0357  0.17494 0.00196] total reward: 0.21261\n",
      "UEHitrate: 0.00284  edgeHitrate 0.38945 sumHitrate 0.39229  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:52:37 2021 Episode: 0   Index: 190000   Loss: 0.0 --\n",
      "Reward: [0.03237 0.17569 0.00198] total reward: 0.21004\n",
      "UEHitrate: 0.00293  edgeHitrate 0.3911 sumHitrate 0.39403  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:52:48 2021 Episode: 0   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.03076 0.17588 0.00198] total reward: 0.20861\n",
      "UEHitrate: 0.00292  edgeHitrate 0.39157 sumHitrate 0.3945  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:52:58 2021 Episode: 0   Index: 210000   Loss: 0.0 --\n",
      "Reward: [0.02828 0.17663 0.002  ] total reward: 0.20692\n",
      "UEHitrate: 0.00299  edgeHitrate 0.39326 sumHitrate 0.39625  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:53:09 2021 Episode: 0   Index: 220000   Loss: 0.0 --\n",
      "Reward: [0.02638 0.17581 0.00202] total reward: 0.20421\n",
      "UEHitrate: 0.00302  edgeHitrate 0.39143 sumHitrate 0.39444  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:53:20 2021 Episode: 0   Index: 230000   Loss: 0.0 --\n",
      "Reward: [0.02476 0.17519 0.00204] total reward: 0.20199\n",
      "UEHitrate: 0.00307  edgeHitrate 0.39003 sumHitrate 0.39309  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:53:31 2021 Episode: 0   Index: 240000   Loss: 0.0 --\n",
      "Reward: [0.02277 0.17464 0.00205] total reward: 0.19946\n",
      "UEHitrate: 0.00309  edgeHitrate 0.3888 sumHitrate 0.39189  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:53:42 2021 Episode: 0   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.02075 0.17484 0.00205] total reward: 0.19764\n",
      "UEHitrate: 0.00314  edgeHitrate 0.38926 sumHitrate 0.3924  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:53:53 2021 Episode: 0   Index: 260000   Loss: 0.0 --\n",
      "Reward: [0.01954 0.17394 0.00205] total reward: 0.19552\n",
      "UEHitrate: 0.00319  edgeHitrate 0.38726 sumHitrate 0.39045  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:54:04 2021 Episode: 0   Index: 270000   Loss: 0.0 --\n",
      "Reward: [0.01925 0.17302 0.00207] total reward: 0.19434\n",
      "UEHitrate: 0.00325  edgeHitrate 0.38521 sumHitrate 0.38846  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:54:15 2021 Episode: 0   Index: 280000   Loss: 0.0 --\n",
      "Reward: [0.01929 0.173   0.00209] total reward: 0.19438\n",
      "UEHitrate: 0.00327  edgeHitrate 0.3852 sumHitrate 0.38847  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:54:26 2021 Episode: 0   Index: 290000   Loss: 0.0 --\n",
      "Reward: [0.0185  0.17403 0.00208] total reward: 0.19461\n",
      "UEHitrate: 0.00328  edgeHitrate 0.3875 sumHitrate 0.39078  privacy: 1.0\n",
      "\n",
      "--Time: Wed Sep 22 14:54:36 2021 Episode: 0   Index: 300000   Loss: 0.0 --\n",
      "Reward: [0.01758 0.17555 0.00208] total reward: 0.19521\n",
      "UEHitrate: 0.00329  edgeHitrate 0.39085 sumHitrate 0.39414  privacy: 1.0\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Wed Sep 22 14:54:38 2021 Episode: 0   Index: 300982   Loss: 0.0 --\n",
      "Reward: [0.01745 0.17569 0.00207] total reward: 0.19521\n",
      "UEHitrate: 0.0033  edgeHitrate 0.39115 sumHitrate 0.39444  privacy: 1.0\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.38336, 0.42513, 0.40929, 0.39689, 0.39467, 0.39188,\n",
       "        0.38434, 0.38802, 0.38563, 0.38521, 0.38561, 0.3852 , 0.38125,\n",
       "        0.38528, 0.3895 , 0.38677, 0.38992, 0.39229, 0.39403, 0.3945 ,\n",
       "        0.39625, 0.39444, 0.39309, 0.39189, 0.3924 , 0.39045, 0.38846,\n",
       "        0.38847, 0.39078, 0.39444, 0.     ]),\n",
       " array([0.     , 0.0012 , 0.0014 , 0.00187, 0.0021 , 0.00242, 0.00232,\n",
       "        0.00227, 0.00227, 0.00238, 0.00239, 0.00242, 0.00249, 0.00267,\n",
       "        0.00273, 0.00281, 0.00282, 0.00285, 0.00284, 0.00293, 0.00292,\n",
       "        0.00299, 0.00302, 0.00307, 0.00309, 0.00314, 0.00319, 0.00325,\n",
       "        0.00327, 0.00328, 0.0033 , 0.     ]),\n",
       " array([0.     , 0.38216, 0.42373, 0.40742, 0.39479, 0.39225, 0.38956,\n",
       "        0.38207, 0.38575, 0.38325, 0.38282, 0.3832 , 0.38271, 0.37858,\n",
       "        0.38255, 0.38669, 0.38395, 0.38706, 0.38945, 0.3911 , 0.39157,\n",
       "        0.39326, 0.39143, 0.39003, 0.3888 , 0.38926, 0.38726, 0.38521,\n",
       "        0.3852 , 0.3875 , 0.39115, 0.     ]))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}