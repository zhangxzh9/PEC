{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((132690, 4), (76252, 4), (106994, 4))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "#import os\n",
    "#pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "#if not os.path.exists(pathName):\n",
    "#     os.makedirs(pathName)\n",
    "#MODELPATH = pathName + 'dnn_v1.0_'\n",
    "\n",
    "data_path = '/home/zhangxz/workspace/dataset/ml-100k/'\n",
    "UIT = pd.read_csv(data_path + 'ml_header.trace')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":0.01,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "UIT.shape,trainUIT.shape,validUIT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE_random(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "\n",
    "            actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "        \n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                action[index] = 1           \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sat Oct 16 14:10:53 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-8.8958e-01  3.2000e-04  2.6800e-02] total reward: -0.86246\n",
      "UEHitrate: 0.0012  edgeHitrate 0.0335 sumHitrate 0.0347  privacy: 6.33933\n",
      "\n",
      "--Time: Sat Oct 16 14:11:18 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-7.7216e-01  2.7000e-04  2.4160e-02] total reward: -0.74773\n",
      "UEHitrate: 0.00115  edgeHitrate 0.0302 sumHitrate 0.03135  privacy: 5.42722\n",
      "\n",
      "--Time: Sat Oct 16 14:11:42 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-7.1296e-01  2.7000e-04  2.3680e-02] total reward: -0.68902\n",
      "UEHitrate: 0.0012  edgeHitrate 0.0296 sumHitrate 0.0308  privacy: 4.93379\n",
      "\n",
      "--Time: Sat Oct 16 14:12:08 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-6.7177e-01  2.4000e-04  2.3800e-02] total reward: -0.64774\n",
      "UEHitrate: 0.00108  edgeHitrate 0.02975 sumHitrate 0.03082  privacy: 4.48328\n",
      "\n",
      "--Time: Sat Oct 16 14:12:34 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-6.3812e-01  2.6000e-04  2.4080e-02] total reward: -0.61378\n",
      "UEHitrate: 0.00106  edgeHitrate 0.0301 sumHitrate 0.03116  privacy: 4.12726\n",
      "\n",
      "--Time: Sat Oct 16 14:13:01 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-6.0579e-01  2.4000e-04  2.3970e-02] total reward: -0.58157\n",
      "UEHitrate: 0.00102  edgeHitrate 0.02997 sumHitrate 0.03098  privacy: 3.78263\n",
      "\n",
      "--Time: Sat Oct 16 14:13:28 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-5.7176e-01  2.3000e-04  2.3780e-02] total reward: -0.54774\n",
      "UEHitrate: 0.001  edgeHitrate 0.02973 sumHitrate 0.03073  privacy: 3.57141\n",
      "\n",
      "--Time: Sat Oct 16 14:13:54 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-5.4212e-01  2.5000e-04  2.3420e-02] total reward: -0.51845\n",
      "UEHitrate: 0.00101  edgeHitrate 0.02927 sumHitrate 0.03029  privacy: 3.36097\n",
      "\n",
      "--Time: Sat Oct 16 14:14:20 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-5.2292e-01  2.5000e-04  2.3710e-02] total reward: -0.49897\n",
      "UEHitrate: 0.00103  edgeHitrate 0.02963 sumHitrate 0.03067  privacy: 3.14692\n",
      "\n",
      "--Time: Sat Oct 16 14:14:44 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-5.0111e-01  2.4000e-04  2.3670e-02] total reward: -0.47719\n",
      "UEHitrate: 0.001  edgeHitrate 0.02959 sumHitrate 0.03059  privacy: 2.96666\n",
      "\n",
      "--Time: Sat Oct 16 14:15:11 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-4.886e-01  2.500e-04  2.376e-02] total reward: -0.46458\n",
      "UEHitrate: 0.00105  edgeHitrate 0.0297 sumHitrate 0.03075  privacy: 2.78713\n",
      "\n",
      "--Time: Sat Oct 16 14:15:37 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-4.768e-01  2.500e-04  2.362e-02] total reward: -0.45293\n",
      "UEHitrate: 0.00103  edgeHitrate 0.02952 sumHitrate 0.03056  privacy: 2.64331\n",
      "\n",
      "--Time: Sat Oct 16 14:16:02 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-4.626e-01  2.500e-04  2.363e-02] total reward: -0.43872\n",
      "UEHitrate: 0.00102  edgeHitrate 0.02954 sumHitrate 0.03055  privacy: 2.52949\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sat Oct 16 14:16:09 2021   Index: 132689   Loss: 0.0 --\n",
      "Reward: [-4.5988e-01  2.5000e-04  2.3630e-02] total reward: -0.436\n",
      "UEHitrate: 0.00101  edgeHitrate 0.02954 sumHitrate 0.03055  privacy: 2.4977\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE_random(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0347 , 0.03135, 0.0308 , 0.03082, 0.03116, 0.03098, 0.03073,\n",
       "        0.03029, 0.03067, 0.03059, 0.03075, 0.03056, 0.03055, 0.03055]),\n",
       " array([0.0012 , 0.00115, 0.0012 , 0.00108, 0.00106, 0.00102, 0.001  ,\n",
       "        0.00101, 0.00103, 0.001  , 0.00105, 0.00103, 0.00102, 0.00101]),\n",
       " array([0.0335 , 0.0302 , 0.0296 , 0.02975, 0.0301 , 0.02997, 0.02973,\n",
       "        0.02927, 0.02963, 0.02959, 0.0297 , 0.02952, 0.02954, 0.02954]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.33933, 5.42722, 4.93379, 4.48328, 4.12726, 3.78263, 3.57141,\n",
       "       3.36097, 3.14692, 2.96666, 2.78713, 2.64331, 2.52949, 2.4977 ])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UE_None(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "       # self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1      \n",
    "            self.lastAction = action\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sat Oct 16 14:16:26 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-0.05229  0.       0.2444 ] total reward: 0.19211\n",
      "UEHitrate: 0.0  edgeHitrate 0.3055 sumHitrate 0.3055  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:16:42 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-0.04966  0.       0.21968] total reward: 0.17002\n",
      "UEHitrate: 0.0  edgeHitrate 0.2746 sumHitrate 0.2746  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:16:59 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-0.04725  0.       0.22579] total reward: 0.17854\n",
      "UEHitrate: 0.0  edgeHitrate 0.28223 sumHitrate 0.28223  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:17:16 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-0.04593  0.       0.21646] total reward: 0.17053\n",
      "UEHitrate: 0.0  edgeHitrate 0.27058 sumHitrate 0.27058  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:17:32 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-0.0442  0.      0.2208] total reward: 0.1766\n",
      "UEHitrate: 0.0  edgeHitrate 0.276 sumHitrate 0.276  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:17:50 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-0.04337  0.       0.21369] total reward: 0.17033\n",
      "UEHitrate: 0.0  edgeHitrate 0.26712 sumHitrate 0.26712  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:18:08 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-0.04317  0.       0.20834] total reward: 0.16518\n",
      "UEHitrate: 0.0  edgeHitrate 0.26043 sumHitrate 0.26043  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:18:26 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-0.04282  0.       0.20334] total reward: 0.16052\n",
      "UEHitrate: 0.0  edgeHitrate 0.25417 sumHitrate 0.25418  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:18:44 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-0.04165  0.       0.20666] total reward: 0.16501\n",
      "UEHitrate: 0.0  edgeHitrate 0.25832 sumHitrate 0.25832  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:19:01 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-0.04137  0.       0.20334] total reward: 0.16196\n",
      "UEHitrate: 0.0  edgeHitrate 0.25417 sumHitrate 0.25417  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:19:19 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-0.04039  0.       0.20859] total reward: 0.1682\n",
      "UEHitrate: 0.0  edgeHitrate 0.26074 sumHitrate 0.26074  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:19:36 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-0.03968  0.       0.20839] total reward: 0.16871\n",
      "UEHitrate: 0.0  edgeHitrate 0.26049 sumHitrate 0.26049  privacy: 1.0\n",
      "\n",
      "--Time: Sat Oct 16 14:19:53 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-0.03924  0.       0.20367] total reward: 0.16442\n",
      "UEHitrate: 0.0  edgeHitrate 0.25458 sumHitrate 0.25458  privacy: 1.0\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sat Oct 16 14:19:59 2021   Index: 132689   Loss: 0.0 --\n",
      "Reward: [-0.03907  0.       0.20393] total reward: 0.16485\n",
      "UEHitrate: 0.0  edgeHitrate 0.25491 sumHitrate 0.25491  privacy: 1.0\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bestReward =  float(\"-inf\")\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "UEHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "edgeHitrate = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "privacyReduction = np.zeros(UIT.shape[0] // 10000 +1)\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE_None(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    actionIndex = np.argwhere(ue.lastAction)\n",
    "    if uit[1] in actionIndex:\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    ue.selectAction(env,uit)\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Ro)\n",
    "    sumReward[2] += float(ue.Rl)\n",
    "     \n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.3055 , 0.2746 , 0.28223, 0.27058, 0.276  , 0.26712, 0.26043,\n",
       "        0.25418, 0.25832, 0.25417, 0.26074, 0.26049, 0.25458, 0.25491]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.3055 , 0.2746 , 0.28223, 0.27058, 0.276  , 0.26712, 0.26043,\n",
       "        0.25417, 0.25832, 0.25417, 0.26074, 0.26049, 0.25458, 0.25491]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
