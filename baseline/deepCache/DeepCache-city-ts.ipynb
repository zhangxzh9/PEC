{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#from torchtext.datasets import Multi30k\n",
    "#from torchtext.data import Field, BucketIterator\n",
    "\n",
    "#import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import torch.utils.data as data\n",
    "import pandas as pd\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        #self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.rnn = nn.LSTM(input_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        embedded = self.dropout(src)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        #self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.rnn = nn.LSTM(output_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input_ = input_.float().unsqueeze(0)\n",
    "        #print(\"decode input shape\",input_.shape)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(input))\n",
    "        embedded = self.dropout(input_)\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        #prediction = self.fc_out(output.squeeze(0)).unsqueeze(-1)\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        #print(prediction.shape)\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        #print(\"trg_len\",trg.shape[0],\"batch_size\",trg.shape[1])\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        #print(input.shape)\n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            #top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            #print(trg[t].shape,output.unsqueeze(-1).shape)\n",
    "            input = trg[t] if teacher_force else output\n",
    "        \n",
    "        return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "INPUT_DIM = 1\n",
    "OUTPUT_DIM = 1\n",
    "ENC_EMB_DIM = 0\n",
    "DEC_EMB_DIM = 0\n",
    "HID_DIM = 64\n",
    "#HID_DIM = 64\n",
    "\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "FOLD = 2\n",
    "CITY = 0\n",
    "BATCHSIZE = 10000\n",
    "\n",
    "model.load_state_dict(torch.load(\"fold{}-city{}-model.pt\".format(FOLD,CITY)))      "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum)\n",
    "        self.e = np.zeros(shape=contentNum)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l_edge = 0.1\n",
    "        self.l_cp = 1\n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=15,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "\n",
    "        self.r[u] = self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (torch.from_numpy(self.r),\n",
    "                torch.from_numpy(self.p) , \n",
    "                torch.from_numpy(self.e),\n",
    "                torch.from_numpy(self.S),\n",
    "                self.l_edge,\n",
    "                self.l_cp)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "data_path = '/home/ubuntu/data/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "UIT['day'] = UIT['time']//(60*60)\n",
    "UIT"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>1030</td>\n",
       "      <td>101001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11807</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15068</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>1035</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5375</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1030</td>\n",
       "      <td>10202</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5992</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3468</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>719</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>719</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>719</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>719</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23340</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>719</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>1030</td>\n",
       "      <td>10203</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10010</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148        1030        101001     0   \n",
       "1       203  5779    0        0         7        1030         10203     0   \n",
       "2       208  4675    0        0        92        1035         10203     0   \n",
       "3       159   332    0        0        56        1030         10202     0   \n",
       "4        50   674    0        0       439        1030         10203     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831  719  2591880        34        1030         10203     0   \n",
       "300979  158  8448  719  2591880        34        1030         10203     0   \n",
       "300980  483  6463  719  2591940        35        1030         10203     0   \n",
       "300981  158  4715  719  2591940        34        1030         10203     0   \n",
       "300982  483  2021  719  2591940        34        1030         10203     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0      11807          1            2  \n",
       "1              0      15068          1            2  \n",
       "2              0       5375          1            2  \n",
       "3              0       5992          1            2  \n",
       "4              0       3468          1            2  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0      10010          1            2  \n",
       "300979         0      23340          1            2  \n",
       "300980         0      10010          1            2  \n",
       "300981         0      23340          1            2  \n",
       "300982         0      10010          1            2  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "contentNum,userNum"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 500)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "\n",
    "        self.Bu = int(env.B[self.u])\n",
    "        self.contentNum = env.contentNum\n",
    "        self.userNum = env.userNum\n",
    "\n",
    "        self.r , self.p , self.e, self.S,self.l_edge, self.l_cp = env.getStatus()\n",
    "\n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.lastAction = self.action\n",
    "\n",
    "        self.reward = 0\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']\n",
    "\n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,action,S,Bu,l_edge,l_cp,e):\n",
    "\n",
    "        self.Rh = - self.ALPHAh * (torch.log(lastru * lastp + (1-lastru) * (1-lastp)).sum() - torch.log(ru * p + (1-ru) * (1-p)).sum())\n",
    "\n",
    "        self.Ro =   self.BETAo * action[i] * (S[i] / Bu + ( e[i] * l_edge + ( 1-e[i] ) * l_cp ) / S[i])\n",
    "\n",
    "        self.Rl =   self.BETAl * ( ( 1 - action[i] )  * ( l_cp - ( e[i] * l_edge + ( 1 - e[i] ) * l_cp ) ) ) / S[i]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        return  self.Rh+self.Ro+self.Rl\n",
    "\n",
    "    def selectAction(self,env,uit,dpc,data):\n",
    "\n",
    "        self.lastAction = self.action\n",
    "        self.lastp = self.p\n",
    "        self.lastr = self.r\n",
    "\n",
    "        self.updateViewContent(uit[1])\n",
    "        self.r , self.p , self.e, self.S, self.l_edge, self.l_cp = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastr[self.u],self.lastp,self.r[self.u],self.p,self.W[-1],self.lastAction,self.S,self.Bu,self.l_edge,self.l_cp,self.e)\n",
    "        \n",
    "        \n",
    "        self.action = torch.zeros(size=(env.contentNum,),dtype=int)\n",
    "        self.action[self.W[-1]] = 1\n",
    "        \n",
    "        if data != None:\n",
    "            actionIndex = self.evaluate(dpc,data,self.Bu+1)\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop() \n",
    "        #else:\n",
    "        #    actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "        #    if self.W[-1] not in actionIndex:\n",
    "        #        actionIndex.pop()\n",
    "            for index in actionIndex:\n",
    "                self.action[index] = 1\n",
    "\n",
    "        env.updateEnv(self.u,self.action.numpy(),uit[2])\n",
    "\n",
    "        #print(self.W[-1],np.argwhere(ue.action.numpy()==1),cacheSet)\n",
    "        return self.action\n",
    "    def evaluate(self,model, batch, cachesize):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            src = batch.permute(1,0,2).float().cuda()\n",
    "\n",
    "            batchsize = src.shape[1]\n",
    "            trg = torch.zeros((26,batchsize,1)).cuda()\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output.squeeze(-1)\n",
    "\n",
    "            output = output[0].cpu().numpy()\n",
    "\n",
    "            max_item=np.argsort(-output)[0:cachesize]\n",
    "            #print(max_item,list(max_item))\n",
    "        return list(max_item)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "Frq_hour = torch.zeros((BATCHSIZE,720)).to(device)\n",
    "m = 20\n",
    "\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":1,\"betal\":1}\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "UEHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "edgeHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "privacyReduction = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "i_episode = 0\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    if uit[1] in np.argwhere(ue.lastAction.numpy()):\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    if uit[2]>=m:\n",
    "        test_iterator = Frq_hour[:,uit[2]-m:uit[2]]/Frq_hour[:,uit[2]-m:uit[2]].sum(dim=0)\n",
    "        test_iterator = test_iterator.unsqueeze(2)\n",
    "        #print(test_iterator.sum())\n",
    "        \n",
    "    else:\n",
    "        test_iterator = None\n",
    "    \n",
    "    ue.selectAction(env,uit,model,test_iterator)\n",
    "\n",
    "    for content in np.argwhere(ue.action.numpy()):\n",
    "        Frq_hour[content,uit[2]]+=1\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Rl)\n",
    "    sumReward[2] += float(ue.Ro)\n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\" Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sun Sep 26 18:00:58 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 1.0\n",
      "\n",
      "--Time: Sun Sep 26 18:01:37 2021 Episode: 0   Index: 10000   Loss: 0.0 --\n",
      "Reward: [0.43948 0.34233 0.00194] total reward: 0.78375\n",
      "UEHitrate: 0.0014  edgeHitrate 0.38096 sumHitrate 0.38236  privacy: 1.05103\n",
      "\n",
      "--Time: Sun Sep 26 18:06:32 2021 Episode: 0   Index: 20000   Loss: 0.0 --\n",
      "Reward: [0.36869 0.37614 0.00267] total reward: 0.7475\n",
      "UEHitrate: 0.00175  edgeHitrate 0.41843 sumHitrate 0.42018  privacy: 0.94254\n",
      "\n",
      "--Time: Sun Sep 26 18:11:26 2021 Episode: 0   Index: 30000   Loss: 0.0 --\n",
      "Reward: [0.31179 0.36191 0.00308] total reward: 0.67678\n",
      "UEHitrate: 0.0023  edgeHitrate 0.40282 sumHitrate 0.40512  privacy: 0.9181\n",
      "\n",
      "--Time: Sun Sep 26 18:16:22 2021 Episode: 0   Index: 40000   Loss: 0.0 --\n",
      "Reward: [0.26125 0.35025 0.00321] total reward: 0.61471\n",
      "UEHitrate: 0.00257  edgeHitrate 0.38982 sumHitrate 0.39239  privacy: 0.91442\n",
      "\n",
      "--Time: Sun Sep 26 18:21:18 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [0.22009 0.34702 0.00344] total reward: 0.57054\n",
      "UEHitrate: 0.003  edgeHitrate 0.38625 sumHitrate 0.38925  privacy: 0.9065\n",
      "\n",
      "--Time: Sun Sep 26 18:25:36 2021 Episode: 0   Index: 60000   Loss: 0.0 --\n",
      "Reward: [0.19339 0.34369 0.00352] total reward: 0.5406\n",
      "UEHitrate: 0.00292  edgeHitrate 0.38249 sumHitrate 0.38541  privacy: 0.91146\n",
      "\n",
      "--Time: Sun Sep 26 18:29:15 2021 Episode: 0   Index: 70000   Loss: 0.0 --\n",
      "Reward: [0.16718 0.33685 0.00336] total reward: 0.50739\n",
      "UEHitrate: 0.00284  edgeHitrate 0.37485 sumHitrate 0.37769  privacy: 0.91532\n",
      "\n",
      "--Time: Sun Sep 26 18:32:54 2021 Episode: 0   Index: 80000   Loss: 0.0 --\n",
      "Reward: [0.14986 0.33991 0.00337] total reward: 0.49315\n",
      "UEHitrate: 0.00285  edgeHitrate 0.37822 sumHitrate 0.38107  privacy: 0.91458\n",
      "\n",
      "--Time: Sun Sep 26 18:36:34 2021 Episode: 0   Index: 90000   Loss: 0.0 --\n",
      "Reward: [0.13579 0.33805 0.00345] total reward: 0.47729\n",
      "UEHitrate: 0.00293  edgeHitrate 0.3761 sumHitrate 0.37903  privacy: 0.91507\n",
      "\n",
      "--Time: Sun Sep 26 18:40:15 2021 Episode: 0   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.12243 0.33742 0.00339] total reward: 0.46324\n",
      "UEHitrate: 0.00292  edgeHitrate 0.37542 sumHitrate 0.37834  privacy: 0.90999\n",
      "\n",
      "--Time: Sun Sep 26 18:43:54 2021 Episode: 0   Index: 110000   Loss: 0.0 --\n",
      "Reward: [0.10976 0.33772 0.00344] total reward: 0.45092\n",
      "UEHitrate: 0.00291  edgeHitrate 0.37581 sumHitrate 0.37872  privacy: 0.90665\n",
      "\n",
      "--Time: Sun Sep 26 18:47:34 2021 Episode: 0   Index: 120000   Loss: 0.0 --\n",
      "Reward: [0.09844 0.33732 0.00352] total reward: 0.43927\n",
      "UEHitrate: 0.003  edgeHitrate 0.37532 sumHitrate 0.37832  privacy: 0.90902\n",
      "\n",
      "--Time: Sun Sep 26 18:51:13 2021 Episode: 0   Index: 130000   Loss: 0.0 --\n",
      "Reward: [0.08975 0.33368 0.00369] total reward: 0.42711\n",
      "UEHitrate: 0.00321  edgeHitrate 0.37124 sumHitrate 0.37445  privacy: 0.9077\n",
      "\n",
      "--Time: Sun Sep 26 18:55:38 2021 Episode: 0   Index: 140000   Loss: 0.0 --\n",
      "Reward: [0.08029 0.33697 0.00376] total reward: 0.42102\n",
      "UEHitrate: 0.0033  edgeHitrate 0.37498 sumHitrate 0.37828  privacy: 0.9102\n",
      "\n",
      "--Time: Sun Sep 26 19:00:32 2021 Episode: 0   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.07601 0.34065 0.00379] total reward: 0.42046\n",
      "UEHitrate: 0.00338  edgeHitrate 0.37906 sumHitrate 0.38244  privacy: 0.91397\n",
      "\n",
      "--Time: Sun Sep 26 19:05:24 2021 Episode: 0   Index: 160000   Loss: 0.0 --\n",
      "Reward: [0.06969 0.3382  0.00393] total reward: 0.41182\n",
      "UEHitrate: 0.00339  edgeHitrate 0.37638 sumHitrate 0.37977  privacy: 0.91537\n",
      "\n",
      "--Time: Sun Sep 26 19:10:17 2021 Episode: 0   Index: 170000   Loss: 0.0 --\n",
      "Reward: [0.0649  0.34101 0.00396] total reward: 0.40987\n",
      "UEHitrate: 0.00339  edgeHitrate 0.37952 sumHitrate 0.38292  privacy: 0.91594\n",
      "\n",
      "--Time: Sun Sep 26 19:15:15 2021 Episode: 0   Index: 180000   Loss: 0.0 --\n",
      "Reward: [0.05972 0.34316 0.00402] total reward: 0.40691\n",
      "UEHitrate: 0.00339  edgeHitrate 0.38196 sumHitrate 0.38535  privacy: 0.90963\n",
      "\n",
      "--Time: Sun Sep 26 19:20:10 2021 Episode: 0   Index: 190000   Loss: 0.0 --\n",
      "Reward: [0.0553  0.34433 0.00407] total reward: 0.4037\n",
      "UEHitrate: 0.00347  edgeHitrate 0.38323 sumHitrate 0.3867  privacy: 0.90822\n",
      "\n",
      "--Time: Sun Sep 26 19:25:04 2021 Episode: 0   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.05261 0.34472 0.00406] total reward: 0.40139\n",
      "UEHitrate: 0.00344  edgeHitrate 0.38372 sumHitrate 0.38717  privacy: 0.90558\n",
      "\n",
      "--Time: Sun Sep 26 19:29:57 2021 Episode: 0   Index: 210000   Loss: 0.0 --\n",
      "Reward: [0.04922 0.34631 0.00411] total reward: 0.39964\n",
      "UEHitrate: 0.0035  edgeHitrate 0.38551 sumHitrate 0.38901  privacy: 0.8902\n",
      "\n",
      "--Time: Sun Sep 26 19:34:57 2021 Episode: 0   Index: 220000   Loss: 0.0 --\n",
      "Reward: [0.04641 0.3447  0.00416] total reward: 0.39527\n",
      "UEHitrate: 0.00353  edgeHitrate 0.38373 sumHitrate 0.38725  privacy: 0.89274\n",
      "\n",
      "--Time: Sun Sep 26 19:39:53 2021 Episode: 0   Index: 230000   Loss: 0.0 --\n",
      "Reward: [0.04394 0.34342 0.00419] total reward: 0.39155\n",
      "UEHitrate: 0.00358  edgeHitrate 0.38226 sumHitrate 0.38585  privacy: 0.8956\n",
      "\n",
      "--Time: Sun Sep 26 19:44:49 2021 Episode: 0   Index: 240000   Loss: 0.0 --\n",
      "Reward: [0.04116 0.34234 0.00422] total reward: 0.38771\n",
      "UEHitrate: 0.00363  edgeHitrate 0.38106 sumHitrate 0.38469  privacy: 0.89818\n",
      "\n",
      "--Time: Sun Sep 26 19:49:47 2021 Episode: 0   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.03841 0.34268 0.00422] total reward: 0.38531\n",
      "UEHitrate: 0.00367  edgeHitrate 0.38146 sumHitrate 0.38513  privacy: 0.90032\n",
      "\n",
      "--Time: Sun Sep 26 19:54:47 2021 Episode: 0   Index: 260000   Loss: 0.0 --\n",
      "Reward: [0.03651 0.34094 0.00421] total reward: 0.38166\n",
      "UEHitrate: 0.00373  edgeHitrate 0.37953 sumHitrate 0.38326  privacy: 0.90215\n",
      "\n",
      "--Time: Sun Sep 26 19:59:43 2021 Episode: 0   Index: 270000   Loss: 0.0 --\n",
      "Reward: [0.0356  0.33921 0.00427] total reward: 0.37908\n",
      "UEHitrate: 0.00379  edgeHitrate 0.3776 sumHitrate 0.38138  privacy: 0.90417\n",
      "\n",
      "--Time: Sun Sep 26 20:04:39 2021 Episode: 0   Index: 280000   Loss: 0.0 --\n",
      "Reward: [0.03506 0.3392  0.00429] total reward: 0.37854\n",
      "UEHitrate: 0.00382  edgeHitrate 0.37761 sumHitrate 0.38142  privacy: 0.90602\n",
      "\n",
      "--Time: Sun Sep 26 20:09:38 2021 Episode: 0   Index: 290000   Loss: 0.0 --\n",
      "Reward: [0.03372 0.34122 0.00429] total reward: 0.37924\n",
      "UEHitrate: 0.00383  edgeHitrate 0.37988 sumHitrate 0.38371  privacy: 0.90755\n",
      "\n",
      "--Time: Sun Sep 26 20:14:35 2021 Episode: 0   Index: 300000   Loss: 0.0 --\n",
      "Reward: [0.0323  0.34432 0.00427] total reward: 0.38089\n",
      "UEHitrate: 0.00384  edgeHitrate 0.38329 sumHitrate 0.38712  privacy: 0.9087\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Sep 26 20:15:04 2021 Episode: 0   Index: 300982  Loss: 0.0 --\n",
      "Reward: [0.03212 0.34458 0.00427] total reward: 0.38097\n",
      "UEHitrate: 0.00384  edgeHitrate 0.38358 sumHitrate 0.38742  privacy: 0.9088\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.     , 0.38236, 0.42018, 0.40512, 0.39239, 0.38925, 0.38541,\n",
       "        0.37769, 0.38107, 0.37903, 0.37834, 0.37872, 0.37832, 0.37445,\n",
       "        0.37828, 0.38244, 0.37977, 0.38292, 0.38535, 0.3867 , 0.38717,\n",
       "        0.38901, 0.38725, 0.38585, 0.38469, 0.38513, 0.38326, 0.38138,\n",
       "        0.38142, 0.38371, 0.38742, 0.     ]),\n",
       " array([0.     , 0.0014 , 0.00175, 0.0023 , 0.00257, 0.003  , 0.00292,\n",
       "        0.00284, 0.00285, 0.00293, 0.00292, 0.00291, 0.003  , 0.00321,\n",
       "        0.0033 , 0.00338, 0.00339, 0.00339, 0.00339, 0.00347, 0.00344,\n",
       "        0.0035 , 0.00353, 0.00358, 0.00363, 0.00367, 0.00373, 0.00379,\n",
       "        0.00382, 0.00383, 0.00384, 0.     ]),\n",
       " array([0.     , 0.38096, 0.41843, 0.40282, 0.38982, 0.38625, 0.38249,\n",
       "        0.37485, 0.37822, 0.3761 , 0.37542, 0.37581, 0.37532, 0.37124,\n",
       "        0.37498, 0.37906, 0.37638, 0.37952, 0.38196, 0.38323, 0.38372,\n",
       "        0.38551, 0.38373, 0.38226, 0.38106, 0.38146, 0.37953, 0.3776 ,\n",
       "        0.37761, 0.37988, 0.38358, 0.     ]))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1.     , 1.05103, 0.94254, 0.9181 , 0.91442, 0.9065 , 0.91146,\n",
       "       0.91532, 0.91458, 0.91507, 0.90999, 0.90665, 0.90902, 0.9077 ,\n",
       "       0.9102 , 0.91397, 0.91537, 0.91594, 0.90963, 0.90822, 0.90558,\n",
       "       0.8902 , 0.89274, 0.8956 , 0.89818, 0.90032, 0.90215, 0.90417,\n",
       "       0.90602, 0.90755, 0.9088 , 0.     ])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "Frq_hour = torch.zeros((BATCHSIZE,720)).to(device)\n",
    "m = 20\n",
    "\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":1,\"betal\":1}\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "UEHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "edgeHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "privacyReduction = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "i_episode = 0\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    if uit[1] in np.argwhere(ue.lastAction.numpy()):\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    if uit[2]>=m:\n",
    "        test_iterator = Frq_hour[:,uit[2]-m:uit[2]]/Frq_hour[:,uit[2]-m:uit[2]].sum(dim=0)\n",
    "        test_iterator = test_iterator.unsqueeze(2)\n",
    "        #print(test_iterator.sum())\n",
    "        \n",
    "    else:\n",
    "        test_iterator = None\n",
    "    \n",
    "    ue.selectAction(env,uit,model,test_iterator)\n",
    "\n",
    "    for content in np.argwhere(ue.action.numpy()):\n",
    "        Frq_hour[content,uit[2]]+=1\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Rl)\n",
    "    sumReward[2] += float(ue.Ro)\n",
    "    if index % 10000 == 0 :\n",
    "        psi = 0\n",
    "        p = torch.from_numpy(env.p)\n",
    "        for u in UEs:\n",
    "            psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "psi = 0\n",
    "p = torch.from_numpy(env.p)\n",
    "for u in UEs:\n",
    "    psi += torch.log(UEs[u].r[u] * p + (1-UEs[u].r[u]) * (1-p)).sum() / torch.log(UEs[u].v * p + (1-UEs[u].v) * (1-p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"Episode:\",i_episode,\"  Index:\",index,\" Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--Time: Sat Sep 25 12:27:40 2021 Episode: 0   Index: 0   Loss: 0.0 --\n",
      "Reward: [0. 0. 0.] total reward: 0.0\n",
      "UEHitrate: 0.0  edgeHitrate 0.0 sumHitrate 0.0  privacy: 4.02118\n",
      "\n",
      "--Time: Sat Sep 25 12:28:36 2021 Episode: 0   Index: 10000   Loss: 0.0 --\n",
      "Reward: [5.15663e+00 1.03760e-01 2.87000e-03] total reward: 5.26325\n",
      "UEHitrate: 0.0022  edgeHitrate 0.11589 sumHitrate 0.11809  privacy: 3.80547\n",
      "\n",
      "--Time: Sat Sep 25 12:33:02 2021 Episode: 0   Index: 20000   Loss: 0.0 --\n",
      "Reward: [2.8981  0.26103 0.0031 ] total reward: 3.16224\n",
      "UEHitrate: 0.0019  edgeHitrate 0.29059 sumHitrate 0.29249  privacy: 3.11271\n",
      "\n",
      "--Time: Sat Sep 25 12:37:05 2021 Episode: 0   Index: 30000   Loss: 0.0 --\n",
      "Reward: [2.0572  0.28613 0.00333] total reward: 2.34667\n",
      "UEHitrate: 0.0022  edgeHitrate 0.31866 sumHitrate 0.32086  privacy: 2.75216\n",
      "\n",
      "--Time: Sat Sep 25 12:41:16 2021 Episode: 0   Index: 40000   Loss: 0.0 --\n",
      "Reward: [1.57752 0.29479 0.00335] total reward: 1.87566\n",
      "UEHitrate: 0.00235  edgeHitrate 0.32822 sumHitrate 0.33057  privacy: 2.54478\n",
      "\n",
      "--Time: Sat Sep 25 12:45:56 2021 Episode: 0   Index: 50000   Loss: 0.0 --\n",
      "Reward: [1.27094 0.3045  0.00352] total reward: 1.57896\n",
      "UEHitrate: 0.00262  edgeHitrate 0.33899 sumHitrate 0.34161  privacy: 2.39712\n",
      "\n",
      "--Time: Sat Sep 25 12:50:23 2021 Episode: 0   Index: 60000   Loss: 0.0 --\n",
      "Reward: [1.07662 0.31012 0.00357] total reward: 1.39031\n",
      "UEHitrate: 0.00248  edgeHitrate 0.34518 sumHitrate 0.34766  privacy: 2.28372\n",
      "\n",
      "--Time: Sat Sep 25 12:54:33 2021 Episode: 0   Index: 70000   Loss: 0.0 --\n",
      "Reward: [0.92521 0.30912 0.00339] total reward: 1.23772\n",
      "UEHitrate: 0.00241  edgeHitrate 0.34402 sumHitrate 0.34644  privacy: 2.19842\n",
      "\n",
      "--Time: Sat Sep 25 12:58:41 2021 Episode: 0   Index: 80000   Loss: 0.0 --\n",
      "Reward: [0.81419 0.31674 0.00338] total reward: 1.13431\n",
      "UEHitrate: 0.0024  edgeHitrate 0.35246 sumHitrate 0.35486  privacy: 2.13151\n",
      "\n",
      "--Time: Sat Sep 25 13:03:11 2021 Episode: 0   Index: 90000   Loss: 0.0 --\n",
      "Reward: [0.72777 0.31788 0.00347] total reward: 1.04912\n",
      "UEHitrate: 0.00249  edgeHitrate 0.35366 sumHitrate 0.35615  privacy: 2.06743\n",
      "\n",
      "--Time: Sat Sep 25 13:07:45 2021 Episode: 0   Index: 100000   Loss: 0.0 --\n",
      "Reward: [0.65516 0.32014 0.0034 ] total reward: 0.97869\n",
      "UEHitrate: 0.00249  edgeHitrate 0.35619 sumHitrate 0.35868  privacy: 2.00378\n",
      "\n",
      "--Time: Sat Sep 25 13:12:11 2021 Episode: 0   Index: 110000   Loss: 0.0 --\n",
      "Reward: [0.59616 0.32258 0.00344] total reward: 0.92218\n",
      "UEHitrate: 0.00251  edgeHitrate 0.35899 sumHitrate 0.3615  privacy: 1.95859\n",
      "\n",
      "--Time: Sat Sep 25 13:16:53 2021 Episode: 0   Index: 120000   Loss: 0.0 --\n",
      "Reward: [0.54461 0.324   0.00351] total reward: 0.87211\n",
      "UEHitrate: 0.00257  edgeHitrate 0.36051 sumHitrate 0.36309  privacy: 1.92519\n",
      "\n",
      "--Time: Sat Sep 25 13:21:20 2021 Episode: 0   Index: 130000   Loss: 0.0 --\n",
      "Reward: [0.50256 0.32185 0.00367] total reward: 0.82808\n",
      "UEHitrate: 0.00275  edgeHitrate 0.3581 sumHitrate 0.36084  privacy: 1.8899\n",
      "\n",
      "--Time: Sat Sep 25 13:25:38 2021 Episode: 0   Index: 140000   Loss: 0.0 --\n",
      "Reward: [0.46446 0.32665 0.00371] total reward: 0.79482\n",
      "UEHitrate: 0.0028  edgeHitrate 0.36353 sumHitrate 0.36633  privacy: 1.86248\n",
      "\n",
      "--Time: Sat Sep 25 13:30:01 2021 Episode: 0   Index: 150000   Loss: 0.0 --\n",
      "Reward: [0.4344  0.33153 0.00374] total reward: 0.76968\n",
      "UEHitrate: 0.00287  edgeHitrate 0.36894 sumHitrate 0.37181  privacy: 1.83901\n",
      "\n",
      "--Time: Sat Sep 25 13:34:19 2021 Episode: 0   Index: 160000   Loss: 0.0 --\n",
      "Reward: [0.40779 0.33002 0.00389] total reward: 0.74169\n",
      "UEHitrate: 0.00288  edgeHitrate 0.3673 sumHitrate 0.37019  privacy: 1.81177\n",
      "\n",
      "--Time: Sat Sep 25 13:38:55 2021 Episode: 0   Index: 170000   Loss: 0.0 --\n",
      "Reward: [0.38355 0.33369 0.00392] total reward: 0.72116\n",
      "UEHitrate: 0.00291  edgeHitrate 0.3714 sumHitrate 0.37431  privacy: 1.79281\n",
      "\n",
      "--Time: Sat Sep 25 13:43:03 2021 Episode: 0   Index: 180000   Loss: 0.0 --\n",
      "Reward: [0.36082 0.33657 0.00397] total reward: 0.70136\n",
      "UEHitrate: 0.0029  edgeHitrate 0.37465 sumHitrate 0.37755  privacy: 1.77253\n",
      "\n",
      "--Time: Sat Sep 25 13:47:10 2021 Episode: 0   Index: 190000   Loss: 0.0 --\n",
      "Reward: [0.34099 0.33877 0.00401] total reward: 0.68377\n",
      "UEHitrate: 0.00298  edgeHitrate 0.37708 sumHitrate 0.38007  privacy: 1.75582\n",
      "\n",
      "--Time: Sat Sep 25 13:51:14 2021 Episode: 0   Index: 200000   Loss: 0.0 --\n",
      "Reward: [0.32394 0.33978 0.004  ] total reward: 0.66771\n",
      "UEHitrate: 0.00297  edgeHitrate 0.37826 sumHitrate 0.38123  privacy: 1.73785\n",
      "\n",
      "--Time: Sat Sep 25 13:55:16 2021 Episode: 0   Index: 210000   Loss: 0.0 --\n",
      "Reward: [0.3071  0.34185 0.00405] total reward: 0.65301\n",
      "UEHitrate: 0.00304  edgeHitrate 0.38057 sumHitrate 0.38361  privacy: 1.72101\n",
      "\n",
      "--Time: Sat Sep 25 13:59:45 2021 Episode: 0   Index: 220000   Loss: 0.0 --\n",
      "Reward: [0.2931  0.34072 0.00409] total reward: 0.63792\n",
      "UEHitrate: 0.00306  edgeHitrate 0.37932 sumHitrate 0.38238  privacy: 1.70723\n",
      "\n",
      "--Time: Sat Sep 25 14:04:09 2021 Episode: 0   Index: 230000   Loss: 0.0 --\n",
      "Reward: [0.28066 0.33997 0.00412] total reward: 0.62474\n",
      "UEHitrate: 0.00311  edgeHitrate 0.37845 sumHitrate 0.38156  privacy: 1.69112\n",
      "\n",
      "--Time: Sat Sep 25 14:08:39 2021 Episode: 0   Index: 240000   Loss: 0.0 --\n",
      "Reward: [0.26797 0.33929 0.00414] total reward: 0.61141\n",
      "UEHitrate: 0.00313  edgeHitrate 0.3777 sumHitrate 0.38084  privacy: 1.67946\n",
      "\n",
      "--Time: Sat Sep 25 14:13:21 2021 Episode: 0   Index: 250000   Loss: 0.0 --\n",
      "Reward: [0.25645 0.34008 0.00414] total reward: 0.60067\n",
      "UEHitrate: 0.00318  edgeHitrate 0.37861 sumHitrate 0.38179  privacy: 1.66804\n",
      "\n",
      "--Time: Sat Sep 25 14:17:22 2021 Episode: 0   Index: 260000   Loss: 0.0 --\n",
      "Reward: [0.2465  0.33866 0.00413] total reward: 0.58929\n",
      "UEHitrate: 0.00323  edgeHitrate 0.37702 sumHitrate 0.38024  privacy: 1.65599\n",
      "\n",
      "--Time: Sat Sep 25 14:22:04 2021 Episode: 0   Index: 270000   Loss: 0.0 --\n",
      "Reward: [0.23712 0.33716 0.00419] total reward: 0.57846\n",
      "UEHitrate: 0.00329  edgeHitrate 0.37535 sumHitrate 0.37864  privacy: 1.6432\n",
      "\n",
      "--Time: Sat Sep 25 14:26:32 2021 Episode: 0   Index: 280000   Loss: 0.0 --\n",
      "Reward: [0.2299  0.33745 0.0042 ] total reward: 0.57155\n",
      "UEHitrate: 0.00331  edgeHitrate 0.37568 sumHitrate 0.379  privacy: 1.63297\n",
      "\n",
      "--Time: Sat Sep 25 14:30:57 2021 Episode: 0   Index: 290000   Loss: 0.0 --\n",
      "Reward: [0.2218  0.33979 0.0042 ] total reward: 0.56579\n",
      "UEHitrate: 0.00332  edgeHitrate 0.37831 sumHitrate 0.38163  privacy: 1.62498\n",
      "\n",
      "--Time: Sat Sep 25 14:35:21 2021 Episode: 0   Index: 300000   Loss: 0.0 --\n",
      "Reward: [0.21383 0.34311 0.00418] total reward: 0.56113\n",
      "UEHitrate: 0.00333  edgeHitrate 0.38197 sumHitrate 0.3853  privacy: 1.61916\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sat Sep 25 14:35:50 2021 Episode: 0   Index: 300982  Loss: 0.0 --\n",
      "Reward: [0.21301 0.34341 0.00418] total reward: 0.56061\n",
      "UEHitrate: 0.00333  edgeHitrate 0.3823 sumHitrate 0.38563  privacy: 1.61867\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "(array([0.     , 0.11809, 0.29249, 0.32086, 0.33057, 0.34161, 0.34766,\n",
       "        0.34644, 0.35486, 0.35615, 0.35868, 0.3615 , 0.36309, 0.36084,\n",
       "        0.36633, 0.37181, 0.37019, 0.37431, 0.37755, 0.38007, 0.38123,\n",
       "        0.38361, 0.38238, 0.38156, 0.38084, 0.38179, 0.38024, 0.37864,\n",
       "        0.379  , 0.38163, 0.38563, 0.     ]),\n",
       " array([0.     , 0.0022 , 0.0019 , 0.0022 , 0.00235, 0.00262, 0.00248,\n",
       "        0.00241, 0.0024 , 0.00249, 0.00249, 0.00251, 0.00257, 0.00275,\n",
       "        0.0028 , 0.00287, 0.00288, 0.00291, 0.0029 , 0.00298, 0.00297,\n",
       "        0.00304, 0.00306, 0.00311, 0.00313, 0.00318, 0.00323, 0.00329,\n",
       "        0.00331, 0.00332, 0.00333, 0.     ]),\n",
       " array([0.     , 0.11589, 0.29059, 0.31866, 0.32822, 0.33899, 0.34518,\n",
       "        0.34402, 0.35246, 0.35366, 0.35619, 0.35899, 0.36051, 0.3581 ,\n",
       "        0.36353, 0.36894, 0.3673 , 0.3714 , 0.37465, 0.37708, 0.37826,\n",
       "        0.38057, 0.37932, 0.37845, 0.3777 , 0.37861, 0.37702, 0.37535,\n",
       "        0.37568, 0.37831, 0.3823 , 0.     ]))"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "privacyReduction"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "array([4.02118, 3.80547, 3.11271, 2.75216, 2.54478, 2.39712, 2.28372,\n",
       "       2.19842, 2.13151, 2.06743, 2.00378, 1.95859, 1.92519, 1.8899 ,\n",
       "       1.86248, 1.83901, 1.81177, 1.79281, 1.77253, 1.75582, 1.73785,\n",
       "       1.72101, 1.70723, 1.69112, 1.67946, 1.66804, 1.65599, 1.6432 ,\n",
       "       1.63297, 1.62498, 1.61867, 0.     ])"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#list_size = [int(BATCHSIZE * ratio) for ratio in [0.001,0.0025,0.005,0.0075,0.01,0.025,0.05,0.075,0.1,0.25]]\n",
    "list_size = [int(BATCHSIZE * ratio) for ratio in [0.0015]]\n",
    "print(list_size)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[15]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "INPUT_DIM = 1\n",
    "OUTPUT_DIM = 1\n",
    "ENC_EMB_DIM = 0\n",
    "DEC_EMB_DIM = 0\n",
    "HID_DIM = 64\n",
    "#HID_DIM = 64\n",
    "\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): LSTM(1, 64, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn): LSTM(1, 64, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 100,929 trainable parameters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch[0].permute(1,0,2).float().to(device)\n",
    "        trg = batch[1].permute(1,0,2).float().to(device)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        #print(src.shape,trg.shape)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        #print(output.shape,trg.shape)\n",
    "        #output_dim = output.shape[-1]\n",
    "        \n",
    "        batchsize = trg.shape[1]\n",
    "        \n",
    "        #output = output[1:].view(-1)\n",
    "        output = output.permute(1,0,2).reshape(batchsize,-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #trg = trg[1:].view(-1)\n",
    "        trg = trg.permute(1,0,2).reshape(batchsize,-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        if i%30 == 0:\n",
    "            print(\"train epoch_loss\",epoch_loss)\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch[0].permute(1,0,2).float().cuda()\n",
    "            trg = batch[1].permute(1,0,2).float().cuda()\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #$print(output[1],trg[1])\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            batchsize = trg.shape[1]\n",
    "            \n",
    "            #output = output[1:].view(-1)\n",
    "            output = output.permute(1,0,2).reshape(batchsize,-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #trg = trg[1:].view(-1)\n",
    "            trg = trg.permute(1,0,2).reshape(batchsize,-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            if i%30 == 0:\n",
    "                print(\"vali epoch_loss\",epoch_loss)\n",
    "    return epoch_loss / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self,data = None,src_len = 20,trg_len=26):\n",
    "        self.data = data\n",
    "        self.data_lengths = len(data)\n",
    "        self.src_len = src_len\n",
    "        self.trg_len = trg_len\n",
    "    def __getitem__(self,index):\n",
    "        data=self.data[index]\n",
    "        src_data = data[0:self.src_len]\n",
    "        trg_data = data[self.src_len:self.trg_len+self.src_len]\n",
    "        return src_data,trg_data\n",
    "    def __len__(self):\n",
    "        return self.data_lengths\n",
    "\n",
    "def dataset_iter(trDataX ,trDataY):\n",
    "    trData = pd.concat([trDataX, trDataY], axis=1).to_numpy()\n",
    "    trData = trData[:,:,np.newaxis]\n",
    "    train_data, validate_data = np.split(trData, [int(.5*len(trData))])\n",
    "    train_data_loader = torch.utils.data.DataLoader(dataset=Dataset(train_data,src_len = 20,trg_len=26),batch_size=BATCHSIZE)\n",
    "    validate_data_loader = torch.utils.data.DataLoader(dataset=Dataset(validate_data,src_len = 20,trg_len=26),batch_size=BATCHSIZE)\n",
    "    return train_data_loader,validate_data_loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "N_EPOCHS = 30\n",
    "CLIP = 1\n",
    "FOLD = 1\n",
    "CITY = 0\n",
    "BATCHSIZE = 40000\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "trDataX  = pd.read_csv(\"fold{}_city{}_trainX.csv\".format(FOLD,CITY),header=None)\n",
    "trDataY  = pd.read_csv(\"fold{}_city{}_trainY.csv\".format(FOLD,CITY),header=None)\n",
    "train_iterator,valid_iterator = dataset_iter(trDataX,trDataY)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    #train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    #valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'fold{}-city{}-model.pt'.format(FOLD,CITY))\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train epoch_loss 7026.6591796875\n",
      "train epoch_loss 21461.371819734573\n",
      "train epoch_loss 21636.382810115814\n",
      "train epoch_loss 21786.870680868626\n",
      "vali epoch_loss 1.3750718832015991\n",
      "vali epoch_loss 42.85216295719147\n",
      "vali epoch_loss 83.69077098369598\n",
      "vali epoch_loss 123.66925489902496\n",
      "Epoch: 01 | Time: 4m 40s\n",
      "\tTrain Loss: 224.676 | Train PPL: 37617647900842016898324570760897361728655058618676675688118049129584683898043946836016248823742464.000\n",
      "\t Val. Loss: 1.351 |  Val. PPL:   3.863\n",
      "train epoch_loss 1.5785493850708008\n",
      "train epoch_loss 60.31514897942543\n",
      "train epoch_loss 81.31142421066761\n",
      "train epoch_loss 117.85134994983673\n",
      "vali epoch_loss 0.5998155474662781\n",
      "vali epoch_loss 18.819255888462067\n",
      "vali epoch_loss 36.400178372859955\n",
      "vali epoch_loss 53.12092709541321\n",
      "Epoch: 02 | Time: 4m 38s\n",
      "\tTrain Loss: 1.258 | Train PPL:   3.519\n",
      "\t Val. Loss: 0.580 |  Val. PPL:   1.786\n",
      "train epoch_loss 0.674651563167572\n",
      "train epoch_loss 41.43838146328926\n",
      "train epoch_loss 110.40423695743084\n",
      "train epoch_loss 181.32895751297474\n",
      "vali epoch_loss 0.9262598752975464\n",
      "vali epoch_loss 28.939022421836853\n",
      "vali epoch_loss 56.31327563524246\n",
      "vali epoch_loss 82.82735830545425\n",
      "Epoch: 03 | Time: 4m 39s\n",
      "\tTrain Loss: 1.900 | Train PPL:   6.685\n",
      "\t Val. Loss: 0.905 |  Val. PPL:   2.471\n",
      "train epoch_loss 0.9966225624084473\n",
      "train epoch_loss 56.50669753551483\n",
      "train epoch_loss 87.79147633910179\n",
      "train epoch_loss 118.9459627121687\n",
      "vali epoch_loss 0.11830209940671921\n",
      "vali epoch_loss 3.892317980527878\n",
      "vali epoch_loss 7.02783627063036\n",
      "vali epoch_loss 9.303201019763947\n",
      "Epoch: 04 | Time: 4m 38s\n",
      "\tTrain Loss: 1.250 | Train PPL:   3.491\n",
      "\t Val. Loss: 0.101 |  Val. PPL:   1.106\n",
      "train epoch_loss 0.18026483058929443\n",
      "train epoch_loss 45.988467037677765\n",
      "train epoch_loss 104.13407836854458\n",
      "train epoch_loss 134.60526306182146\n",
      "vali epoch_loss 0.331094354391098\n",
      "vali epoch_loss 10.488887429237366\n",
      "vali epoch_loss 20.008173406124115\n",
      "vali epoch_loss 28.667302757501602\n",
      "Epoch: 05 | Time: 4m 39s\n",
      "\tTrain Loss: 1.415 | Train PPL:   4.116\n",
      "\t Val. Loss: 0.313 |  Val. PPL:   1.367\n",
      "train epoch_loss 0.3860422968864441\n",
      "train epoch_loss 64.27782368659973\n",
      "train epoch_loss 115.20236539840698\n",
      "train epoch_loss 129.1232629418373\n",
      "vali epoch_loss 3.8100156784057617\n",
      "vali epoch_loss 118.33549213409424\n",
      "vali epoch_loss 232.22242307662964\n",
      "vali epoch_loss 345.24913120269775\n",
      "Epoch: 06 | Time: 4m 37s\n",
      "\tTrain Loss: 1.435 | Train PPL:   4.201\n",
      "\t Val. Loss: 3.774 |  Val. PPL:  43.542\n",
      "train epoch_loss 3.856611490249634\n",
      "train epoch_loss 36.340931579470634\n",
      "train epoch_loss 56.04370056092739\n",
      "train epoch_loss 72.21836599707603\n",
      "vali epoch_loss 0.8031560182571411\n",
      "vali epoch_loss 25.122868478298187\n",
      "vali epoch_loss 48.80401372909546\n",
      "vali epoch_loss 71.6249098777771\n",
      "Epoch: 07 | Time: 4m 39s\n",
      "\tTrain Loss: 0.773 | Train PPL:   2.167\n",
      "\t Val. Loss: 0.782 |  Val. PPL:   2.187\n",
      "train epoch_loss 0.8554885387420654\n",
      "train epoch_loss 61.77888062596321\n",
      "train epoch_loss 112.77047614008188\n",
      "train epoch_loss 120.46271128207445\n",
      "vali epoch_loss 1.1233940124511719\n",
      "vali epoch_loss 35.05023896694183\n",
      "vali epoch_loss 68.33852505683899\n",
      "vali epoch_loss 100.7665708065033\n",
      "Epoch: 08 | Time: 4m 37s\n",
      "\tTrain Loss: 1.272 | Train PPL:   3.568\n",
      "\t Val. Loss: 1.101 |  Val. PPL:   3.007\n",
      "train epoch_loss 1.1717263460159302\n",
      "train epoch_loss 36.36652626097202\n",
      "train epoch_loss 47.7217862457037\n",
      "train epoch_loss 69.46558356285095\n",
      "vali epoch_loss 1.7942187786102295\n",
      "vali epoch_loss 55.845800280570984\n",
      "vali epoch_loss 109.25882911682129\n",
      "vali epoch_loss 161.8116351366043\n",
      "Epoch: 09 | Time: 4m 39s\n",
      "\tTrain Loss: 0.742 | Train PPL:   2.100\n",
      "\t Val. Loss: 1.768 |  Val. PPL:   5.861\n",
      "train epoch_loss 1.8441500663757324\n",
      "train epoch_loss 19.936563923954964\n",
      "train epoch_loss 50.44252385944128\n",
      "train epoch_loss 62.84145901352167\n",
      "vali epoch_loss 0.2396751046180725\n",
      "vali epoch_loss 7.6549378633499146\n",
      "vali epoch_loss 14.431652888655663\n",
      "vali epoch_loss 20.348154962062836\n",
      "Epoch: 10 | Time: 4m 36s\n",
      "\tTrain Loss: 0.669 | Train PPL:   1.952\n",
      "\t Val. Loss: 0.222 |  Val. PPL:   1.248\n",
      "train epoch_loss 0.29638779163360596\n",
      "train epoch_loss 14.894925326108932\n",
      "train epoch_loss 27.351700007915497\n",
      "train epoch_loss 35.148579105734825\n",
      "vali epoch_loss 0.11767047643661499\n",
      "vali epoch_loss 3.8727855682373047\n",
      "vali epoch_loss 6.989359423518181\n",
      "vali epoch_loss 9.24573539197445\n",
      "Epoch: 11 | Time: 4m 39s\n",
      "\tTrain Loss: 0.380 | Train PPL:   1.462\n",
      "\t Val. Loss: 0.100 |  Val. PPL:   1.106\n",
      "train epoch_loss 0.17367565631866455\n",
      "train epoch_loss 29.976163312792778\n",
      "train epoch_loss 44.96180857717991\n",
      "train epoch_loss 64.64495564997196\n",
      "vali epoch_loss 1.030928134918213\n",
      "vali epoch_loss 32.183738589286804\n",
      "vali epoch_loss 62.6980362534523\n",
      "vali epoch_loss 92.35219502449036\n",
      "Epoch: 12 | Time: 4m 38s\n",
      "\tTrain Loss: 0.701 | Train PPL:   2.016\n",
      "\t Val. Loss: 1.009 |  Val. PPL:   2.743\n",
      "train epoch_loss 1.0787339210510254\n",
      "train epoch_loss 20.334199607372284\n",
      "train epoch_loss 39.70045205950737\n",
      "train epoch_loss 64.85380057990551\n",
      "vali epoch_loss 0.1078459620475769\n",
      "vali epoch_loss 3.568201594054699\n",
      "vali epoch_loss 6.390036076307297\n",
      "vali epoch_loss 8.351711049675941\n",
      "Epoch: 13 | Time: 4m 40s\n",
      "\tTrain Loss: 0.679 | Train PPL:   1.971\n",
      "\t Val. Loss: 0.091 |  Val. PPL:   1.095\n",
      "train epoch_loss 0.16397406160831451\n",
      "train epoch_loss 13.974960133433342\n",
      "train epoch_loss 25.46775385737419\n",
      "train epoch_loss 33.27849715203047\n",
      "vali epoch_loss 0.5007672309875488\n",
      "vali epoch_loss 15.748724460601807\n",
      "vali epoch_loss 30.35818886756897\n",
      "vali epoch_loss 44.10755956172943\n",
      "Epoch: 14 | Time: 4m 37s\n",
      "\tTrain Loss: 0.358 | Train PPL:   1.430\n",
      "\t Val. Loss: 0.482 |  Val. PPL:   1.619\n",
      "train epoch_loss 0.5520063638687134\n",
      "train epoch_loss 12.630281209945679\n",
      "train epoch_loss 25.424199238419533\n",
      "train epoch_loss 105.35354829579592\n",
      "vali epoch_loss 0.3226662874221802\n",
      "vali epoch_loss 10.227612376213074\n",
      "vali epoch_loss 19.494048327207565\n",
      "vali epoch_loss 27.90036889910698\n",
      "Epoch: 15 | Time: 4m 40s\n",
      "\tTrain Loss: 1.162 | Train PPL:   3.196\n",
      "\t Val. Loss: 0.304 |  Val. PPL:   1.356\n",
      "train epoch_loss 0.3760868012905121\n",
      "train epoch_loss 16.440250277519226\n",
      "train epoch_loss 57.72258222103119\n",
      "train epoch_loss 67.45217791944742\n",
      "vali epoch_loss 0.24112826585769653\n",
      "vali epoch_loss 7.699880212545395\n",
      "vali epoch_loss 14.520159393548965\n",
      "vali epoch_loss 20.480396330356598\n",
      "Epoch: 16 | Time: 4m 37s\n",
      "\tTrain Loss: 0.740 | Train PPL:   2.095\n",
      "\t Val. Loss: 0.223 |  Val. PPL:   1.250\n",
      "train epoch_loss 0.2936520576477051\n",
      "train epoch_loss 15.69606427848339\n",
      "train epoch_loss 69.55162657797337\n",
      "train epoch_loss 77.69378580898046\n",
      "vali epoch_loss 0.27321821451187134\n",
      "vali epoch_loss 8.694498658180237\n",
      "vali epoch_loss 16.47742936015129\n",
      "vali epoch_loss 23.40059918165207\n",
      "Epoch: 17 | Time: 4m 40s\n",
      "\tTrain Loss: 0.815 | Train PPL:   2.260\n",
      "\t Val. Loss: 0.255 |  Val. PPL:   1.291\n",
      "train epoch_loss 0.32120007276535034\n",
      "train epoch_loss 11.524225652217865\n",
      "train epoch_loss 26.98222866654396\n",
      "train epoch_loss 126.51170393824577\n",
      "vali epoch_loss 0.2076999545097351\n",
      "vali epoch_loss 6.6639095693826675\n",
      "vali epoch_loss 12.481369376182556\n",
      "vali epoch_loss 17.43834763765335\n",
      "Epoch: 18 | Time: 4m 38s\n",
      "\tTrain Loss: 1.388 | Train PPL:   4.007\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n",
      "train epoch_loss 0.27086958289146423\n",
      "train epoch_loss 21.441187769174576\n",
      "train epoch_loss 42.727245941758156\n",
      "train epoch_loss 55.04771238565445\n",
      "vali epoch_loss 0.23373249173164368\n",
      "vali epoch_loss 7.4708782732486725\n",
      "vali epoch_loss 14.069309800863266\n",
      "vali epoch_loss 19.807318657636642\n",
      "Epoch: 19 | Time: 4m 40s\n",
      "\tTrain Loss: 0.576 | Train PPL:   1.779\n",
      "\t Val. Loss: 0.216 |  Val. PPL:   1.241\n",
      "train epoch_loss 0.29037541151046753\n",
      "train epoch_loss 8.934713587164879\n",
      "train epoch_loss 15.822585545480251\n",
      "train epoch_loss 19.719234190881252\n",
      "vali epoch_loss 0.11088521778583527\n",
      "vali epoch_loss 3.662225842475891\n",
      "vali epoch_loss 6.575086884200573\n",
      "vali epoch_loss 8.6280970685184\n",
      "Epoch: 20 | Time: 4m 37s\n",
      "\tTrain Loss: 0.213 | Train PPL:   1.237\n",
      "\t Val. Loss: 0.094 |  Val. PPL:   1.098\n",
      "train epoch_loss 0.16333593428134918\n",
      "train epoch_loss 8.842158570885658\n",
      "train epoch_loss 23.908724769949913\n",
      "train epoch_loss 33.51555170118809\n",
      "vali epoch_loss 0.10949119925498962\n",
      "vali epoch_loss 3.618630699813366\n",
      "vali epoch_loss 6.489540360867977\n",
      "vali epoch_loss 8.501187983900309\n",
      "Epoch: 21 | Time: 4m 38s\n",
      "\tTrain Loss: 0.361 | Train PPL:   1.434\n",
      "\t Val. Loss: 0.092 |  Val. PPL:   1.097\n",
      "train epoch_loss 0.15658238530158997\n",
      "train epoch_loss 16.815067559480667\n",
      "train epoch_loss 27.958064049482346\n",
      "train epoch_loss 57.36575222015381\n",
      "vali epoch_loss 0.5052874088287354\n",
      "vali epoch_loss 15.888313889503479\n",
      "vali epoch_loss 30.633106738328934\n",
      "vali epoch_loss 44.518612921237946\n",
      "Epoch: 22 | Time: 4m 37s\n",
      "\tTrain Loss: 0.665 | Train PPL:   1.944\n",
      "\t Val. Loss: 0.486 |  Val. PPL:   1.626\n",
      "train epoch_loss 0.5241251587867737\n",
      "train epoch_loss 12.262395411729813\n",
      "train epoch_loss 34.19167157262564\n",
      "train epoch_loss 43.66523353010416\n",
      "vali epoch_loss 0.24407105147838593\n",
      "vali epoch_loss 7.790414497256279\n",
      "vali epoch_loss 14.698654130101204\n",
      "vali epoch_loss 20.74791230261326\n",
      "Epoch: 23 | Time: 4m 39s\n",
      "\tTrain Loss: 0.482 | Train PPL:   1.619\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.254\n",
      "train epoch_loss 0.27641797065734863\n",
      "train epoch_loss 19.375138714909554\n",
      "train epoch_loss 33.43418326228857\n",
      "train epoch_loss 45.09287963062525\n",
      "vali epoch_loss 0.11210751533508301\n",
      "vali epoch_loss 3.6996005922555923\n",
      "vali epoch_loss 6.648944579064846\n",
      "vali epoch_loss 8.739202871918678\n",
      "Epoch: 24 | Time: 4m 37s\n",
      "\tTrain Loss: 0.477 | Train PPL:   1.612\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "train epoch_loss 0.15843631327152252\n",
      "train epoch_loss 18.246073737740517\n",
      "train epoch_loss 33.65356355905533\n",
      "train epoch_loss 39.00534489750862\n",
      "vali epoch_loss 0.2436804175376892\n",
      "vali epoch_loss 7.778045579791069\n",
      "vali epoch_loss 14.674503400921822\n",
      "vali epoch_loss 20.712415486574173\n",
      "Epoch: 25 | Time: 4m 39s\n",
      "\tTrain Loss: 0.418 | Train PPL:   1.519\n",
      "\t Val. Loss: 0.226 |  Val. PPL:   1.253\n",
      "train epoch_loss 0.27200713753700256\n",
      "train epoch_loss 11.331189408898354\n",
      "train epoch_loss 20.47865478694439\n",
      "train epoch_loss 25.107427701354027\n",
      "vali epoch_loss 0.17782330513000488\n",
      "vali epoch_loss 5.736198782920837\n",
      "vali epoch_loss 10.656898647546768\n",
      "vali epoch_loss 14.719550043344498\n",
      "Epoch: 26 | Time: 4m 38s\n",
      "\tTrain Loss: 0.279 | Train PPL:   1.321\n",
      "\t Val. Loss: 0.160 |  Val. PPL:   1.174\n",
      "train epoch_loss 0.2062831073999405\n",
      "train epoch_loss 54.064570903778076\n",
      "train epoch_loss 84.44681122899055\n",
      "train epoch_loss 111.00611461699009\n",
      "vali epoch_loss 0.5737996101379395\n",
      "vali epoch_loss 18.01198846101761\n",
      "vali epoch_loss 34.812023878097534\n",
      "vali epoch_loss 50.75301551818848\n",
      "Epoch: 27 | Time: 4m 40s\n",
      "\tTrain Loss: 1.197 | Train PPL:   3.311\n",
      "\t Val. Loss: 0.554 |  Val. PPL:   1.741\n",
      "train epoch_loss 0.5791473388671875\n",
      "train epoch_loss 31.211638629436493\n",
      "train epoch_loss 49.78172507882118\n",
      "train epoch_loss 58.46992986649275\n",
      "vali epoch_loss 0.28241217136383057\n",
      "vali epoch_loss 8.97898730635643\n",
      "vali epoch_loss 17.037399157881737\n",
      "vali epoch_loss 24.236747056245804\n",
      "Epoch: 28 | Time: 4m 37s\n",
      "\tTrain Loss: 0.633 | Train PPL:   1.884\n",
      "\t Val. Loss: 0.264 |  Val. PPL:   1.303\n",
      "train epoch_loss 0.31287115812301636\n",
      "train epoch_loss 11.637789115309715\n",
      "train epoch_loss 27.56987127661705\n",
      "train epoch_loss 39.32846002280712\n",
      "vali epoch_loss 0.10724765807390213\n",
      "vali epoch_loss 3.5487039014697075\n",
      "vali epoch_loss 6.352118372917175\n",
      "vali epoch_loss 8.296757519245148\n",
      "Epoch: 29 | Time: 4m 40s\n",
      "\tTrain Loss: 0.413 | Train PPL:   1.511\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "train epoch_loss 0.14985135197639465\n",
      "train epoch_loss 11.391269624233246\n",
      "train epoch_loss 29.066094756126404\n",
      "train epoch_loss 60.25092834979296\n",
      "vali epoch_loss 0.30294269323349\n",
      "vali epoch_loss 9.613481491804123\n",
      "vali epoch_loss 18.28779637813568\n",
      "vali epoch_loss 26.10686954855919\n",
      "Epoch: 30 | Time: 4m 37s\n",
      "\tTrain Loss: 0.636 | Train PPL:   1.888\n",
      "\t Val. Loss: 0.285 |  Val. PPL:   1.329\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "INPUT_DIM = 1\n",
    "OUTPUT_DIM = 1\n",
    "ENC_EMB_DIM = 0\n",
    "DEC_EMB_DIM = 0\n",
    "HID_DIM = 64\n",
    "#HID_DIM = 64\n",
    "\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (rnn): LSTM(1, 64, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (rnn): LSTM(1, 64, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "#TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has 100,929 trainable parameters\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        \n",
    "        src = batch[0].permute(1,0,2).float().to(device)\n",
    "        trg = batch[1].permute(1,0,2).float().to(device)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        #print(src.shape,trg.shape)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, trg)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        #print(output.shape,trg.shape)\n",
    "        #output_dim = output.shape[-1]\n",
    "        \n",
    "        batchsize = trg.shape[1]\n",
    "        \n",
    "        #output = output[1:].view(-1)\n",
    "        output = output.permute(1,0,2).reshape(batchsize,-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #trg = trg[1:].view(-1)\n",
    "        trg = trg.permute(1,0,2).reshape(batchsize,-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        if i%30 == 0:\n",
    "            print(\"train epoch_loss\",epoch_loss)\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch[0].permute(1,0,2).float().cuda()\n",
    "            trg = batch[1].permute(1,0,2).float().cuda()\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "            #$print(output[1],trg[1])\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            batchsize = trg.shape[1]\n",
    "            \n",
    "            #output = output[1:].view(-1)\n",
    "            output = output.permute(1,0,2).reshape(batchsize,-1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #trg = trg[1:].view(-1)\n",
    "            trg = trg.permute(1,0,2).reshape(batchsize,-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            if i%30 == 0:\n",
    "                print(\"vali epoch_loss\",epoch_loss)\n",
    "    return epoch_loss / len(iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self,data = None,src_len = 20,trg_len=26):\n",
    "        self.data = data\n",
    "        self.data_lengths = len(data)\n",
    "        self.src_len = src_len\n",
    "        self.trg_len = trg_len\n",
    "    def __getitem__(self,index):\n",
    "        data=self.data[index]\n",
    "        src_data = data[0:self.src_len]\n",
    "        trg_data = data[self.src_len:self.trg_len+self.src_len]\n",
    "        return src_data,trg_data\n",
    "    def __len__(self):\n",
    "        return self.data_lengths\n",
    "\n",
    "def dataset_iter(trDataX ,trDataY):\n",
    "    trData = pd.concat([trDataX, trDataY], axis=1).to_numpy()\n",
    "    trData = trData[:,:,np.newaxis]\n",
    "    train_data, validate_data = np.split(trData, [int(.5*len(trData))])\n",
    "    train_data_loader = torch.utils.data.DataLoader(dataset=Dataset(train_data,src_len = 20,trg_len=26),batch_size=BATCHSIZE)\n",
    "    validate_data_loader = torch.utils.data.DataLoader(dataset=Dataset(validate_data,src_len = 20,trg_len=26),batch_size=BATCHSIZE)\n",
    "    return train_data_loader,validate_data_loader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "\n",
    "\n",
    "FOLD = 2\n",
    "CITY = 0\n",
    "BATCHSIZE = 10000\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "trDataX  = pd.read_csv(\"fold{}_city{}_trainX.csv\".format(FOLD,CITY),header=None)\n",
    "trDataY  = pd.read_csv(\"fold{}_city{}_trainY.csv\".format(FOLD,CITY),header=None)\n",
    "train_iterator,valid_iterator = dataset_iter(trDataX,trDataY)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    #train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    #valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'fold{}-city{}-model.pt'.format(FOLD,CITY))\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train epoch_loss 1667.7232666015625\n",
      "train epoch_loss 3723.234430551529\n",
      "train epoch_loss 3768.006135880947\n",
      "train epoch_loss 3837.3120723068714\n",
      "train epoch_loss 3856.373703852296\n",
      "vali epoch_loss 0.7135035395622253\n",
      "vali epoch_loss 22.004001677036285\n",
      "vali epoch_loss 42.97784161567688\n",
      "vali epoch_loss 65.34676241874695\n",
      "vali epoch_loss 86.48709148168564\n",
      "Epoch: 01 | Time: 6m 23s\n",
      "\tTrain Loss: 29.079 | Train PPL: 4255484729634.600\n",
      "\t Val. Loss: 0.712 |  Val. PPL:   2.037\n",
      "train epoch_loss 0.8098439574241638\n",
      "train epoch_loss 14.967001616954803\n",
      "train epoch_loss 44.25553038716316\n",
      "train epoch_loss 70.65797173976898\n",
      "train epoch_loss 94.44342571496964\n",
      "vali epoch_loss 2.4661550521850586\n",
      "vali epoch_loss 76.33621072769165\n",
      "vali epoch_loss 149.88958287239075\n",
      "vali epoch_loss 224.83810091018677\n",
      "vali epoch_loss 298.55797004699707\n",
      "Epoch: 02 | Time: 6m 21s\n",
      "\tTrain Loss: 0.769 | Train PPL:   2.157\n",
      "\t Val. Loss: 2.458 |  Val. PPL:  11.677\n",
      "train epoch_loss 2.551815986633301\n",
      "train epoch_loss 18.213331952691078\n",
      "train epoch_loss 120.29230735450983\n",
      "train epoch_loss 173.705033890903\n",
      "train epoch_loss 193.09934977442026\n",
      "vali epoch_loss 0.19029080867767334\n",
      "vali epoch_loss 5.784472838044167\n",
      "vali epoch_loss 11.061855614185333\n",
      "vali epoch_loss 17.734723508358\n",
      "vali epoch_loss 23.17862504720688\n",
      "Epoch: 03 | Time: 6m 23s\n",
      "\tTrain Loss: 1.485 | Train PPL:   4.417\n",
      "\t Val. Loss: 0.190 |  Val. PPL:   1.210\n",
      "train epoch_loss 0.27459198236465454\n",
      "train epoch_loss 12.835963934659958\n",
      "train epoch_loss 24.463913314044476\n",
      "train epoch_loss 33.70518849790096\n",
      "train epoch_loss 62.66448622941971\n",
      "vali epoch_loss 0.08430787920951843\n",
      "vali epoch_loss 2.4989781975746155\n",
      "vali epoch_loss 4.596909929066896\n",
      "vali epoch_loss 8.090151611715555\n",
      "vali epoch_loss 10.35458954796195\n",
      "Epoch: 04 | Time: 6m 21s\n",
      "\tTrain Loss: 0.540 | Train PPL:   1.716\n",
      "\t Val. Loss: 0.085 |  Val. PPL:   1.088\n",
      "train epoch_loss 0.16633105278015137\n",
      "train epoch_loss 25.771008104085922\n",
      "train epoch_loss 57.54320988059044\n",
      "train epoch_loss 80.98527524620295\n",
      "train epoch_loss 140.38675781339407\n",
      "vali epoch_loss 0.3486352562904358\n",
      "vali epoch_loss 10.693161249160767\n",
      "vali epoch_loss 20.72088250517845\n",
      "vali epoch_loss 32.144115060567856\n",
      "vali epoch_loss 42.338354736566544\n",
      "Epoch: 05 | Time: 6m 22s\n",
      "\tTrain Loss: 1.113 | Train PPL:   3.042\n",
      "\t Val. Loss: 0.348 |  Val. PPL:   1.416\n",
      "train epoch_loss 0.4326333999633789\n",
      "train epoch_loss 21.213178545236588\n",
      "train epoch_loss 31.758697897195816\n",
      "train epoch_loss 40.94380000978708\n",
      "train epoch_loss 59.48212441802025\n",
      "vali epoch_loss 0.3756779432296753\n",
      "vali epoch_loss 11.531467854976654\n",
      "vali epoch_loss 22.370507776737213\n",
      "vali epoch_loss 34.60490137338638\n",
      "vali epoch_loss 45.61044901609421\n",
      "Epoch: 06 | Time: 6m 20s\n",
      "\tTrain Loss: 0.604 | Train PPL:   1.829\n",
      "\t Val. Loss: 0.375 |  Val. PPL:   1.455\n",
      "train epoch_loss 0.45535698533058167\n",
      "train epoch_loss 20.73870286345482\n",
      "train epoch_loss 71.35735458135605\n",
      "train epoch_loss 85.7324530929327\n",
      "train epoch_loss 97.36656890064478\n",
      "vali epoch_loss 0.09463003277778625\n",
      "vali epoch_loss 2.818985514342785\n",
      "vali epoch_loss 5.226585395634174\n",
      "vali epoch_loss 9.029560916125774\n",
      "vali epoch_loss 11.603668682277203\n",
      "Epoch: 07 | Time: 6m 23s\n",
      "\tTrain Loss: 0.757 | Train PPL:   2.133\n",
      "\t Val. Loss: 0.095 |  Val. PPL:   1.100\n",
      "train epoch_loss 0.17593267560005188\n",
      "train epoch_loss 22.673701152205467\n",
      "train epoch_loss 47.36992000043392\n",
      "train epoch_loss 67.43186264485121\n",
      "train epoch_loss 82.88173102587461\n",
      "vali epoch_loss 0.1759171187877655\n",
      "vali epoch_loss 5.338881775736809\n",
      "vali epoch_loss 10.18509817123413\n",
      "vali epoch_loss 16.42666447162628\n",
      "vali epoch_loss 21.439387783408165\n",
      "Epoch: 08 | Time: 6m 20s\n",
      "\tTrain Loss: 0.646 | Train PPL:   1.908\n",
      "\t Val. Loss: 0.176 |  Val. PPL:   1.192\n",
      "train epoch_loss 0.257084459066391\n",
      "train epoch_loss 17.93548959493637\n",
      "train epoch_loss 34.1141292154789\n",
      "train epoch_loss 48.93194743990898\n",
      "train epoch_loss 52.89442943781614\n",
      "vali epoch_loss 0.13899490237236023\n",
      "vali epoch_loss 4.194248467683792\n",
      "vali epoch_loss 7.932877726852894\n",
      "vali epoch_loss 13.066524617373943\n",
      "vali epoch_loss 16.97163336724043\n",
      "Epoch: 09 | Time: 6m 23s\n",
      "\tTrain Loss: 0.414 | Train PPL:   1.513\n",
      "\t Val. Loss: 0.139 |  Val. PPL:   1.149\n",
      "train epoch_loss 0.219294473528862\n",
      "train epoch_loss 11.226967334747314\n",
      "train epoch_loss 22.88159281015396\n",
      "train epoch_loss 26.62745178490877\n",
      "train epoch_loss 30.86860316991806\n",
      "vali epoch_loss 0.13513299822807312\n",
      "vali epoch_loss 4.074288345873356\n",
      "vali epoch_loss 7.697495922446251\n",
      "vali epoch_loss 12.713894203305244\n",
      "vali epoch_loss 16.503435268998146\n",
      "Epoch: 10 | Time: 6m 20s\n",
      "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
      "\t Val. Loss: 0.135 |  Val. PPL:   1.145\n",
      "train epoch_loss 0.21015208959579468\n",
      "train epoch_loss 8.357813596725464\n",
      "train epoch_loss 14.719736866652966\n",
      "train epoch_loss 18.227491937577724\n",
      "train epoch_loss 22.095745474100113\n",
      "vali epoch_loss 0.1638731062412262\n",
      "vali epoch_loss 4.965049117803574\n",
      "vali epoch_loss 9.45076185464859\n",
      "vali epoch_loss 15.328330010175705\n",
      "vali epoch_loss 19.98027513921261\n",
      "Epoch: 11 | Time: 6m 23s\n",
      "\tTrain Loss: 0.183 | Train PPL:   1.200\n",
      "\t Val. Loss: 0.164 |  Val. PPL:   1.178\n",
      "train epoch_loss 0.2341858148574829\n",
      "train epoch_loss 8.47027014195919\n",
      "train epoch_loss 15.425499856472015\n",
      "train epoch_loss 20.614585548639297\n",
      "train epoch_loss 25.474534511566162\n",
      "vali epoch_loss 2.012571096420288\n",
      "vali epoch_loss 62.270694732666016\n",
      "vali epoch_loss 122.22602427005768\n",
      "vali epoch_loss 183.52814161777496\n",
      "vali epoch_loss 243.64816844463348\n",
      "Epoch: 12 | Time: 6m 21s\n",
      "\tTrain Loss: 0.244 | Train PPL:   1.277\n",
      "\t Val. Loss: 2.006 |  Val. PPL:   7.430\n",
      "train epoch_loss 1.3119840621948242\n",
      "train epoch_loss 38.416382282972336\n",
      "train epoch_loss 54.784380830824375\n",
      "train epoch_loss 60.18689027428627\n",
      "train epoch_loss 69.34169849008322\n",
      "vali epoch_loss 0.08987574279308319\n",
      "vali epoch_loss 2.6700844764709473\n",
      "vali epoch_loss 4.937623240053654\n",
      "vali epoch_loss 8.589111648499966\n",
      "vali epoch_loss 11.022325091063976\n",
      "Epoch: 13 | Time: 6m 23s\n",
      "\tTrain Loss: 0.587 | Train PPL:   1.798\n",
      "\t Val. Loss: 0.090 |  Val. PPL:   1.094\n",
      "train epoch_loss 0.15208908915519714\n",
      "train epoch_loss 12.314957112073898\n",
      "train epoch_loss 23.53506228327751\n",
      "train epoch_loss 33.37903115898371\n",
      "train epoch_loss 42.80272613465786\n",
      "vali epoch_loss 0.10161226987838745\n",
      "vali epoch_loss 3.033512383699417\n",
      "vali epoch_loss 5.65394439548254\n",
      "vali epoch_loss 9.654735058546066\n",
      "vali epoch_loss 12.440623842179775\n",
      "Epoch: 14 | Time: 6m 21s\n",
      "\tTrain Loss: 0.338 | Train PPL:   1.402\n",
      "\t Val. Loss: 0.102 |  Val. PPL:   1.107\n",
      "train epoch_loss 0.1562255471944809\n",
      "train epoch_loss 13.027619048953056\n",
      "train epoch_loss 26.240635737776756\n",
      "train epoch_loss 35.77487479895353\n",
      "train epoch_loss 39.62721549719572\n",
      "vali epoch_loss 0.21715113520622253\n",
      "vali epoch_loss 6.6147505939006805\n",
      "vali epoch_loss 12.702367469668388\n",
      "vali epoch_loss 20.16575911641121\n",
      "vali epoch_loss 26.418582692742348\n",
      "Epoch: 15 | Time: 6m 23s\n",
      "\tTrain Loss: 0.314 | Train PPL:   1.369\n",
      "\t Val. Loss: 0.217 |  Val. PPL:   1.242\n",
      "train epoch_loss 0.2395792007446289\n",
      "train epoch_loss 13.816444858908653\n",
      "train epoch_loss 32.7633820772171\n",
      "train epoch_loss 43.849173814058304\n",
      "train epoch_loss 52.8278544396162\n",
      "vali epoch_loss 0.502602219581604\n",
      "vali epoch_loss 15.463334888219833\n",
      "vali epoch_loss 30.115088909864426\n",
      "vali epoch_loss 46.139922589063644\n",
      "vali epoch_loss 60.95663845539093\n",
      "Epoch: 16 | Time: 6m 21s\n",
      "\tTrain Loss: 0.480 | Train PPL:   1.616\n",
      "\t Val. Loss: 0.501 |  Val. PPL:   1.651\n",
      "train epoch_loss 0.4443507194519043\n",
      "train epoch_loss 9.469995245337486\n",
      "train epoch_loss 38.39063446223736\n",
      "train epoch_loss 62.21405790746212\n",
      "train epoch_loss 70.79122733324766\n",
      "vali epoch_loss 0.3393874764442444\n",
      "vali epoch_loss 10.40647467970848\n",
      "vali epoch_loss 20.15702450275421\n",
      "vali epoch_loss 31.30261778831482\n",
      "vali epoch_loss 41.21959328651428\n",
      "Epoch: 17 | Time: 6m 23s\n",
      "\tTrain Loss: 0.566 | Train PPL:   1.761\n",
      "\t Val. Loss: 0.339 |  Val. PPL:   1.403\n",
      "train epoch_loss 0.40783894062042236\n",
      "train epoch_loss 14.177359744906425\n",
      "train epoch_loss 29.173986241221428\n",
      "train epoch_loss 32.91968722641468\n",
      "train epoch_loss 40.84773521870375\n",
      "vali epoch_loss 0.36761748790740967\n",
      "vali epoch_loss 11.281207144260406\n",
      "vali epoch_loss 21.879208654165268\n",
      "vali epoch_loss 33.86977097392082\n",
      "vali epoch_loss 44.63397380709648\n",
      "Epoch: 18 | Time: 6m 21s\n",
      "\tTrain Loss: 0.324 | Train PPL:   1.382\n",
      "\t Val. Loss: 0.367 |  Val. PPL:   1.443\n",
      "train epoch_loss 0.4207940697669983\n",
      "train epoch_loss 9.370151802897453\n",
      "train epoch_loss 17.647354617714882\n",
      "train epoch_loss 25.960464596748352\n",
      "train epoch_loss 34.00196145474911\n",
      "vali epoch_loss 0.14931872487068176\n",
      "vali epoch_loss 4.5133233070373535\n",
      "vali epoch_loss 8.563415423035622\n",
      "vali epoch_loss 14.001700609922409\n",
      "vali epoch_loss 18.21760204434395\n",
      "Epoch: 19 | Time: 6m 23s\n",
      "\tTrain Loss: 0.285 | Train PPL:   1.330\n",
      "\t Val. Loss: 0.149 |  Val. PPL:   1.161\n",
      "train epoch_loss 0.2045784890651703\n",
      "train epoch_loss 11.526172861456871\n",
      "train epoch_loss 17.215991660952568\n",
      "train epoch_loss 32.143825083971024\n",
      "train epoch_loss 43.31955972313881\n",
      "vali epoch_loss 0.1497800052165985\n",
      "vali epoch_loss 4.525543347001076\n",
      "vali epoch_loss 8.595133818686008\n",
      "vali epoch_loss 14.030549339950085\n",
      "vali epoch_loss 18.264478377997875\n",
      "Epoch: 20 | Time: 6m 20s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.408\n",
      "\t Val. Loss: 0.150 |  Val. PPL:   1.162\n",
      "train epoch_loss 0.15735428035259247\n",
      "train epoch_loss 9.846535548567772\n",
      "train epoch_loss 21.667809411883354\n",
      "train epoch_loss 29.323182336986065\n",
      "train epoch_loss 44.89983008801937\n",
      "vali epoch_loss 0.30123743414878845\n",
      "vali epoch_loss 9.220480561256409\n",
      "vali epoch_loss 17.83431887626648\n",
      "vali epoch_loss 27.8116238117218\n",
      "vali epoch_loss 36.589646488428116\n",
      "Epoch: 21 | Time: 6m 24s\n",
      "\tTrain Loss: 0.356 | Train PPL:   1.427\n",
      "\t Val. Loss: 0.301 |  Val. PPL:   1.351\n",
      "train epoch_loss 0.24969838559627533\n",
      "train epoch_loss 7.098322227597237\n",
      "train epoch_loss 14.16629447042942\n",
      "train epoch_loss 19.161025278270245\n",
      "train epoch_loss 23.767942130565643\n",
      "vali epoch_loss 0.08897647261619568\n",
      "vali epoch_loss 2.639878422021866\n",
      "vali epoch_loss 4.889313146471977\n",
      "vali epoch_loss 8.488865375518799\n",
      "vali epoch_loss 10.901953287422657\n",
      "Epoch: 22 | Time: 6m 21s\n",
      "\tTrain Loss: 0.198 | Train PPL:   1.219\n",
      "\t Val. Loss: 0.089 |  Val. PPL:   1.093\n",
      "train epoch_loss 0.11122162640094757\n",
      "train epoch_loss 9.060703173279762\n",
      "train epoch_loss 22.336769312620163\n",
      "train epoch_loss 30.770110830664635\n",
      "train epoch_loss 42.56141024082899\n",
      "vali epoch_loss 0.7382442355155945\n",
      "vali epoch_loss 22.766542613506317\n",
      "vali epoch_loss 44.49803817272186\n",
      "vali epoch_loss 67.56615161895752\n",
      "vali epoch_loss 89.46061420440674\n",
      "Epoch: 23 | Time: 6m 24s\n",
      "\tTrain Loss: 0.337 | Train PPL:   1.401\n",
      "\t Val. Loss: 0.736 |  Val. PPL:   2.088\n",
      "train epoch_loss 0.37805238366127014\n",
      "train epoch_loss 18.160643324255943\n",
      "train epoch_loss 27.777712613344193\n",
      "train epoch_loss 40.3085795044899\n",
      "train epoch_loss 45.96640405803919\n",
      "vali epoch_loss 0.25660040974617004\n",
      "vali epoch_loss 7.835557401180267\n",
      "vali epoch_loss 15.118055984377861\n",
      "vali epoch_loss 23.735162898898125\n",
      "vali epoch_loss 31.180662602186203\n",
      "Epoch: 24 | Time: 6m 21s\n",
      "\tTrain Loss: 0.367 | Train PPL:   1.444\n",
      "\t Val. Loss: 0.256 |  Val. PPL:   1.292\n",
      "train epoch_loss 0.18218128383159637\n",
      "train epoch_loss 8.658061683177948\n",
      "train epoch_loss 13.82834156602621\n",
      "train epoch_loss 30.639975979924202\n",
      "train epoch_loss 54.412573873996735\n",
      "vali epoch_loss 0.0955476462841034\n",
      "vali epoch_loss 2.8429050892591476\n",
      "vali epoch_loss 5.292366534471512\n",
      "vali epoch_loss 9.081527076661587\n",
      "vali epoch_loss 11.694097325205803\n",
      "Epoch: 25 | Time: 6m 24s\n",
      "\tTrain Loss: 0.438 | Train PPL:   1.550\n",
      "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n",
      "train epoch_loss 0.10229793936014175\n",
      "train epoch_loss 6.547980584204197\n",
      "train epoch_loss 12.568430192768574\n",
      "train epoch_loss 27.448280923068523\n",
      "train epoch_loss 42.68724159896374\n",
      "vali epoch_loss 0.6376515626907349\n",
      "vali epoch_loss 19.647998571395874\n",
      "vali epoch_loss 38.36185383796692\n",
      "vali epoch_loss 58.40995413064957\n",
      "vali epoch_loss 77.28681373596191\n",
      "Epoch: 26 | Time: 6m 21s\n",
      "\tTrain Loss: 0.344 | Train PPL:   1.411\n",
      "\t Val. Loss: 0.636 |  Val. PPL:   1.889\n",
      "train epoch_loss 0.34012582898139954\n",
      "train epoch_loss 9.217195138335228\n",
      "train epoch_loss 15.854448281228542\n",
      "train epoch_loss 21.062965843826532\n",
      "train epoch_loss 36.8957987613976\n",
      "vali epoch_loss 0.15279075503349304\n",
      "vali epoch_loss 4.616647690534592\n",
      "vali epoch_loss 8.800244241952896\n",
      "vali epoch_loss 14.247191205620766\n",
      "vali epoch_loss 18.593049854040146\n",
      "Epoch: 27 | Time: 6m 23s\n",
      "\tTrain Loss: 0.301 | Train PPL:   1.351\n",
      "\t Val. Loss: 0.153 |  Val. PPL:   1.165\n",
      "train epoch_loss 0.10226494073867798\n",
      "train epoch_loss 18.67053060233593\n",
      "train epoch_loss 28.708118066191673\n",
      "train epoch_loss 34.12414238601923\n",
      "train epoch_loss 39.93454125523567\n",
      "vali epoch_loss 0.5158138275146484\n",
      "vali epoch_loss 15.870563209056854\n",
      "vali epoch_loss 30.937922686338425\n",
      "vali epoch_loss 47.30253109335899\n",
      "vali epoch_loss 62.53230795264244\n",
      "Epoch: 28 | Time: 6m 21s\n",
      "\tTrain Loss: 0.323 | Train PPL:   1.381\n",
      "\t Val. Loss: 0.514 |  Val. PPL:   1.673\n",
      "train epoch_loss 0.22547613084316254\n",
      "train epoch_loss 7.276357218623161\n",
      "train epoch_loss 13.502791833132505\n",
      "train epoch_loss 18.610458184033632\n",
      "train epoch_loss 23.859670601785183\n",
      "vali epoch_loss 0.8196135759353638\n",
      "vali epoch_loss 25.288164258003235\n",
      "vali epoch_loss 49.469262421131134\n",
      "vali epoch_loss 74.94706565141678\n",
      "vali epoch_loss 99.2906014919281\n",
      "Epoch: 29 | Time: 6m 24s\n",
      "\tTrain Loss: 0.203 | Train PPL:   1.225\n",
      "\t Val. Loss: 0.817 |  Val. PPL:   2.264\n",
      "train epoch_loss 0.3191739022731781\n",
      "train epoch_loss 15.33985847979784\n",
      "train epoch_loss 29.918714456260204\n",
      "train epoch_loss 37.412865810096264\n",
      "train epoch_loss 40.835052493959665\n",
      "vali epoch_loss 0.1128639280796051\n",
      "vali epoch_loss 3.378690518438816\n",
      "vali epoch_loss 6.354548461735249\n",
      "vali epoch_loss 10.638349764049053\n",
      "vali epoch_loss 13.776650547981262\n",
      "Epoch: 30 | Time: 6m 20s\n",
      "\tTrain Loss: 0.342 | Train PPL:   1.407\n",
      "\t Val. Loss: 0.113 |  Val. PPL:   1.120\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('py38': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}