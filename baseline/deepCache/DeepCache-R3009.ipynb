{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((300983, 12), (198175, 12), (253630, 12))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "#import os\n",
    "#pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "#if not os.path.exists(pathName):\n",
    "#     os.makedirs(pathName)\n",
    "#MODELPATH = pathName + 'dnn_v1.0_'\n",
    "\n",
    "data_path = '/home/zhangxz/workspace/dataset/R3009_U5_V100/'\n",
    "UIT = pd.read_csv(data_path + 'UIT.csv')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":0.01,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "UIT.shape,trainUIT.shape,validUIT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        #self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.rnn = nn.LSTM(input_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        embedded = self.dropout(src)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        #self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.rnn = nn.LSTM(output_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input_ = input_.float().unsqueeze(0)\n",
    "        #print(\"decode input shape\",input_.shape)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(input))\n",
    "        embedded = self.dropout(input_)\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        #prediction = self.fc_out(output.squeeze(0)).unsqueeze(-1)\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        #print(prediction.shape)\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        #print(\"trg_len\",trg.shape[0],\"batch_size\",trg.shape[1])\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        #print(input.shape)\n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            #top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            #print(trg[t].shape,output.unsqueeze(-1).shape)\n",
    "            input = trg[t] if teacher_force else output\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 1\n",
    "OUTPUT_DIM = 1\n",
    "ENC_EMB_DIM = 0\n",
    "DEC_EMB_DIM = 0\n",
    "HID_DIM = 64\n",
    "#HID_DIM = 64\n",
    "\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLD = 2\n",
    "CITY = 0\n",
    "BATCHSIZE = contentNum\n",
    "\n",
    "model.load_state_dict(torch.load(\"fold{}-city{}-model.pt\".format(FOLD,CITY)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "        #self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,dpc,data):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "        \n",
    "            if data != None:\n",
    "                actionIndex = self.evaluate(dpc,data,self.Bu+1)\n",
    "                if self.W[-1] not in actionIndex:\n",
    "                    actionIndex.pop() \n",
    "                for index in actionIndex:\n",
    "                    action[index] = 1\n",
    "                env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n",
    "    def evaluate(self,model, batch, cachesize):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            src = batch.permute(1,0,2).float().to(device)\n",
    "\n",
    "            batchsize = src.shape[1]\n",
    "            trg = torch.zeros((26,batchsize,1)).to(device)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output.squeeze(-1)\n",
    "\n",
    "            output = output[0].cpu().numpy()\n",
    "\n",
    "            max_item=np.argsort(-output)[0:cachesize]\n",
    "            #print(max_item,list(max_item))\n",
    "        return list(max_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8812</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4058</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1549</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>29</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>29</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148          11            21     0   \n",
       "1       203  5779    0        0         7          11             4     0   \n",
       "2       208  4675    0        0        92          13             4     0   \n",
       "3       159   332    0        0        56          11             3     0   \n",
       "4        50   674    0        0       439          11             4     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831   29  2591880        34          11             4     0   \n",
       "300979  158  8448   29  2591880        34          11             4     0   \n",
       "300980  483  6463   29  2591940        35          11             4     0   \n",
       "300981  158  4715   29  2591940        34          11             4     0   \n",
       "300982  483  2021   29  2591940        34          11             4     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0       8812          1            0  \n",
       "1              0       9063          1            0  \n",
       "2              0       3444          1            0  \n",
       "3              0       4058          1            0  \n",
       "4              0       1549          1            0  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0       7061          1            0  \n",
       "300979         0      11316          1            0  \n",
       "300980         0       7061          1            0  \n",
       "300981         0      11316          1            0  \n",
       "300982         0       7061          1            0  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>viewtime</th>\n",
       "      <th>video_type</th>\n",
       "      <th>video_format</th>\n",
       "      <th>city</th>\n",
       "      <th>city_isp</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>conn_type</th>\n",
       "      <th>device_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>365</td>\n",
       "      <td>3391</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8812</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>203</td>\n",
       "      <td>5779</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9063</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208</td>\n",
       "      <td>4675</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>92</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3444</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>159</td>\n",
       "      <td>332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4058</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>674</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1549</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300978</th>\n",
       "      <td>483</td>\n",
       "      <td>6831</td>\n",
       "      <td>719</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300979</th>\n",
       "      <td>158</td>\n",
       "      <td>8448</td>\n",
       "      <td>719</td>\n",
       "      <td>2591880</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300980</th>\n",
       "      <td>483</td>\n",
       "      <td>6463</td>\n",
       "      <td>719</td>\n",
       "      <td>2591940</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300981</th>\n",
       "      <td>158</td>\n",
       "      <td>4715</td>\n",
       "      <td>719</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11316</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300982</th>\n",
       "      <td>483</td>\n",
       "      <td>2021</td>\n",
       "      <td>719</td>\n",
       "      <td>2591940</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7061</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300983 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u     i  day     time  viewtime  video_type  video_format  city  \\\n",
       "0       365  3391    0        0       148          11            21     0   \n",
       "1       203  5779    0        0         7          11             4     0   \n",
       "2       208  4675    0        0        92          13             4     0   \n",
       "3       159   332    0        0        56          11             3     0   \n",
       "4        50   674    0        0       439          11             4     0   \n",
       "...     ...   ...  ...      ...       ...         ...           ...   ...   \n",
       "300978  483  6831  719  2591880        34          11             4     0   \n",
       "300979  158  8448  719  2591880        34          11             4     0   \n",
       "300980  483  6463  719  2591940        35          11             4     0   \n",
       "300981  158  4715  719  2591940        34          11             4     0   \n",
       "300982  483  2021  719  2591940        34          11             4     0   \n",
       "\n",
       "        city_isp  client_ip  conn_type  device_type  \n",
       "0              0       8812          1            0  \n",
       "1              0       9063          1            0  \n",
       "2              0       3444          1            0  \n",
       "3              0       4058          1            0  \n",
       "4              0       1549          1            0  \n",
       "...          ...        ...        ...          ...  \n",
       "300978         0       7061          1            0  \n",
       "300979         0      11316          1            0  \n",
       "300980         0       7061          1            0  \n",
       "300981         0      11316          1            0  \n",
       "300982         0       7061          1            0  \n",
       "\n",
       "[300983 rows x 12 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UIT['day'] = UIT['time']//(60*60)\n",
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sun Oct 17 00:18:02 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-0.75377  0.02616  0.     ] total reward: -0.72761\n",
      "UEHitrate: 0.0  edgeHitrate 0.0327 sumHitrate 0.0327  privacy: 0.43735\n",
      "\n",
      "--Time: Sun Oct 17 00:31:22 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-2.43069  0.19496  0.     ] total reward: -2.23573\n",
      "UEHitrate: 0.0  edgeHitrate 0.2437 sumHitrate 0.2437  privacy: 0.64891\n",
      "\n",
      "--Time: Sun Oct 17 00:44:41 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-2.73119  0.22909  0.     ] total reward: -2.5021\n",
      "UEHitrate: 0.0  edgeHitrate 0.28637 sumHitrate 0.28637  privacy: 0.72124\n",
      "\n",
      "--Time: Sun Oct 17 00:57:09 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-2.79039  0.24174  0.     ] total reward: -2.54865\n",
      "UEHitrate: 0.0  edgeHitrate 0.30218 sumHitrate 0.30218  privacy: 0.76267\n",
      "\n",
      "--Time: Sun Oct 17 01:08:01 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-2.72758  0.25296  0.     ] total reward: -2.47462\n",
      "UEHitrate: 0.0  edgeHitrate 0.3162 sumHitrate 0.3162  privacy: 0.77896\n",
      "\n",
      "--Time: Sun Oct 17 01:18:47 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-2.68372  0.25904  0.     ] total reward: -2.42468\n",
      "UEHitrate: 0.0  edgeHitrate 0.3238 sumHitrate 0.3238  privacy: 0.80359\n",
      "\n",
      "--Time: Sun Oct 17 01:29:36 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-2.62342  0.25944  0.     ] total reward: -2.36398\n",
      "UEHitrate: 0.0  edgeHitrate 0.3243 sumHitrate 0.3243  privacy: 0.82002\n",
      "\n",
      "--Time: Sun Oct 17 01:40:27 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-2.5237   0.26708  0.     ] total reward: -2.25662\n",
      "UEHitrate: 0.0  edgeHitrate 0.33385 sumHitrate 0.33385  privacy: 0.8286\n",
      "\n",
      "--Time: Sun Oct 17 01:51:11 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-2.44582  0.26919  0.     ] total reward: -2.17663\n",
      "UEHitrate: 0.0  edgeHitrate 0.33649 sumHitrate 0.33649  privacy: 0.83705\n",
      "\n",
      "--Time: Sun Oct 17 02:00:42 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-2.38361  0.27174  0.     ] total reward: -2.11187\n",
      "UEHitrate: 0.0  edgeHitrate 0.33967 sumHitrate 0.33967  privacy: 0.83736\n",
      "\n",
      "--Time: Sun Oct 17 02:10:13 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-2.31557  0.27457  0.     ] total reward: -2.04099\n",
      "UEHitrate: 0.0  edgeHitrate 0.34322 sumHitrate 0.34322  privacy: 0.83769\n",
      "\n",
      "--Time: Sun Oct 17 02:19:37 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-2.26465  0.27632  0.     ] total reward: -1.98833\n",
      "UEHitrate: 0.0  edgeHitrate 0.3454 sumHitrate 0.3454  privacy: 0.84429\n",
      "\n",
      "--Time: Sun Oct 17 02:29:09 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-2.20764  0.27494  0.     ] total reward: -1.9327\n",
      "UEHitrate: 0.0  edgeHitrate 0.34367 sumHitrate 0.34367  privacy: 0.84554\n",
      "\n",
      "--Time: Sun Oct 17 02:38:38 2021   Index: 139999   Loss: 0.0 --\n",
      "Reward: [-2.1578   0.27951  0.     ] total reward: -1.87829\n",
      "UEHitrate: 0.0  edgeHitrate 0.34939 sumHitrate 0.34939  privacy: 0.85112\n",
      "\n",
      "--Time: Sun Oct 17 02:48:05 2021   Index: 149999   Loss: 0.0 --\n",
      "Reward: [-2.10698  0.28416  0.     ] total reward: -1.82282\n",
      "UEHitrate: 0.0  edgeHitrate 0.3552 sumHitrate 0.3552  privacy: 0.85774\n",
      "\n",
      "--Time: Sun Oct 17 02:57:28 2021   Index: 159999   Loss: 0.0 --\n",
      "Reward: [-2.06602  0.28323  0.     ] total reward: -1.7828\n",
      "UEHitrate: 0.0  edgeHitrate 0.35403 sumHitrate 0.35403  privacy: 0.86172\n",
      "\n",
      "--Time: Sun Oct 17 03:06:57 2021   Index: 169999   Loss: 0.0 --\n",
      "Reward: [-2.02127  0.28675  0.     ] total reward: -1.73452\n",
      "UEHitrate: 0.0  edgeHitrate 0.35844 sumHitrate 0.35844  privacy: 0.86432\n",
      "\n",
      "--Time: Sun Oct 17 03:15:53 2021   Index: 179999   Loss: 0.0 --\n",
      "Reward: [-1.97562  0.28956  0.     ] total reward: -1.68605\n",
      "UEHitrate: 0.0  edgeHitrate 0.36196 sumHitrate 0.36196  privacy: 0.85868\n",
      "\n",
      "--Time: Sun Oct 17 03:24:56 2021   Index: 189999   Loss: 0.0 --\n",
      "Reward: [-1.92941  0.2914   0.     ] total reward: -1.63801\n",
      "UEHitrate: 0.0  edgeHitrate 0.36425 sumHitrate 0.36425  privacy: 0.85816\n",
      "\n",
      "--Time: Sun Oct 17 03:33:55 2021   Index: 199999   Loss: 0.0 --\n",
      "Reward: [-1.88745  0.29247  0.     ] total reward: -1.59498\n",
      "UEHitrate: 0.0  edgeHitrate 0.36559 sumHitrate 0.36559  privacy: 0.85602\n",
      "\n",
      "--Time: Sun Oct 17 03:42:50 2021   Index: 209999   Loss: 0.0 --\n",
      "Reward: [-1.85221  0.29463  0.     ] total reward: -1.55758\n",
      "UEHitrate: 0.0  edgeHitrate 0.36829 sumHitrate 0.36829  privacy: 0.83835\n",
      "\n",
      "--Time: Sun Oct 17 03:51:45 2021   Index: 219999   Loss: 0.0 --\n",
      "Reward: [-1.81756  0.2938   0.     ] total reward: -1.52376\n",
      "UEHitrate: 0.0  edgeHitrate 0.36725 sumHitrate 0.36725  privacy: 0.84243\n",
      "\n",
      "--Time: Sun Oct 17 04:01:25 2021   Index: 229999   Loss: 0.0 --\n",
      "Reward: [-1.7911   0.29321  0.     ] total reward: -1.49789\n",
      "UEHitrate: 0.0  edgeHitrate 0.36651 sumHitrate 0.36651  privacy: 0.84704\n",
      "\n",
      "--Time: Sun Oct 17 04:10:50 2021   Index: 239999   Loss: 0.0 --\n",
      "Reward: [-1.7616   0.29273  0.     ] total reward: -1.46887\n",
      "UEHitrate: 0.0  edgeHitrate 0.36591 sumHitrate 0.36591  privacy: 0.85096\n",
      "\n",
      "--Time: Sun Oct 17 04:20:10 2021   Index: 249999   Loss: 0.0 --\n",
      "Reward: [-1.73221  0.29348  0.     ] total reward: -1.43872\n",
      "UEHitrate: 0.0  edgeHitrate 0.36685 sumHitrate 0.36685  privacy: 0.85426\n",
      "\n",
      "--Time: Sun Oct 17 04:29:32 2021   Index: 259999   Loss: 0.0 --\n",
      "Reward: [-1.70327  0.29242  0.     ] total reward: -1.41085\n",
      "UEHitrate: 0.0  edgeHitrate 0.36552 sumHitrate 0.36552  privacy: 0.85716\n",
      "\n",
      "--Time: Sun Oct 17 04:39:01 2021   Index: 269999   Loss: 0.0 --\n",
      "Reward: [-1.67901  0.29132  0.     ] total reward: -1.38769\n",
      "UEHitrate: 0.0  edgeHitrate 0.36416 sumHitrate 0.36416  privacy: 0.86023\n",
      "\n",
      "--Time: Sun Oct 17 04:48:13 2021   Index: 279999   Loss: 0.0 --\n",
      "Reward: [-1.65455  0.29174  0.     ] total reward: -1.36282\n",
      "UEHitrate: 0.0  edgeHitrate 0.36467 sumHitrate 0.36467  privacy: 0.86308\n",
      "\n",
      "--Time: Sun Oct 17 04:57:24 2021   Index: 289999   Loss: 0.0 --\n",
      "Reward: [-1.627    0.29389  0.     ] total reward: -1.33311\n",
      "UEHitrate: 0.0  edgeHitrate 0.36737 sumHitrate 0.36737  privacy: 0.86552\n",
      "\n",
      "--Time: Sun Oct 17 05:06:35 2021   Index: 299999   Loss: 0.0 --\n",
      "Reward: [-1.59648  0.29696  0.     ] total reward: -1.29952\n",
      "UEHitrate: 0.0  edgeHitrate 0.3712 sumHitrate 0.3712  privacy: 0.8674\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 17 05:07:30 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-1.59359  0.29721  0.     ] total reward: -1.29637\n",
      "UEHitrate: 0.0  edgeHitrate 0.37152 sumHitrate 0.37152  privacy: 0.86756\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "Frq_hour = torch.zeros((BATCHSIZE,720*6)).to(device)\n",
    "m = 20\n",
    "\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":1,\"betal\":1}\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "UEHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "edgeHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "privacyReduction = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "i_episode = 0\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    if uit[1] in np.argwhere(ue.lastAction):\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    if uit[2]>=m:\n",
    "        test_iterator = Frq_hour[:,uit[2]-m:uit[2]]/Frq_hour[:,uit[2]-m:uit[2]].sum(dim=0)\n",
    "        test_iterator = test_iterator.unsqueeze(2)\n",
    "        #print(test_iterator.sum())\n",
    "        \n",
    "    else:\n",
    "        test_iterator = None\n",
    "    \n",
    "    ue.selectAction(env,uit,model,test_iterator)\n",
    "\n",
    "    for content in np.argwhere(ue.lastAction):\n",
    "        Frq_hour[content,uit[2]]+=1\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Rl)\n",
    "    sumReward[2] += float(ue.Ro)\n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.0327 , 0.2437 , 0.28637, 0.30218, 0.3162 , 0.3238 , 0.3243 ,\n",
       "        0.33385, 0.33649, 0.33967, 0.34322, 0.3454 , 0.34367, 0.34939,\n",
       "        0.3552 , 0.35403, 0.35844, 0.36196, 0.36425, 0.36559, 0.36829,\n",
       "        0.36725, 0.36651, 0.36591, 0.36685, 0.36552, 0.36416, 0.36467,\n",
       "        0.36737, 0.3712 , 0.37152, 0.     ]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.0327 , 0.2437 , 0.28637, 0.30218, 0.3162 , 0.3238 , 0.3243 ,\n",
       "        0.33385, 0.33649, 0.33967, 0.34322, 0.3454 , 0.34367, 0.34939,\n",
       "        0.3552 , 0.35403, 0.35844, 0.36196, 0.36425, 0.36559, 0.36829,\n",
       "        0.36725, 0.36651, 0.36591, 0.36685, 0.36552, 0.36416, 0.36467,\n",
       "        0.36737, 0.3712 , 0.37152, 0.     ]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43735, 0.64891, 0.72124, 0.76267, 0.77896, 0.80359, 0.82002,\n",
       "       0.8286 , 0.83705, 0.83736, 0.83769, 0.84429, 0.84554, 0.85112,\n",
       "       0.85774, 0.86172, 0.86432, 0.85868, 0.85816, 0.85602, 0.83835,\n",
       "       0.84243, 0.84704, 0.85096, 0.85426, 0.85716, 0.86023, 0.86308,\n",
       "       0.86552, 0.8674 , 0.86756, 0.     ])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "        #self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,dpc,data):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "        \n",
    "            if data != None:\n",
    "                actionIndex = self.evaluate(dpc,data,self.Bu+1)\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop() \n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n",
    "    def evaluate(self,model, batch, cachesize):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            src = batch.permute(1,0,2).float().to(device)\n",
    "\n",
    "            batchsize = src.shape[1]\n",
    "            trg = torch.zeros((26,batchsize,1)).to(device)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output.squeeze(-1)\n",
    "\n",
    "            output = output[0].cpu().numpy()\n",
    "\n",
    "            max_item=np.argsort(-output)[0:cachesize]\n",
    "            #print(max_item,list(max_item))\n",
    "        return list(max_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sun Oct 17 10:07:39 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-62.61858   0.08216   0.     ] total reward: -62.53642\n",
      "UEHitrate: 0.0  edgeHitrate 0.1027 sumHitrate 0.1027  privacy: 3.94465\n",
      "\n",
      "--Time: Sun Oct 17 10:20:09 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-32.62463   0.22296   0.     ] total reward: -32.40167\n",
      "UEHitrate: 0.0  edgeHitrate 0.2787 sumHitrate 0.2787  privacy: 3.15531\n",
      "\n",
      "--Time: Sun Oct 17 10:31:59 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-22.26648   0.24776   0.     ] total reward: -22.01872\n",
      "UEHitrate: 0.0  edgeHitrate 0.3097 sumHitrate 0.3097  privacy: 2.77583\n",
      "\n",
      "--Time: Sun Oct 17 10:44:09 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-17.13323   0.25574   0.     ] total reward: -16.87749\n",
      "UEHitrate: 0.0  edgeHitrate 0.31967 sumHitrate 0.31968  privacy: 2.5636\n",
      "\n",
      "--Time: Sun Oct 17 10:56:41 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-14.01512   0.26416   0.     ] total reward: -13.75096\n",
      "UEHitrate: 0.0  edgeHitrate 0.3302 sumHitrate 0.3302  privacy: 2.4029\n",
      "\n",
      "--Time: Sun Oct 17 11:09:08 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-11.92821   0.26837   0.     ] total reward: -11.65983\n",
      "UEHitrate: 0.0  edgeHitrate 0.33547 sumHitrate 0.33547  privacy: 2.29392\n",
      "\n",
      "--Time: Sun Oct 17 11:21:54 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-10.43634   0.26744   0.     ] total reward: -10.1689\n",
      "UEHitrate: 0.0  edgeHitrate 0.3343 sumHitrate 0.3343  privacy: 2.2119\n",
      "\n",
      "--Time: Sun Oct 17 11:34:19 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-9.2814   0.27408  0.     ] total reward: -9.00732\n",
      "UEHitrate: 0.0  edgeHitrate 0.3426 sumHitrate 0.3426  privacy: 2.14106\n",
      "\n",
      "--Time: Sun Oct 17 11:46:49 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-8.3831   0.27541  0.     ] total reward: -8.10769\n",
      "UEHitrate: 0.0  edgeHitrate 0.34427 sumHitrate 0.34427  privacy: 2.07378\n",
      "\n",
      "--Time: Sun Oct 17 11:59:46 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-7.67101  0.27734  0.     ] total reward: -7.39367\n",
      "UEHitrate: 0.0  edgeHitrate 0.34667 sumHitrate 0.34667  privacy: 1.99797\n",
      "\n",
      "--Time: Sun Oct 17 12:12:31 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-7.07459  0.27967  0.     ] total reward: -6.79493\n",
      "UEHitrate: 0.0  edgeHitrate 0.34958 sumHitrate 0.34958  privacy: 1.94271\n",
      "\n",
      "--Time: Sun Oct 17 12:22:59 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-6.58381  0.28099  0.     ] total reward: -6.30282\n",
      "UEHitrate: 0.0  edgeHitrate 0.35123 sumHitrate 0.35123  privacy: 1.90889\n",
      "\n",
      "--Time: Sun Oct 17 12:33:01 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-6.15782  0.27924  0.     ] total reward: -5.87858\n",
      "UEHitrate: 0.0  edgeHitrate 0.34905 sumHitrate 0.34905  privacy: 1.86838\n",
      "\n",
      "--Time: Sun Oct 17 12:43:03 2021   Index: 139999   Loss: 0.0 --\n",
      "Reward: [-5.79373  0.28351  0.     ] total reward: -5.51023\n",
      "UEHitrate: 0.0  edgeHitrate 0.35439 sumHitrate 0.35439  privacy: 1.84176\n",
      "\n",
      "--Time: Sun Oct 17 12:53:17 2021   Index: 149999   Loss: 0.0 --\n",
      "Reward: [-5.47355  0.28789  0.     ] total reward: -5.18565\n",
      "UEHitrate: 0.0  edgeHitrate 0.35987 sumHitrate 0.35987  privacy: 1.82212\n",
      "\n",
      "--Time: Sun Oct 17 13:03:34 2021   Index: 159999   Loss: 0.0 --\n",
      "Reward: [-5.1947   0.28673  0.     ] total reward: -4.90798\n",
      "UEHitrate: 0.0  edgeHitrate 0.35841 sumHitrate 0.35841  privacy: 1.7953\n",
      "\n",
      "--Time: Sun Oct 17 13:13:43 2021   Index: 169999   Loss: 0.0 --\n",
      "Reward: [-4.945    0.29005  0.     ] total reward: -4.65496\n",
      "UEHitrate: 0.0  edgeHitrate 0.36256 sumHitrate 0.36256  privacy: 1.77474\n",
      "\n",
      "--Time: Sun Oct 17 13:21:55 2021   Index: 179999   Loss: 0.0 --\n",
      "Reward: [-4.71756  0.29268  0.     ] total reward: -4.42489\n",
      "UEHitrate: 0.0  edgeHitrate 0.36584 sumHitrate 0.36584  privacy: 1.74094\n",
      "\n",
      "--Time: Sun Oct 17 13:29:49 2021   Index: 189999   Loss: 0.0 --\n",
      "Reward: [-4.51065  0.29435  0.     ] total reward: -4.2163\n",
      "UEHitrate: 0.0  edgeHitrate 0.36794 sumHitrate 0.36794  privacy: 1.71988\n",
      "\n",
      "--Time: Sun Oct 17 13:37:05 2021   Index: 199999   Loss: 0.0 --\n",
      "Reward: [-4.32375  0.29527  0.     ] total reward: -4.02848\n",
      "UEHitrate: 0.0  edgeHitrate 0.36909 sumHitrate 0.36909  privacy: 1.69583\n",
      "\n",
      "--Time: Sun Oct 17 13:44:19 2021   Index: 209999   Loss: 0.0 --\n",
      "Reward: [-4.15925  0.2973   0.     ] total reward: -3.86195\n",
      "UEHitrate: 0.0  edgeHitrate 0.37162 sumHitrate 0.37162  privacy: 1.64948\n",
      "\n",
      "--Time: Sun Oct 17 13:53:04 2021   Index: 219999   Loss: 0.0 --\n",
      "Reward: [-4.00671  0.29634  0.     ] total reward: -3.71037\n",
      "UEHitrate: 0.0  edgeHitrate 0.37043 sumHitrate 0.37043  privacy: 1.63865\n",
      "\n",
      "--Time: Sun Oct 17 14:04:02 2021   Index: 229999   Loss: 0.0 --\n",
      "Reward: [-3.87019  0.29565  0.     ] total reward: -3.57454\n",
      "UEHitrate: 0.0  edgeHitrate 0.36956 sumHitrate 0.36956  privacy: 1.62586\n",
      "\n",
      "--Time: Sun Oct 17 14:14:24 2021   Index: 239999   Loss: 0.0 --\n",
      "Reward: [-3.74294  0.29506  0.     ] total reward: -3.44788\n",
      "UEHitrate: 0.0  edgeHitrate 0.36883 sumHitrate 0.36882  privacy: 1.6169\n",
      "\n",
      "--Time: Sun Oct 17 14:23:07 2021   Index: 249999   Loss: 0.0 --\n",
      "Reward: [-3.62346  0.29572  0.     ] total reward: -3.32774\n",
      "UEHitrate: 0.0  edgeHitrate 0.36965 sumHitrate 0.36965  privacy: 1.60787\n",
      "\n",
      "--Time: Sun Oct 17 14:33:04 2021   Index: 259999   Loss: 0.0 --\n",
      "Reward: [-3.51182  0.29457  0.     ] total reward: -3.21725\n",
      "UEHitrate: 0.0  edgeHitrate 0.36822 sumHitrate 0.36822  privacy: 1.59832\n",
      "\n",
      "--Time: Sun Oct 17 14:44:02 2021   Index: 269999   Loss: 0.0 --\n",
      "Reward: [-3.41125  0.2934   0.     ] total reward: -3.11785\n",
      "UEHitrate: 0.0  edgeHitrate 0.36675 sumHitrate 0.36675  privacy: 1.58801\n",
      "\n",
      "--Time: Sun Oct 17 14:54:50 2021   Index: 279999   Loss: 0.0 --\n",
      "Reward: [-3.31473  0.29374  0.     ] total reward: -3.02099\n",
      "UEHitrate: 0.0  edgeHitrate 0.36717 sumHitrate 0.36717  privacy: 1.57983\n",
      "\n",
      "--Time: Sun Oct 17 15:05:39 2021   Index: 289999   Loss: 0.0 --\n",
      "Reward: [-3.22257  0.29582  0.     ] total reward: -2.92674\n",
      "UEHitrate: 0.0  edgeHitrate 0.36978 sumHitrate 0.36978  privacy: 1.57357\n",
      "\n",
      "--Time: Sun Oct 17 15:16:35 2021   Index: 299999   Loss: 0.0 --\n",
      "Reward: [-3.13356  0.29882  0.     ] total reward: -2.83474\n",
      "UEHitrate: 0.0  edgeHitrate 0.37353 sumHitrate 0.37353  privacy: 1.56894\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 17 15:17:40 2021   Index: 300982   Loss: 0.0 --\n",
      "Reward: [-3.12512  0.29907  0.     ] total reward: -2.82604\n",
      "UEHitrate: 0.0  edgeHitrate 0.37384 sumHitrate 0.37384  privacy: 1.56854\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "Frq_hour = torch.zeros((BATCHSIZE,720*6)).to(device)\n",
    "m = 20\n",
    "\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":1,\"betal\":1}\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "UEHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "edgeHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "privacyReduction = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "i_episode = 0\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    if uit[1] in np.argwhere(ue.lastAction):\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    if uit[2]>=m:\n",
    "        test_iterator = Frq_hour[:,uit[2]-m:uit[2]]/Frq_hour[:,uit[2]-m:uit[2]].sum(dim=0)\n",
    "        test_iterator = test_iterator.unsqueeze(2)\n",
    "        #print(test_iterator.sum())\n",
    "        \n",
    "    else:\n",
    "        test_iterator = None\n",
    "    \n",
    "    ue.selectAction(env,uit,model,test_iterator)\n",
    "\n",
    "    for content in np.argwhere(ue.lastAction):\n",
    "        Frq_hour[content,uit[2]]+=1\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Rl)\n",
    "    sumReward[2] += float(ue.Ro)\n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.1027 , 0.2787 , 0.3097 , 0.31968, 0.3302 , 0.33547, 0.3343 ,\n",
       "        0.3426 , 0.34427, 0.34667, 0.34958, 0.35123, 0.34905, 0.35439,\n",
       "        0.35987, 0.35841, 0.36256, 0.36584, 0.36794, 0.36909, 0.37162,\n",
       "        0.37043, 0.36956, 0.36882, 0.36965, 0.36822, 0.36675, 0.36717,\n",
       "        0.36978, 0.37353, 0.37384, 0.     ]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.1027 , 0.2787 , 0.3097 , 0.31967, 0.3302 , 0.33547, 0.3343 ,\n",
       "        0.3426 , 0.34427, 0.34667, 0.34958, 0.35123, 0.34905, 0.35439,\n",
       "        0.35987, 0.35841, 0.36256, 0.36584, 0.36794, 0.36909, 0.37162,\n",
       "        0.37043, 0.36956, 0.36883, 0.36965, 0.36822, 0.36675, 0.36717,\n",
       "        0.36978, 0.37353, 0.37384, 0.     ]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.94465, 3.15531, 2.77583, 2.5636 , 2.4029 , 2.29392, 2.2119 ,\n",
       "       2.14106, 2.07378, 1.99797, 1.94271, 1.90889, 1.86838, 1.84176,\n",
       "       1.82212, 1.7953 , 1.77474, 1.74094, 1.71988, 1.69583, 1.64948,\n",
       "       1.63865, 1.62586, 1.6169 , 1.60787, 1.59832, 1.58801, 1.57983,\n",
       "       1.57357, 1.56894, 1.56854, 0.     ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
