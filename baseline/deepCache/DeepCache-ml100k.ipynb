{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((132690, 4), (76252, 4), (106994, 4))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "#env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "#import os\n",
    "#pathName = '/home/ubuntu/data/PEC/model_dict/ml100k/'\n",
    "#if not os.path.exists(pathName):\n",
    "#     os.makedirs(pathName)\n",
    "#MODELPATH = pathName + 'dnn_v1.0_'\n",
    "\n",
    "data_path = '/home/zhangxz/workspace/dataset/ml-100k/'\n",
    "UIT = pd.read_csv(data_path + 'ml_header.trace')\n",
    "simUsers = np.loadtxt(data_path+\"simUser.csv\",  delimiter=',',dtype=int)\n",
    "\n",
    "trainUIT = UIT[UIT['day']<max(UIT['day']+1)*0.6]\n",
    "validUIT = UIT[UIT['day']<max(UIT['day']+1)*0.8]\n",
    "\n",
    "contentNum = len(UIT.i.drop_duplicates())\n",
    "userNum = len(UIT.u.drop_duplicates())\n",
    "\n",
    "rewardPara = {\"alpha\":0.01,\"betao\":1,\"betal\":1}\n",
    "latency = [0.2,1,0.8]\n",
    "Bu = 20\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "\n",
    "num_episodes = 10\n",
    "EPS_START = 0.99\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = trainUIT.shape[0]*3\n",
    "agentStep = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "UIT.shape,trainUIT.shape,validUIT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        #self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.rnn = nn.LSTM(input_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(src))\n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        embedded = self.dropout(src)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        \n",
    "        #outputs = [src len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden, cell\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        #self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.rnn = nn.LSTM(output_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_, hidden, cell):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input_ = input_.float().unsqueeze(0)\n",
    "        #print(\"decode input shape\",input_.shape)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        #embedded = self.dropout(self.embedding(input))\n",
    "        embedded = self.dropout(input_)\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        \n",
    "        #output = [seq len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        \n",
    "        #prediction = self.fc_out(output.squeeze(0)).unsqueeze(-1)\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        #print(prediction.shape)\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        #print(\"trg_len\",trg.shape[0],\"batch_size\",trg.shape[1])\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        #print(input.shape)\n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            #top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            #print(trg[t].shape,output.unsqueeze(-1).shape)\n",
    "            input = trg[t] if teacher_force else output\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 1\n",
    "OUTPUT_DIM = 1\n",
    "ENC_EMB_DIM = 0\n",
    "DEC_EMB_DIM = 0\n",
    "HID_DIM = 64\n",
    "#HID_DIM = 64\n",
    "\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FOLD = 2\n",
    "CITY = 0\n",
    "BATCHSIZE = contentNum\n",
    "\n",
    "model.load_state_dict(torch.load(\"fold{}-city{}-model.pt\".format(FOLD,CITY)))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "        #self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,dpc,data):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "        \n",
    "            if data != None:\n",
    "                actionIndex = self.evaluate(dpc,data,self.Bu+1)\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop() \n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n",
    "    def evaluate(self,model, batch, cachesize):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            src = batch.permute(1,0,2).float().to(device)\n",
    "\n",
    "            batchsize = src.shape[1]\n",
    "            trg = torch.zeros((26,batchsize,1)).to(device)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output.squeeze(-1)\n",
    "\n",
    "            output = output[0].cpu().numpy()\n",
    "\n",
    "            max_item=np.argsort(-output)[0:cachesize]\n",
    "            #print(max_item,list(max_item))\n",
    "        return list(max_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>8033</td>\n",
       "      <td>0</td>\n",
       "      <td>2239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>363</td>\n",
       "      <td>13244</td>\n",
       "      <td>1</td>\n",
       "      <td>3907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>363</td>\n",
       "      <td>15904</td>\n",
       "      <td>1</td>\n",
       "      <td>4013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>979</td>\n",
       "      <td>3842</td>\n",
       "      <td>1</td>\n",
       "      <td>5523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>979</td>\n",
       "      <td>8313</td>\n",
       "      <td>1</td>\n",
       "      <td>5536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132685</th>\n",
       "      <td>717</td>\n",
       "      <td>8224</td>\n",
       "      <td>4319</td>\n",
       "      <td>15549531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132686</th>\n",
       "      <td>717</td>\n",
       "      <td>11752</td>\n",
       "      <td>4319</td>\n",
       "      <td>15549743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132687</th>\n",
       "      <td>717</td>\n",
       "      <td>8805</td>\n",
       "      <td>4319</td>\n",
       "      <td>15549756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132688</th>\n",
       "      <td>717</td>\n",
       "      <td>5436</td>\n",
       "      <td>4319</td>\n",
       "      <td>15549819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132689</th>\n",
       "      <td>898</td>\n",
       "      <td>18456</td>\n",
       "      <td>4319</td>\n",
       "      <td>15551461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132690 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u      i   day      time\n",
       "0        46   8033     0      2239\n",
       "1       363  13244     1      3907\n",
       "2       363  15904     1      4013\n",
       "3       979   3842     1      5523\n",
       "4       979   8313     1      5536\n",
       "...     ...    ...   ...       ...\n",
       "132685  717   8224  4319  15549531\n",
       "132686  717  11752  4319  15549743\n",
       "132687  717   8805  4319  15549756\n",
       "132688  717   5436  4319  15549819\n",
       "132689  898  18456  4319  15551461\n",
       "\n",
       "[132690 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>i</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>8033</td>\n",
       "      <td>0</td>\n",
       "      <td>2239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>363</td>\n",
       "      <td>13244</td>\n",
       "      <td>1</td>\n",
       "      <td>3907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>363</td>\n",
       "      <td>15904</td>\n",
       "      <td>1</td>\n",
       "      <td>4013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>979</td>\n",
       "      <td>3842</td>\n",
       "      <td>1</td>\n",
       "      <td>5523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>979</td>\n",
       "      <td>8313</td>\n",
       "      <td>1</td>\n",
       "      <td>5536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132685</th>\n",
       "      <td>717</td>\n",
       "      <td>8224</td>\n",
       "      <td>4319</td>\n",
       "      <td>15549531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132686</th>\n",
       "      <td>717</td>\n",
       "      <td>11752</td>\n",
       "      <td>4319</td>\n",
       "      <td>15549743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132687</th>\n",
       "      <td>717</td>\n",
       "      <td>8805</td>\n",
       "      <td>4319</td>\n",
       "      <td>15549756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132688</th>\n",
       "      <td>717</td>\n",
       "      <td>5436</td>\n",
       "      <td>4319</td>\n",
       "      <td>15549819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132689</th>\n",
       "      <td>898</td>\n",
       "      <td>18456</td>\n",
       "      <td>4319</td>\n",
       "      <td>15551461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132690 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          u      i   day      time\n",
       "0        46   8033     0      2239\n",
       "1       363  13244     1      3907\n",
       "2       363  15904     1      4013\n",
       "3       979   3842     1      5523\n",
       "4       979   8313     1      5536\n",
       "...     ...    ...   ...       ...\n",
       "132685  717   8224  4319  15549531\n",
       "132686  717  11752  4319  15549743\n",
       "132687  717   8805  4319  15549756\n",
       "132688  717   5436  4319  15549819\n",
       "132689  898  18456  4319  15551461\n",
       "\n",
       "[132690 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UIT['day'] = UIT['time']//(60*60)\n",
    "UIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sat Oct 16 23:57:40 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-5.57074  0.222    0.     ] total reward: -5.34874\n",
      "UEHitrate: 0.0  edgeHitrate 0.2775 sumHitrate 0.2775  privacy: 1.23117\n",
      "\n",
      "--Time: Sun Oct 17 00:06:33 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-5.18181  0.2046   0.     ] total reward: -4.97721\n",
      "UEHitrate: 0.0  edgeHitrate 0.25575 sumHitrate 0.25575  privacy: 1.07531\n",
      "\n",
      "--Time: Sun Oct 17 00:15:45 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-4.86024  0.21331  0.     ] total reward: -4.64693\n",
      "UEHitrate: 0.0  edgeHitrate 0.26663 sumHitrate 0.26663  privacy: 1.01628\n",
      "\n",
      "--Time: Sun Oct 17 00:26:08 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-4.68612  0.20518  0.     ] total reward: -4.48094\n",
      "UEHitrate: 0.0  edgeHitrate 0.25648 sumHitrate 0.25648  privacy: 0.98865\n",
      "\n",
      "--Time: Sun Oct 17 00:36:27 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-4.47599  0.21037  0.     ] total reward: -4.26563\n",
      "UEHitrate: 0.0  edgeHitrate 0.26296 sumHitrate 0.26296  privacy: 0.96617\n",
      "\n",
      "--Time: Sun Oct 17 00:46:57 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-4.36481  0.20401  0.     ] total reward: -4.1608\n",
      "UEHitrate: 0.0  edgeHitrate 0.25502 sumHitrate 0.25502  privacy: 0.95102\n",
      "\n",
      "--Time: Sun Oct 17 00:56:43 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-4.32765  0.19872  0.     ] total reward: -4.12893\n",
      "UEHitrate: 0.0  edgeHitrate 0.2484 sumHitrate 0.2484  privacy: 0.94363\n",
      "\n",
      "--Time: Sun Oct 17 01:05:19 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-4.2757   0.19411  0.     ] total reward: -4.08159\n",
      "UEHitrate: 0.0  edgeHitrate 0.24264 sumHitrate 0.24264  privacy: 0.93239\n",
      "\n",
      "--Time: Sun Oct 17 01:13:55 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-4.14134  0.19765  0.     ] total reward: -3.94369\n",
      "UEHitrate: 0.0  edgeHitrate 0.24707 sumHitrate 0.24707  privacy: 0.91877\n",
      "\n",
      "--Time: Sun Oct 17 01:22:20 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-4.09462  0.1948   0.     ] total reward: -3.89982\n",
      "UEHitrate: 0.0  edgeHitrate 0.2435 sumHitrate 0.2435  privacy: 0.90363\n",
      "\n",
      "--Time: Sun Oct 17 01:30:48 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-3.9737   0.20009  0.     ] total reward: -3.77362\n",
      "UEHitrate: 0.0  edgeHitrate 0.25011 sumHitrate 0.25011  privacy: 0.88124\n",
      "\n",
      "--Time: Sun Oct 17 01:39:13 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-3.88171  0.20006  0.     ] total reward: -3.68165\n",
      "UEHitrate: 0.0  edgeHitrate 0.25007 sumHitrate 0.25008  privacy: 0.85571\n",
      "\n",
      "--Time: Sun Oct 17 01:47:45 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-3.82348  0.19551  0.     ] total reward: -3.62797\n",
      "UEHitrate: 0.0  edgeHitrate 0.24438 sumHitrate 0.24438  privacy: 0.81495\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 17 01:50:01 2021   Index: 132689   Loss: 0.0 --\n",
      "Reward: [-3.79895  0.19582  0.     ] total reward: -3.60312\n",
      "UEHitrate: 0.0  edgeHitrate 0.24478 sumHitrate 0.24478  privacy: 0.77372\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "Frq_hour = torch.zeros((BATCHSIZE,720*6)).to(device)\n",
    "m = 20\n",
    "\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":1,\"betal\":1}\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "UEHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "edgeHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "privacyReduction = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "i_episode = 0\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    if uit[1] in np.argwhere(ue.lastAction):\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    if uit[2]>=m:\n",
    "        test_iterator = Frq_hour[:,uit[2]-m:uit[2]]/Frq_hour[:,uit[2]-m:uit[2]].sum(dim=0)\n",
    "        test_iterator = test_iterator.unsqueeze(2)\n",
    "        #print(test_iterator.sum())\n",
    "        \n",
    "    else:\n",
    "        test_iterator = None\n",
    "    \n",
    "    ue.selectAction(env,uit,model,test_iterator)\n",
    "\n",
    "    for content in np.argwhere(ue.lastAction):\n",
    "        Frq_hour[content,uit[2]]+=1\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Rl)\n",
    "    sumReward[2] += float(ue.Ro)\n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.2775 , 0.25575, 0.26663, 0.25648, 0.26296, 0.25502, 0.2484 ,\n",
       "        0.24264, 0.24707, 0.2435 , 0.25011, 0.25008, 0.24438, 0.24478,\n",
       "        0.     ]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.2775 , 0.25575, 0.26663, 0.25648, 0.26296, 0.25502, 0.2484 ,\n",
       "        0.24264, 0.24707, 0.2435 , 0.25011, 0.25007, 0.24438, 0.24478,\n",
       "        0.     ]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.23117, 1.07531, 1.01628, 0.98865, 0.96617, 0.95102, 0.94363,\n",
       "       0.93239, 0.91877, 0.90363, 0.88124, 0.85571, 0.81495, 0.77372,\n",
       "       0.     ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV(object):\n",
    "    def __init__(self,userNum,contentNum,latency,Bu):\n",
    "        self.userNum = userNum\n",
    "        self.contentNum =contentNum\n",
    "\n",
    "        self.r = np.zeros(shape=(userNum,contentNum),dtype=int)\n",
    "        self.p = np.full(shape=contentNum,fill_value = 1/userNum,dtype=np.float32)\n",
    "        self.e = np.zeros(shape=contentNum,dtype=int)\n",
    "        self.S = np.ones(shape=contentNum,dtype=int)\n",
    "        self.l = np.array(latency,dtype=np.float32)\n",
    "        \n",
    "\n",
    "        self.B = np.full(shape=userNum,fill_value=Bu,dtype=int)\n",
    "\n",
    "        self.pipe = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    #有序字典实现LRU\n",
    "    def updateEgdeCache(self,action,t):\n",
    "        for i in np.argwhere(action==1).squeeze(-1):\n",
    "            if i in self.pipe.keys():\n",
    "                self.pipe.pop(i)\n",
    "            elif len(self.pipe) >= 500:\n",
    "                self.e[self.pipe.popitem(last=False)[0]] = 0\n",
    "            self.pipe[i] = t\n",
    "            self.e[i] = 1\n",
    "\n",
    "    \n",
    "    def updateEnv(self,u,action,t):\n",
    "        \n",
    "        p_tmp = ((self.r[u] | action)-self.r[u])*(1/self.userNum) + self.p\n",
    "        self.p = np.where(p_tmp<1-1/self.userNum,p_tmp,1-1/self.userNum)\n",
    "        self.r[u] =  self.r[u] | action\n",
    "\n",
    "        self.updateEgdeCache(action,t)\n",
    "\n",
    "    def getStatus(self):\n",
    "        return (self.r,\n",
    "                self.p, \n",
    "                self.e,\n",
    "                self.S,\n",
    "                self.l)\n",
    "\n",
    "    #def reset(self):\n",
    "    #    self.r = np.zeros(shape=(self.userNum,self.contentNum),dtype=int)\n",
    "    #    self.p = np.full(shape=self.contentNum,fill_value = 1/self.userNum)\n",
    "    #    self.e = np.zeros(shape=self.contentNum)\n",
    "    #    self.S = np.ones(shape=self.contentNum,dtype=int)\n",
    "    #    self.l_edge = 0.1\n",
    "    #    self.l_cp = 1\n",
    "    #    self.B = np.full(shape=self.userNum,fill_value=15,dtype=int)\n",
    "    #    self.pipe = collections.OrderedDict()\n",
    "\n",
    "class UE(object):\n",
    "    def __init__(self,u,env,rewardPara):\n",
    "        self.u = u\n",
    "\n",
    "        self.W = []\n",
    "        self.v = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.Bu = Bu\n",
    "        self.contentNum = contentNum\n",
    "        self.userNum = userNum\n",
    "\n",
    "        r , p , e, S, l = env.getStatus()\n",
    "\n",
    "        #self.lastAction = torch.zeros(size=(contentNum,),dtype=int)\n",
    "        self.lastAction = np.zeros(contentNum,dtype=int)\n",
    "\n",
    "        self.ALPHAh = rewardPara['alpha']\n",
    "        self.BETAo =  rewardPara['betao']\n",
    "        self.BETAl =  rewardPara['betal']             \n",
    "        self.reward = 0\n",
    "\n",
    "        self.simU = simUsers[u]\n",
    "        self.lastStatusFeature = torch.from_numpy(self.statusEmbedding(r,p,e))\n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastp = p.copy()\n",
    "        \n",
    "        self.lastp = p\n",
    "        #self.lastr = r.clone()\n",
    "        #self.lastr = r.copy()\n",
    "        self.lastru = r[self.u]\n",
    "        \n",
    "    def updateViewContent(self,i):\n",
    "        self.W.append(i)\n",
    "        self.v[i] = 1\n",
    "\n",
    "    def statusEmbedding(self,r,p,e):\n",
    "        tmp = np.zeros(contentNum,dtype=np.float32)\n",
    "        for simUser in self.simU:\n",
    "            tmp += r[simUser]\n",
    "        simUserRu = (tmp / len(self.simU))\n",
    "        ru = r[self.u]\n",
    "        statusFeature = np.row_stack((self.v,ru,simUserRu,p,e))\n",
    "        \n",
    "        return statusFeature.T.astype(np.float32)\n",
    "    \n",
    "    def getReward(self,lastru,lastp,ru,p,i,lastAction,S,l,e,v):\n",
    "        \n",
    "        #self.Rh =   self.ALPHAh * (np.log(v * p + (1-v) * (1-p)).sum() / np.log(ru * p + (1-ru) * (1-p)).sum() )\n",
    "\n",
    "        #self.Rh = self.ALPHAh * ( 1 / ( 1 + np.exp( 0.5 * np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() ) ) )\n",
    "\n",
    "        self.Rh = - self.ALPHAh * ( np.log( ( lastru * lastp + ( 1 - lastru ) * ( 1 - lastp ) ) / ( ru * p + ( 1 - ru ) * ( 1 - p ) ) ).sum() )\n",
    "\n",
    "\n",
    "        self.Ro =   self.BETAo * lastAction[i] * S[i] * ( e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "        \n",
    "        # self.Ro =   self.BETAo * lastAction[i] * S[i] * ( 1 + e[i] * l[0] + ( 1 - e[i] ) * l[1] )\n",
    "\n",
    "        self.Rl =   self.BETAl * ( 1 - lastAction[i] ) * S[i] * e[i] * l[2]\n",
    "\n",
    "        #self.Rh[i] = self.Rh[i] + self.Ro + self.Rl\n",
    "\n",
    "        #return  self.Rh.sum()\n",
    "        return  (self.Rh + self.Ro + self.Rl).astype(np.float32)\n",
    "\n",
    "    def selectAction(self,env,uit,dpc,data):\n",
    "        \n",
    "        self.updateViewContent(uit[1])\n",
    "        \n",
    "        r , p , e, S, l = env.getStatus()\n",
    "        \n",
    "        self.reward = self.getReward(self.lastru,self.lastp,r[self.u],p,self.W[-1],self.lastAction,S,l,e,self.v)\n",
    "\n",
    "        self.lastp = p\n",
    "        self.lastru = copy.copy(r[self.u])\n",
    "        \n",
    "        #self.lastp = p.clone()\n",
    "        #self.lastr = r.clone()\n",
    "        \n",
    "        if self.W[-1] not in np.argwhere(self.lastAction): #local cache miss\n",
    "            action = np.zeros(contentNum,dtype=int)\n",
    "            action[self.W[-1]] = 1\n",
    "        \n",
    "            if data != None:\n",
    "                actionIndex = self.evaluate(dpc,data,self.Bu+1)\n",
    "            else:\n",
    "                actionIndex = list(torch.randint(0,self.contentNum,(self.Bu,)))\n",
    "            if self.W[-1] not in actionIndex:\n",
    "                actionIndex.pop() \n",
    "            for index in actionIndex:\n",
    "                action[index] = 1\n",
    "\n",
    "            env.updateEnv(self.u,action,uit[2])\n",
    "        else:\n",
    "            action = self.lastAction # keep the cache and no request the new video\n",
    "        return action\n",
    "    def evaluate(self,model, batch, cachesize):\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            src = batch.permute(1,0,2).float().to(device)\n",
    "\n",
    "            batchsize = src.shape[1]\n",
    "            trg = torch.zeros((26,batchsize,1)).to(device)\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            output = output.squeeze(-1)\n",
    "\n",
    "            output = output[0].cpu().numpy()\n",
    "\n",
    "            max_item=np.argsort(-output)[0:cachesize]\n",
    "            #print(max_item,list(max_item))\n",
    "        return list(max_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time: Sun Oct 17 10:13:50 2021   Index: 9999   Loss: 0.0 --\n",
      "Reward: [-13.59472   0.22424   0.     ] total reward: -13.37048\n",
      "UEHitrate: 0.0  edgeHitrate 0.2803 sumHitrate 0.2803  privacy: 2.06928\n",
      "\n",
      "--Time: Sun Oct 17 10:23:04 2021   Index: 19999   Loss: 0.0 --\n",
      "Reward: [-9.14193  0.20572  0.     ] total reward: -8.93621\n",
      "UEHitrate: 0.0  edgeHitrate 0.25715 sumHitrate 0.25715  privacy: 1.65084\n",
      "\n",
      "--Time: Sun Oct 17 10:32:20 2021   Index: 29999   Loss: 0.0 --\n",
      "Reward: [-7.46918  0.21405  0.     ] total reward: -7.25512\n",
      "UEHitrate: 0.0  edgeHitrate 0.26757 sumHitrate 0.26757  privacy: 1.47097\n",
      "\n",
      "--Time: Sun Oct 17 10:41:58 2021   Index: 39999   Loss: 0.0 --\n",
      "Reward: [-6.62011  0.20574  0.     ] total reward: -6.41437\n",
      "UEHitrate: 0.0  edgeHitrate 0.25717 sumHitrate 0.25718  privacy: 1.37198\n",
      "\n",
      "--Time: Sun Oct 17 10:51:45 2021   Index: 49999   Loss: 0.0 --\n",
      "Reward: [-6.01392  0.21082  0.     ] total reward: -5.80311\n",
      "UEHitrate: 0.0  edgeHitrate 0.26352 sumHitrate 0.26352  privacy: 1.29012\n",
      "\n",
      "--Time: Sun Oct 17 11:01:39 2021   Index: 59999   Loss: 0.0 --\n",
      "Reward: [-5.63214  0.20439  0.     ] total reward: -5.42775\n",
      "UEHitrate: 0.0  edgeHitrate 0.25548 sumHitrate 0.25548  privacy: 1.23405\n",
      "\n",
      "--Time: Sun Oct 17 11:11:18 2021   Index: 69999   Loss: 0.0 --\n",
      "Reward: [-5.39758  0.19904  0.     ] total reward: -5.19854\n",
      "UEHitrate: 0.0  edgeHitrate 0.2488 sumHitrate 0.2488  privacy: 1.19906\n",
      "\n",
      "--Time: Sun Oct 17 11:21:06 2021   Index: 79999   Loss: 0.0 --\n",
      "Reward: [-5.199    0.19439  0.     ] total reward: -5.00461\n",
      "UEHitrate: 0.0  edgeHitrate 0.24299 sumHitrate 0.24299  privacy: 1.15923\n",
      "\n",
      "--Time: Sun Oct 17 11:30:39 2021   Index: 89999   Loss: 0.0 --\n",
      "Reward: [-4.95723  0.1979   0.     ] total reward: -4.75933\n",
      "UEHitrate: 0.0  edgeHitrate 0.24738 sumHitrate 0.24738  privacy: 1.12433\n",
      "\n",
      "--Time: Sun Oct 17 11:40:41 2021   Index: 99999   Loss: 0.0 --\n",
      "Reward: [-4.82025  0.19502  0.     ] total reward: -4.62522\n",
      "UEHitrate: 0.0  edgeHitrate 0.24378 sumHitrate 0.24378  privacy: 1.08759\n",
      "\n",
      "--Time: Sun Oct 17 11:50:52 2021   Index: 109999   Loss: 0.0 --\n",
      "Reward: [-4.62901  0.20029  0.     ] total reward: -4.42872\n",
      "UEHitrate: 0.0  edgeHitrate 0.25036 sumHitrate 0.25036  privacy: 1.04539\n",
      "\n",
      "--Time: Sun Oct 17 12:00:49 2021   Index: 119999   Loss: 0.0 --\n",
      "Reward: [-4.47914  0.20025  0.     ] total reward: -4.2789\n",
      "UEHitrate: 0.0  edgeHitrate 0.25031 sumHitrate 0.25031  privacy: 1.00383\n",
      "\n",
      "--Time: Sun Oct 17 12:10:34 2021   Index: 129999   Loss: 0.0 --\n",
      "Reward: [-4.37086  0.19568  0.     ] total reward: -4.17518\n",
      "UEHitrate: 0.0  edgeHitrate 0.2446 sumHitrate 0.2446  privacy: 0.94799\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "--Time: Sun Oct 17 12:13:22 2021   Index: 132689   Loss: 0.0 --\n",
      "Reward: [-4.33437  0.19599  0.     ] total reward: -4.13838\n",
      "UEHitrate: 0.0  edgeHitrate 0.24499 sumHitrate 0.24499  privacy: 0.89549\n",
      "----------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1\n",
    "TARGET_UPDATE = 1\n",
    "bestReward =  float(\"-inf\")\n",
    "\n",
    "env = ENV(userNum,contentNum,latency,Bu)\n",
    "UEs = {}\n",
    "sumReward = np.zeros(3)\n",
    "loss = 0\n",
    "UEHit = np.zeros(userNum)\n",
    "edgeHit = 0\n",
    "\n",
    "Frq_hour = torch.zeros((BATCHSIZE,720*6)).to(device)\n",
    "m = 20\n",
    "\n",
    "\n",
    "rewardPara = {\"alpha\":1,\"betao\":1,\"betal\":1}\n",
    "\n",
    "sumHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "UEHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "edgeHitrate = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "privacyReduction = np.zeros(UIT.shape[0]// 10000 +2)\n",
    "i_episode = 0\n",
    "\n",
    "for index,trace in UIT.iterrows():\n",
    "    uit = trace.to_numpy()\n",
    "    if uit[0] not in UEs:\n",
    "        UEs[uit[0]] = UE(uit[0],env,rewardPara)\n",
    "    ue = UEs[uit[0]]\n",
    "    \n",
    "    if uit[1] in np.argwhere(ue.lastAction):\n",
    "        UEHit[uit[0]] += 1\n",
    "    elif uit[1] in env.pipe.keys():\n",
    "        edgeHit += 1\n",
    "    if uit[2]>=m:\n",
    "        test_iterator = Frq_hour[:,uit[2]-m:uit[2]]/Frq_hour[:,uit[2]-m:uit[2]].sum(dim=0)\n",
    "        test_iterator = test_iterator.unsqueeze(2)\n",
    "        #print(test_iterator.sum())\n",
    "        \n",
    "    else:\n",
    "        test_iterator = None\n",
    "    \n",
    "    ue.selectAction(env,uit,model,test_iterator)\n",
    "\n",
    "    for content in np.argwhere(ue.lastAction):\n",
    "        Frq_hour[content,uit[2]]+=1\n",
    "    \n",
    "    sumReward[0] += float(ue.Rh)\n",
    "    sumReward[1] += float(ue.Rl)\n",
    "    sumReward[2] += float(ue.Ro)\n",
    "    if (index+1) % 10000 == 0 :\n",
    "        psi = 0\n",
    "        for u in UEs:\n",
    "            psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "        print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "        print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "        print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "        print()\n",
    "        sumHitrate[int(index // 10000)]   = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "        UEHitrate [int(index // 10000)]   = round(UEHit.sum()/(index+1),5)\n",
    "        edgeHitrate [int(index // 10000)] = round(edgeHit/(index+1),5)\n",
    "        privacyReduction [int(index // 10000)] = round(float(psi)/len(UEs),5)\n",
    "        \n",
    "psi = 0\n",
    "for u in UEs:\n",
    "    psi += np.log(env.r[u] * env.p + (1-env.r[u]) * (1-env.p)).sum() / np.log(UEs[u].v * env.p + (1-UEs[u].v) * (1-env.p)).sum()\n",
    "print()\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print(\"--Time:\",time.asctime( time.localtime(time.time())),\"  Index:\",index,\"  Loss:\",round(loss/(index+1),5),\"--\")\n",
    "print(\"Reward:\",np.around(sumReward/(index+1),5),\"total reward:\",round(sumReward.sum()/(index+1),5))\n",
    "print(\"UEHitrate:\",round(UEHit.sum()/(index+1),5),\" edgeHitrate\",round(edgeHit/(index+1),5),\"sumHitrate\",round((edgeHit+UEHit.sum())/(index+1),5),\" privacy:\",round(float(psi)/len(UEs),5))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "print()\n",
    "sumHitrate [int(round(index / 10000,0))]  = round((edgeHit+UEHit.sum())/(index+1),5)\n",
    "UEHitrate  [int(round(index / 10000,0))]  = round(UEHit.sum()/(index+1),5)\n",
    "edgeHitrate[int(round(index / 10000,0))]  = round(edgeHit/(index+1),5)\n",
    "privacyReduction [int(round(index / 10000,0))] = round(float(psi)/len(UEs),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.2803 , 0.25715, 0.26757, 0.25718, 0.26352, 0.25548, 0.2488 ,\n",
       "        0.24299, 0.24738, 0.24378, 0.25036, 0.25031, 0.2446 , 0.24499,\n",
       "        0.     ]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0.2803 , 0.25715, 0.26757, 0.25717, 0.26352, 0.25548, 0.2488 ,\n",
       "        0.24299, 0.24738, 0.24378, 0.25036, 0.25031, 0.2446 , 0.24499,\n",
       "        0.     ]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumHitrate, UEHitrate, edgeHitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.06928, 1.65084, 1.47097, 1.37198, 1.29012, 1.23405, 1.19906,\n",
       "       1.15923, 1.12433, 1.08759, 1.04539, 1.00383, 0.94799, 0.89549,\n",
       "       0.     ])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacyReduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "53e075add4fc865efaed3001cae69f5b66291fd877e6c0fafb5013552ba051ca"
  },
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
